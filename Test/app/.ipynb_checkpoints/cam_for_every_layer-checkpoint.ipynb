{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多分类观察验证集中每张图片在所有conv layer后的feature map的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import easydict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functions import cal_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=easydict.EasyDict({\n",
    "    'batch_size':32,\n",
    "    'epoch':20,\n",
    "    'data_root_path':'/media/kzy/tt/Data/imgnet_localization/',\n",
    "    'csv_path':'../save/data.csv',\n",
    "    'lr':0.001,\n",
    "    'train_log':'train_log.txt',\n",
    "    'class_nums':20\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self, args, data_csv_path='data.csv', mode='train', img_size=224):\n",
    "        self.data = pd.read_csv(args.csv_path)\n",
    "        self.mode = mode\n",
    "        self.args = args\n",
    "        self.img_size = img_size\n",
    "        self.class_nums=args.class_nums\n",
    "\n",
    "        self.target_csv=self.data[self.data['label'].isin(range(self.class_nums))]\n",
    "        self.train_csv=self.target_csv[self.target_csv['is_val']==0]\n",
    "        self.val_csv=self.target_csv[self.target_csv['is_val']==1]\n",
    "        self.train_csv.reset_index(inplace=True,drop=True)\n",
    "        self.val_csv.reset_index(inplace=True,drop=True)\n",
    "        \n",
    "        if self.mode=='train':\n",
    "            self.cur_csv=self.train_csv\n",
    "        else:\n",
    "            self.cur_csv=self.val_csv\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.cur_csv.loc[index]\n",
    "\n",
    "        img_id = item['ImageId']\n",
    "        path = item['path']\n",
    "        label = item['label']\n",
    "        bbox = item['bbox']\n",
    "\n",
    "        raw_img = Image.open(self.args.data_root_path + path).convert('RGB')\n",
    "        img = self.image_transform(img_size=self.img_size, mode=self.mode)(raw_img)\n",
    "\n",
    "        return img_id, img, label, bbox\n",
    "\n",
    "    def to_train(self):\n",
    "        self.mode = 'train'\n",
    "        self.cur_csv = self.train_csv\n",
    "\n",
    "    def to_val(self):\n",
    "        self.mode = 'val'\n",
    "        self.cur_csv = self.val_csv\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cur_csv)\n",
    "    \n",
    "    @staticmethod\n",
    "    def image_transform(img_size, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], mode='train'):\n",
    "        if mode == 'train':\n",
    "            horizontal_flip = 0.5\n",
    "            vertical_flip = 0.5\n",
    "\n",
    "            t = [\n",
    "                transforms.RandomResizedCrop(size=img_size),\n",
    "                transforms.RandomHorizontalFlip(horizontal_flip),\n",
    "                transforms.RandomVerticalFlip(vertical_flip),\n",
    "                transforms.ColorJitter(saturation=0.4, brightness=0.4, hue=0.05),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            t = [\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ]\n",
    "\n",
    "        return transforms.Compose([v for v in t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-070f680dc249>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mEvery_cam_layer_vgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_trained_vgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_nums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze_vgg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvery_cam_layer_vgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze_vgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreeze_vgg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Every_cam_layer_vgg(nn.Module):\n",
    "    def __init__(self, pre_trained_vgg, class_nums=20, inference=False, freeze_vgg=False):\n",
    "        super(Every_cam_layer_vgg, self).__init__()\n",
    "        self.inference = inference\n",
    "        self.freeze_vgg = freeze_vgg\n",
    "\n",
    "        self.features = pre_trained_vgg.features\n",
    "        self.cls = nn.Sequential(\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1, dilation=1),  # fc6\n",
    "            nn.ReLU(True),\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1, dilation=1),  # fc6\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(1024, class_nums, kernel_size=1, padding=0)  # fc8\n",
    "        )\n",
    "        \n",
    "        self.hook_handle={}\n",
    "        self.each_layer_feature_map=[]\n",
    "\n",
    "        if self.freeze_vgg:\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.inference:\n",
    "            x.requires_grad_()\n",
    "            x.retain_grad()\n",
    "\n",
    "        base = self.features(x)\n",
    "        avg_pool = F.avg_pool2d(base, kernel_size=3, stride=1, padding=1)\n",
    "        cam = self.cls(avg_pool)\n",
    "        logits = torch.mean(torch.mean(cam, dim=2), dim=2)  # (1,200)\n",
    "\n",
    "        if self.inference:\n",
    "            pass\n",
    "\n",
    "        return logits, cam\n",
    "    \n",
    "    def use_hook(self):\n",
    "        for index,module in enumerate(self.modules()):\n",
    "            if isinstance(module,nn.ReLU):\n",
    "                self.hook_handle['relu_{}'.format(index)]=module.register_forward_hook(for_hook)\n",
    "                \n",
    "    def cancel_hook(self):\n",
    "        for key in self.hook_handle.keys():\n",
    "            self.hook_handle[key].remove()\n",
    "        \n",
    "        self.each_layer_feature_map=[]\n",
    "            \n",
    "    @staticmethod\n",
    "    def for_hook(module, input, output):\n",
    "        self.each_layer_feature_map.append(input.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(pretrained=True, trained=False, **kwargs):\n",
    "    pre_trained_model = models.vgg16(pretrained=pretrained)\n",
    "\n",
    "    model = Every_cam_layer_vgg(pre_trained_vgg=pre_trained_model, **kwargs)\n",
    "    model.cuda()\n",
    "\n",
    "    if trained:\n",
    "        model.load_state_dict(torch.load('../save/models/binary_vgg.pt'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 属性函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finetune_optimizer(args, model):\n",
    "    lr = args.lr\n",
    "    weight_list = []\n",
    "    bias_list = []\n",
    "    last_weight_list = []\n",
    "    last_bias_list = []\n",
    "    for name, value in model.named_parameters():\n",
    "        if 'cls' in name:\n",
    "            if 'weight' in name:\n",
    "                last_weight_list.append(value)\n",
    "            elif 'bias' in name:\n",
    "                last_bias_list.append(value)\n",
    "        else:\n",
    "            if 'weight' in name:\n",
    "                weight_list.append(value)\n",
    "            elif 'bias' in name:\n",
    "                bias_list.append(value)\n",
    "\n",
    "    opt = optim.SGD([{'params': weight_list, 'lr': lr / 10},\n",
    "                     {'params': bias_list, 'lr': lr / 5},\n",
    "                     {'params': last_weight_list, 'lr': lr},\n",
    "                     {'params': last_bias_list, 'lr': lr * 2}], momentum=0.9, weight_decay=0.0001, nesterov=False)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=Loader(args=args)\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果可视"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_process(result):\n",
    "    train,val=result\n",
    "    \n",
    "    x=range(len(train))\n",
    "    plt.figure()\n",
    "    plt.plot(x,train,'o-',label='train',color='green')\n",
    "    plt.plot(x,val,'o-',label='val',color='red')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.savefig('../train.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train loss:2.986095428466797 train acc:0.0\n",
      "epoch:0 train loss:3.0075504779815674 train acc:0.03125\n",
      "epoch:0 train loss:2.999685049057007 train acc:0.0625\n",
      "epoch:0 train loss:2.9855949878692627 train acc:0.0\n",
      "epoch:0 train loss:3.0027616024017334 train acc:0.0\n",
      "epoch:0 train loss:2.9969112873077393 train acc:0.0625\n",
      "epoch:0 train loss:2.9842212200164795 train acc:0.03125\n",
      "epoch:0 train loss:2.969090223312378 train acc:0.09375\n",
      "epoch:0 train loss:2.989107131958008 train acc:0.03125\n",
      "epoch:0 train loss:2.9970946311950684 train acc:0.03125\n",
      "epoch:0 train loss:2.9983911514282227 train acc:0.0625\n",
      "epoch:0 train loss:2.9800021648406982 train acc:0.09375\n",
      "epoch:0 train loss:2.9941763877868652 train acc:0.0625\n",
      "epoch:0 train loss:2.9791274070739746 train acc:0.09375\n",
      "epoch:0 train loss:2.973191976547241 train acc:0.0625\n",
      "epoch:0 train loss:2.9795587062835693 train acc:0.1875\n",
      "epoch:0 train loss:2.956615447998047 train acc:0.125\n",
      "epoch:0 train loss:2.949517250061035 train acc:0.125\n",
      "epoch:0 train loss:2.9689149856567383 train acc:0.09375\n",
      "epoch:0 train loss:2.960737466812134 train acc:0.03125\n",
      "epoch:0 train loss:2.954254627227783 train acc:0.21875\n",
      "epoch:0 train loss:2.9232051372528076 train acc:0.25\n",
      "epoch:0 train loss:2.952105760574341 train acc:0.125\n",
      "epoch:0 train loss:2.9252779483795166 train acc:0.1875\n",
      "epoch:0 train loss:2.9632670879364014 train acc:0.09375\n",
      "epoch:0 train loss:2.927682399749756 train acc:0.0625\n",
      "epoch:0 train loss:2.939582347869873 train acc:0.0625\n",
      "epoch:0 train loss:2.934743881225586 train acc:0.15625\n",
      "epoch:0 train loss:2.9070322513580322 train acc:0.125\n",
      "epoch:0 train loss:2.9025752544403076 train acc:0.1875\n",
      "epoch:0 train loss:2.948859214782715 train acc:0.0625\n",
      "epoch:0 train loss:2.963395595550537 train acc:0.03125\n",
      "epoch:0 train loss:2.8751442432403564 train acc:0.09375\n",
      "epoch:0 train loss:2.8781800270080566 train acc:0.1875\n",
      "epoch:0 train loss:2.8800370693206787 train acc:0.15625\n",
      "epoch:0 train loss:2.897902488708496 train acc:0.125\n",
      "epoch:0 train loss:2.9214465618133545 train acc:0.09375\n",
      "epoch:0 train loss:2.9072022438049316 train acc:0.09375\n",
      "epoch:0 train loss:2.892324209213257 train acc:0.15625\n",
      "epoch:0 train loss:2.9245383739471436 train acc:0.125\n",
      "epoch:0 train loss:2.8698313236236572 train acc:0.09375\n",
      "epoch:0 train loss:2.880553960800171 train acc:0.125\n",
      "epoch:0 train loss:2.8786215782165527 train acc:0.09375\n",
      "epoch:0 train loss:2.897141456604004 train acc:0.0625\n",
      "epoch:0 train loss:2.819066047668457 train acc:0.09375\n",
      "epoch:0 train loss:2.938555955886841 train acc:0.0\n",
      "epoch:0 train loss:2.8429455757141113 train acc:0.09375\n",
      "epoch:0 train loss:2.8682196140289307 train acc:0.125\n",
      "epoch:0 train loss:2.838576078414917 train acc:0.15625\n",
      "epoch:0 train loss:2.882155418395996 train acc:0.125\n",
      "epoch:0 train loss:2.886469841003418 train acc:0.09375\n",
      "epoch:0 train loss:2.8311798572540283 train acc:0.09375\n",
      "epoch:0 train loss:2.8820407390594482 train acc:0.0625\n",
      "epoch:0 train loss:2.8827993869781494 train acc:0.09375\n",
      "epoch:0 train loss:2.7249603271484375 train acc:0.15625\n",
      "epoch:0 train loss:2.6857693195343018 train acc:0.15625\n",
      "epoch:0 train loss:2.7792434692382812 train acc:0.125\n",
      "epoch:0 train loss:2.8372976779937744 train acc:0.09375\n",
      "epoch:0 train loss:2.742356538772583 train acc:0.15625\n",
      "epoch:0 train loss:2.7925667762756348 train acc:0.15625\n",
      "epoch:0 train loss:2.7408297061920166 train acc:0.15625\n",
      "epoch:0 train loss:2.8347768783569336 train acc:0.09375\n",
      "epoch:0 train loss:2.760035991668701 train acc:0.125\n",
      "epoch:0 train loss:2.7706587314605713 train acc:0.1875\n",
      "epoch:0 train loss:2.8408944606781006 train acc:0.09375\n",
      "epoch:0 train loss:2.785159111022949 train acc:0.125\n",
      "epoch:0 train loss:2.73378586769104 train acc:0.15625\n",
      "epoch:0 train loss:2.6764073371887207 train acc:0.1875\n",
      "epoch:0 train loss:2.636577844619751 train acc:0.34375\n",
      "epoch:0 train loss:2.5848870277404785 train acc:0.21875\n",
      "epoch:0 train loss:2.764716625213623 train acc:0.1875\n",
      "epoch:0 train loss:2.666595220565796 train acc:0.09375\n",
      "epoch:0 train loss:2.7281148433685303 train acc:0.15625\n",
      "epoch:0 train loss:2.629246711730957 train acc:0.21875\n",
      "epoch:0 train loss:2.509577989578247 train acc:0.3125\n",
      "epoch:0 train loss:2.7436373233795166 train acc:0.15625\n",
      "epoch:0 train loss:2.764940023422241 train acc:0.09375\n",
      "epoch:0 train loss:2.5691983699798584 train acc:0.25\n",
      "epoch:0 train loss:2.669501781463623 train acc:0.25\n",
      "epoch:0 train loss:2.7187607288360596 train acc:0.09375\n",
      "epoch:0 train loss:2.6465649604797363 train acc:0.25\n",
      "epoch:0 train loss:2.69103741645813 train acc:0.09375\n",
      "epoch:0 train loss:2.5426156520843506 train acc:0.3125\n",
      "epoch:0 train loss:2.666954755783081 train acc:0.3125\n",
      "epoch:0 train loss:2.4827120304107666 train acc:0.34375\n",
      "epoch:0 train loss:2.515876293182373 train acc:0.34375\n",
      "epoch:0 train loss:2.56512451171875 train acc:0.3125\n",
      "epoch:0 train loss:2.5418152809143066 train acc:0.28125\n",
      "epoch:0 train loss:2.3983044624328613 train acc:0.4375\n",
      "epoch:0 train loss:2.6254076957702637 train acc:0.21875\n",
      "epoch:0 train loss:2.2746148109436035 train acc:0.53125\n",
      "epoch:0 train loss:2.4535505771636963 train acc:0.375\n",
      "epoch:0 train loss:2.4048690795898438 train acc:0.34375\n",
      "epoch:0 train loss:2.3211286067962646 train acc:0.5\n",
      "epoch:0 train loss:2.5845189094543457 train acc:0.375\n",
      "epoch:0 train loss:2.4001169204711914 train acc:0.28125\n",
      "epoch:0 train loss:2.2146944999694824 train acc:0.4375\n",
      "epoch:0 train loss:2.199608325958252 train acc:0.5\n",
      "epoch:0 train loss:2.391904354095459 train acc:0.40625\n",
      "epoch:0 train loss:2.2544026374816895 train acc:0.46875\n",
      "epoch:0 train loss:2.252105951309204 train acc:0.40625\n",
      "epoch:0 train loss:2.480970859527588 train acc:0.34375\n",
      "epoch:0 train loss:2.408135414123535 train acc:0.4375\n",
      "epoch:0 train loss:2.183331251144409 train acc:0.4375\n",
      "epoch:0 train loss:2.2168989181518555 train acc:0.4375\n",
      "epoch:0 train loss:2.3041298389434814 train acc:0.5\n",
      "epoch:0 train loss:2.1358134746551514 train acc:0.59375\n",
      "epoch:0 train loss:2.271267890930176 train acc:0.3125\n",
      "epoch:0 train loss:2.2742490768432617 train acc:0.53125\n",
      "epoch:0 train loss:2.0043177604675293 train acc:0.53125\n",
      "epoch:0 train loss:2.3058719635009766 train acc:0.5\n",
      "epoch:0 train loss:2.101224184036255 train acc:0.625\n",
      "epoch:0 train loss:1.9500274658203125 train acc:0.53125\n",
      "epoch:0 train loss:1.7228912115097046 train acc:0.71875\n",
      "epoch:0 train loss:1.8610209226608276 train acc:0.65625\n",
      "epoch:0 train loss:1.9944790601730347 train acc:0.59375\n",
      "epoch:0 train loss:1.9269466400146484 train acc:0.625\n",
      "epoch:0 train loss:1.898766040802002 train acc:0.6875\n",
      "epoch:0 train loss:1.8047006130218506 train acc:0.65625\n",
      "epoch:0 train loss:1.8144387006759644 train acc:0.5625\n",
      "epoch:0 train loss:1.9375874996185303 train acc:0.625\n",
      "epoch:0 train loss:1.9082458019256592 train acc:0.53125\n",
      "epoch:0 train loss:1.8712373971939087 train acc:0.4375\n",
      "epoch:0 train loss:1.7254492044448853 train acc:0.53125\n",
      "epoch:0 train loss:1.88398015499115 train acc:0.5\n",
      "epoch:0 train loss:1.6408913135528564 train acc:0.59375\n",
      "epoch:0 train loss:1.7411472797393799 train acc:0.71875\n",
      "epoch:0 train loss:1.6810556650161743 train acc:0.5\n",
      "epoch:0 train loss:1.6644600629806519 train acc:0.59375\n",
      "epoch:0 train loss:1.3598573207855225 train acc:0.6875\n",
      "epoch:0 train loss:1.5476723909378052 train acc:0.65625\n",
      "epoch:0 train loss:1.5632290840148926 train acc:0.625\n",
      "epoch:0 train loss:1.639830470085144 train acc:0.65625\n",
      "epoch:0 train loss:1.7588919401168823 train acc:0.625\n",
      "epoch:0 train loss:1.6143499612808228 train acc:0.5625\n",
      "epoch:0 train loss:1.479337215423584 train acc:0.625\n",
      "epoch:0 train loss:1.1210832595825195 train acc:0.75\n",
      "epoch:0 train loss:1.7105666399002075 train acc:0.59375\n",
      "epoch:0 train loss:1.5216389894485474 train acc:0.65625\n",
      "epoch:0 train loss:1.3632253408432007 train acc:0.625\n",
      "epoch:0 train loss:1.4202088117599487 train acc:0.625\n",
      "epoch:0 train loss:1.2236729860305786 train acc:0.6875\n",
      "epoch:0 train loss:1.2399046421051025 train acc:0.625\n",
      "epoch:0 train loss:1.1416696310043335 train acc:0.59375\n",
      "epoch:0 train loss:1.225406527519226 train acc:0.75\n",
      "epoch:0 train loss:1.4647232294082642 train acc:0.53125\n",
      "epoch:0 train loss:1.318889856338501 train acc:0.65625\n",
      "epoch:0 train loss:1.4917874336242676 train acc:0.59375\n",
      "epoch:0 train loss:1.351976990699768 train acc:0.78125\n",
      "epoch:0 train loss:1.4088661670684814 train acc:0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train loss:1.1816108226776123 train acc:0.75\n",
      "epoch:0 train loss:0.9636920690536499 train acc:0.84375\n",
      "epoch:0 train loss:1.082040786743164 train acc:0.65625\n",
      "epoch:0 train loss:1.0284955501556396 train acc:0.75\n",
      "epoch:0 train loss:1.244316577911377 train acc:0.625\n",
      "epoch:0 train loss:1.0563418865203857 train acc:0.71875\n",
      "epoch:0 train loss:1.125197172164917 train acc:0.65625\n",
      "epoch:0 train loss:1.2609870433807373 train acc:0.65625\n",
      "epoch:0 train loss:1.2130755186080933 train acc:0.625\n",
      "epoch:0 train loss:0.9762181639671326 train acc:0.75\n",
      "epoch:0 train loss:1.0113587379455566 train acc:0.84375\n",
      "epoch:0 train loss:1.1593775749206543 train acc:0.65625\n",
      "epoch:0 train loss:1.3293428421020508 train acc:0.59375\n",
      "epoch:0 train loss:1.6615078449249268 train acc:0.5625\n",
      "epoch:0 train loss:1.1037852764129639 train acc:0.625\n",
      "epoch:0 train loss:0.975103497505188 train acc:0.8125\n",
      "epoch:0 train loss:1.1313925981521606 train acc:0.65625\n",
      "epoch:0 train loss:1.1059222221374512 train acc:0.65625\n",
      "epoch:0 train loss:1.2876371145248413 train acc:0.65625\n",
      "epoch:0 train loss:1.1987396478652954 train acc:0.65625\n",
      "epoch:0 train loss:1.1663755178451538 train acc:0.59375\n",
      "epoch:0 train loss:1.150164008140564 train acc:0.6875\n",
      "epoch:0 train loss:0.7984778881072998 train acc:0.84375\n",
      "epoch:0 train loss:1.2902908325195312 train acc:0.5625\n",
      "epoch:0 train loss:0.8392842411994934 train acc:0.75\n",
      "epoch:0 train loss:1.1469433307647705 train acc:0.6875\n",
      "epoch:0 train loss:0.87993985414505 train acc:0.65625\n",
      "epoch:0 train loss:1.0123939514160156 train acc:0.6875\n",
      "epoch:0 train loss:0.9473609328269958 train acc:0.84375\n",
      "epoch:0 train loss:1.2241237163543701 train acc:0.65625\n",
      "epoch:0 train loss:1.288914442062378 train acc:0.5625\n",
      "epoch:0 train loss:0.8878192901611328 train acc:0.75\n",
      "epoch:0 train loss:1.1097549200057983 train acc:0.78125\n",
      "epoch:0 train loss:0.7857128381729126 train acc:0.78125\n",
      "epoch:0 train loss:1.3134369850158691 train acc:0.625\n",
      "epoch:0 train loss:0.9820822477340698 train acc:0.71875\n",
      "epoch:0 train loss:0.8599618077278137 train acc:0.71875\n",
      "epoch:0 train loss:1.4038609266281128 train acc:0.6875\n",
      "epoch:0 train loss:0.8071151375770569 train acc:0.75\n",
      "epoch:0 train loss:0.9295382499694824 train acc:0.875\n",
      "epoch:0 train loss:0.6469419598579407 train acc:0.8125\n",
      "epoch:0 train loss:0.9382834434509277 train acc:0.6875\n",
      "epoch:0 train loss:0.8492975831031799 train acc:0.8125\n",
      "epoch:0 train loss:0.7115514278411865 train acc:0.8125\n",
      "epoch:0 train loss:0.6397426128387451 train acc:0.8125\n",
      "epoch:0 train loss:1.3144645690917969 train acc:0.78125\n",
      "epoch:0 train loss:1.0751668214797974 train acc:0.65625\n",
      "epoch:0 train loss:0.7305032014846802 train acc:0.84375\n",
      "epoch:0 train loss:0.8001395463943481 train acc:0.8125\n",
      "epoch:0 train loss:0.7510899305343628 train acc:0.8125\n",
      "epoch:0 train loss:0.5487590432167053 train acc:0.875\n",
      "epoch:0 train loss:0.9027056694030762 train acc:0.75\n",
      "epoch:0 train loss:1.1792709827423096 train acc:0.71875\n",
      "epoch:0 train loss:0.7994890213012695 train acc:0.78125\n",
      "epoch:0 train loss:0.9872111678123474 train acc:0.8125\n",
      "epoch:0 train loss:0.6777074933052063 train acc:0.78125\n",
      "epoch:0 train loss:0.9859031438827515 train acc:0.78125\n",
      "epoch:0 train loss:0.8767344355583191 train acc:0.78125\n",
      "epoch:0 train loss:0.8019834756851196 train acc:0.8125\n",
      "epoch:0 train loss:0.6810745596885681 train acc:0.8125\n",
      "epoch:0 train loss:0.6011097431182861 train acc:0.84375\n",
      "epoch:0 train loss:0.865092396736145 train acc:0.8125\n",
      "epoch:0 train loss:1.1384309530258179 train acc:0.6875\n",
      "epoch:0 train loss:0.8501018285751343 train acc:0.8125\n",
      "epoch:0 train loss:0.5373681783676147 train acc:0.90625\n",
      "epoch:0 train loss:1.0193787813186646 train acc:0.6875\n",
      "epoch:0 train loss:0.6172647476196289 train acc:0.8125\n",
      "epoch:0 train loss:0.6375463008880615 train acc:0.84375\n",
      "epoch:0 train loss:0.7906908988952637 train acc:0.71875\n",
      "epoch:0 train loss:0.6821475625038147 train acc:0.78125\n",
      "epoch:0 train loss:0.8302801251411438 train acc:0.71875\n",
      "epoch:0 train loss:0.7219640016555786 train acc:0.84375\n",
      "epoch:0 train loss:0.7471851110458374 train acc:0.71875\n",
      "epoch:0 train loss:0.6571983098983765 train acc:0.78125\n",
      "epoch:0 train loss:0.550518274307251 train acc:0.875\n",
      "epoch:0 train loss:0.7597841024398804 train acc:0.75\n",
      "epoch:0 train loss:0.7305195331573486 train acc:0.75\n",
      "epoch:0 train loss:1.1123887300491333 train acc:0.59375\n",
      "epoch:0 train loss:0.8108408451080322 train acc:0.84375\n",
      "epoch:0 train loss:0.3347184956073761 train acc:0.90625\n",
      "epoch:0 train loss:0.5343261957168579 train acc:0.8125\n",
      "epoch:0 train loss:0.8152529001235962 train acc:0.6875\n",
      "epoch:0 train loss:1.2798314094543457 train acc:0.75\n",
      "epoch:0 train loss:0.7960723042488098 train acc:0.75\n",
      "epoch:0 train loss:0.7803099155426025 train acc:0.75\n",
      "epoch:0 train loss:1.05958890914917 train acc:0.75\n",
      "epoch:0 train loss:1.0447583198547363 train acc:0.75\n",
      "epoch:0 train loss:1.2541260719299316 train acc:0.625\n",
      "epoch:0 train loss:0.6097605228424072 train acc:0.78125\n",
      "epoch:0 train loss:0.8622729778289795 train acc:0.71875\n",
      "epoch:0 train loss:0.7300146818161011 train acc:0.6875\n",
      "epoch:0 train loss:0.8087226152420044 train acc:0.71875\n",
      "epoch:0 train loss:0.9355493187904358 train acc:0.78125\n",
      "epoch:0 train loss:0.5639945864677429 train acc:0.875\n",
      "epoch:0 train loss:0.3731401860713959 train acc:0.90625\n",
      "epoch:0 train loss:0.41307371854782104 train acc:0.875\n",
      "epoch:0 train loss:0.819843590259552 train acc:0.71875\n",
      "epoch:0 train loss:0.9309338331222534 train acc:0.65625\n",
      "epoch:0 train loss:0.5688704252243042 train acc:0.84375\n",
      "epoch:0 train loss:0.6980339288711548 train acc:0.84375\n",
      "epoch:0 train loss:0.7247148752212524 train acc:0.90625\n",
      "epoch:0 train loss:0.8691709041595459 train acc:0.75\n",
      "epoch:0 train loss:0.40212252736091614 train acc:0.90625\n",
      "epoch:0 train loss:0.9449453949928284 train acc:0.71875\n",
      "epoch:0 train loss:0.40118929743766785 train acc:0.90625\n",
      "epoch:0 train loss:0.7852464914321899 train acc:0.75\n",
      "epoch:0 train loss:0.5244817733764648 train acc:0.9375\n",
      "epoch:0 train loss:0.6310958862304688 train acc:0.84375\n",
      "epoch:0 train loss:0.824016809463501 train acc:0.78125\n",
      "epoch:0 train loss:1.0096499919891357 train acc:0.71875\n",
      "epoch:0 train loss:0.700150728225708 train acc:0.78125\n",
      "epoch:0 train loss:1.0029337406158447 train acc:0.78125\n",
      "epoch:0 train loss:0.8714367151260376 train acc:0.71875\n",
      "epoch:0 train loss:0.5679653286933899 train acc:0.875\n",
      "epoch:0 train loss:0.3042297959327698 train acc:0.9375\n",
      "epoch:0 train loss:0.6098088622093201 train acc:0.8125\n",
      "epoch:0 train loss:0.7634799480438232 train acc:0.75\n",
      "epoch:0 train loss:0.5269149541854858 train acc:0.875\n",
      "epoch:0 train loss:0.8632704615592957 train acc:0.78125\n",
      "epoch:0 train loss:0.8125084638595581 train acc:0.8125\n",
      "epoch:0 train loss:0.7586175203323364 train acc:0.78125\n",
      "epoch:0 train loss:1.2684204578399658 train acc:0.6875\n",
      "epoch:0 train loss:0.4685095548629761 train acc:0.875\n",
      "epoch:0 train loss:0.7470731735229492 train acc:0.84375\n",
      "epoch:0 train loss:0.6810515522956848 train acc:0.78125\n",
      "epoch:0 train loss:0.7245802879333496 train acc:0.84375\n",
      "epoch:0 train loss:0.24786822497844696 train acc:0.9375\n",
      "epoch:0 train loss:0.993389368057251 train acc:0.78125\n",
      "epoch:0 train loss:0.8282780647277832 train acc:0.75\n",
      "epoch:0 train loss:0.44616878032684326 train acc:0.875\n",
      "epoch:0 train loss:0.43686676025390625 train acc:0.8125\n",
      "epoch:0 train loss:0.6430622935295105 train acc:0.84375\n",
      "epoch:0 train loss:0.800838828086853 train acc:0.78125\n",
      "epoch:0 train loss:0.5346015691757202 train acc:0.90625\n",
      "epoch:0 train loss:0.9199709892272949 train acc:0.71875\n",
      "epoch:0 train loss:0.6569085717201233 train acc:0.8125\n",
      "epoch:0 train loss:0.6027594208717346 train acc:0.84375\n",
      "epoch:0 train loss:0.43426835536956787 train acc:0.84375\n",
      "epoch:0 train loss:0.8605326414108276 train acc:0.75\n",
      "epoch:0 train loss:1.3541971445083618 train acc:0.5\n",
      "epoch:0 train loss:0.7079048156738281 train acc:0.78125\n",
      "epoch:0 train loss:0.8574149012565613 train acc:0.71875\n",
      "epoch:0 train loss:0.4746412932872772 train acc:0.84375\n",
      "epoch:0 train loss:0.42187654972076416 train acc:0.90625\n",
      "epoch:0 train loss:0.9916290044784546 train acc:0.78125\n",
      "epoch:0 train loss:0.7698427438735962 train acc:0.84375\n",
      "epoch:0 train loss:0.6025809049606323 train acc:0.84375\n",
      "epoch:0 train loss:1.0880323648452759 train acc:0.65625\n",
      "epoch:0 train loss:0.8449299335479736 train acc:0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train loss:0.5963963270187378 train acc:0.84375\n",
      "epoch:0 train loss:0.8211761713027954 train acc:0.75\n",
      "epoch:0 train loss:0.5019216537475586 train acc:0.90625\n",
      "epoch:0 train loss:0.6706184148788452 train acc:0.8125\n",
      "epoch:0 train loss:0.7715954780578613 train acc:0.6875\n",
      "epoch:0 train loss:0.43808606266975403 train acc:0.8125\n",
      "epoch:0 train loss:0.4784342646598816 train acc:0.875\n",
      "epoch:0 train loss:0.38395926356315613 train acc:0.90625\n",
      "epoch:0 train loss:0.6618663668632507 train acc:0.875\n",
      "epoch:0 train loss:0.4122239947319031 train acc:0.875\n",
      "epoch:0 train loss:0.585814356803894 train acc:0.78125\n",
      "epoch:0 train loss:0.9320248365402222 train acc:0.71875\n",
      "epoch:0 train loss:0.4905889928340912 train acc:0.78125\n",
      "epoch:0 train loss:0.6551468372344971 train acc:0.84375\n",
      "epoch:0 train loss:0.6515697836875916 train acc:0.8125\n",
      "epoch:0 train loss:0.33361953496932983 train acc:0.875\n",
      "epoch:0 train loss:0.2021626979112625 train acc:0.9375\n",
      "epoch:0 train loss:0.5381942987442017 train acc:0.875\n",
      "epoch:0 train loss:0.4731518626213074 train acc:0.875\n",
      "epoch:0 train loss:0.169310063123703 train acc:0.96875\n",
      "epoch:0 train loss:0.3888291120529175 train acc:0.9375\n",
      "epoch:0 train loss:0.5628469586372375 train acc:0.90625\n",
      "epoch:0 train loss:0.45540061593055725 train acc:0.84375\n",
      "epoch:0 train loss:0.8682307004928589 train acc:0.71875\n",
      "epoch:0 train loss:0.6351574063301086 train acc:0.84375\n",
      "epoch:0 train loss:0.6269140243530273 train acc:0.78125\n",
      "epoch:0 train loss:0.5008084177970886 train acc:0.875\n",
      "epoch:0 train loss:0.6143257021903992 train acc:0.84375\n",
      "epoch:0 train loss:0.6021645069122314 train acc:0.8125\n",
      "epoch:0 train loss:0.6011653542518616 train acc:0.8125\n",
      "epoch:0 train loss:0.7348860502243042 train acc:0.8125\n",
      "epoch:0 train loss:0.7374356985092163 train acc:0.8125\n",
      "epoch:0 train loss:0.47577232122421265 train acc:0.8125\n",
      "epoch:0 train loss:0.6698347926139832 train acc:0.75\n",
      "epoch:0 train loss:0.8926324248313904 train acc:0.7333333492279053\n",
      "epoch:1 train loss:0.44030696153640747 train acc:0.84375\n",
      "epoch:1 train loss:0.62331622838974 train acc:0.8125\n",
      "epoch:1 train loss:0.4801035523414612 train acc:0.8125\n",
      "epoch:1 train loss:1.0486778020858765 train acc:0.84375\n",
      "epoch:1 train loss:0.7301580309867859 train acc:0.75\n",
      "epoch:1 train loss:0.5595364570617676 train acc:0.84375\n",
      "epoch:1 train loss:0.42771196365356445 train acc:0.9375\n",
      "epoch:1 train loss:0.7668156623840332 train acc:0.78125\n",
      "epoch:1 train loss:0.903800368309021 train acc:0.71875\n",
      "epoch:1 train loss:0.2827647924423218 train acc:0.9375\n",
      "epoch:1 train loss:0.7407861948013306 train acc:0.78125\n",
      "epoch:1 train loss:0.6286172270774841 train acc:0.78125\n",
      "epoch:1 train loss:0.36663010716438293 train acc:0.90625\n",
      "epoch:1 train loss:0.5813091993331909 train acc:0.8125\n",
      "epoch:1 train loss:0.47871237993240356 train acc:0.90625\n",
      "epoch:1 train loss:0.731442928314209 train acc:0.75\n",
      "epoch:1 train loss:0.4499669671058655 train acc:0.875\n",
      "epoch:1 train loss:0.47258299589157104 train acc:0.84375\n",
      "epoch:1 train loss:0.7144647240638733 train acc:0.78125\n",
      "epoch:1 train loss:0.48988664150238037 train acc:0.90625\n",
      "epoch:1 train loss:0.6414780020713806 train acc:0.8125\n",
      "epoch:1 train loss:0.7914537191390991 train acc:0.78125\n",
      "epoch:1 train loss:0.6608444452285767 train acc:0.8125\n",
      "epoch:1 train loss:0.39212357997894287 train acc:0.9375\n",
      "epoch:1 train loss:0.49264901876449585 train acc:0.8125\n",
      "epoch:1 train loss:0.7931431531906128 train acc:0.71875\n",
      "epoch:1 train loss:0.754270076751709 train acc:0.78125\n",
      "epoch:1 train loss:0.22039783000946045 train acc:0.9375\n",
      "epoch:1 train loss:0.9221801161766052 train acc:0.78125\n",
      "epoch:1 train loss:0.5139801502227783 train acc:0.8125\n",
      "epoch:1 train loss:1.1094862222671509 train acc:0.65625\n",
      "epoch:1 train loss:0.5437907576560974 train acc:0.75\n",
      "epoch:1 train loss:0.40114182233810425 train acc:0.875\n",
      "epoch:1 train loss:0.4384905695915222 train acc:0.84375\n",
      "epoch:1 train loss:0.5076653957366943 train acc:0.84375\n",
      "epoch:1 train loss:1.0485093593597412 train acc:0.78125\n",
      "epoch:1 train loss:0.6186218857765198 train acc:0.78125\n",
      "epoch:1 train loss:0.5946746468544006 train acc:0.90625\n",
      "epoch:1 train loss:0.4820232093334198 train acc:0.84375\n",
      "epoch:1 train loss:0.41874417662620544 train acc:0.90625\n",
      "epoch:1 train loss:0.516806423664093 train acc:0.8125\n",
      "epoch:1 train loss:0.7002604007720947 train acc:0.6875\n",
      "epoch:1 train loss:0.5729503631591797 train acc:0.875\n",
      "epoch:1 train loss:0.7148295044898987 train acc:0.78125\n",
      "epoch:1 train loss:0.43409931659698486 train acc:0.875\n",
      "epoch:1 train loss:0.35411542654037476 train acc:0.9375\n",
      "epoch:1 train loss:0.2770039141178131 train acc:0.875\n",
      "epoch:1 train loss:0.7463259696960449 train acc:0.71875\n",
      "epoch:1 train loss:0.36779654026031494 train acc:0.9375\n",
      "epoch:1 train loss:0.4591066241264343 train acc:0.84375\n",
      "epoch:1 train loss:0.8878286480903625 train acc:0.8125\n",
      "epoch:1 train loss:0.7520761489868164 train acc:0.71875\n",
      "epoch:1 train loss:0.738504946231842 train acc:0.875\n",
      "epoch:1 train loss:0.2965295612812042 train acc:0.9375\n",
      "epoch:1 train loss:0.47916653752326965 train acc:0.84375\n",
      "epoch:1 train loss:0.541053831577301 train acc:0.90625\n",
      "epoch:1 train loss:1.006091833114624 train acc:0.6875\n",
      "epoch:1 train loss:0.6936587691307068 train acc:0.78125\n",
      "epoch:1 train loss:0.4332270324230194 train acc:0.90625\n",
      "epoch:1 train loss:0.552477240562439 train acc:0.875\n",
      "epoch:1 train loss:0.44894468784332275 train acc:0.8125\n",
      "epoch:1 train loss:0.43975818157196045 train acc:0.84375\n",
      "epoch:1 train loss:0.6758506894111633 train acc:0.75\n",
      "epoch:1 train loss:0.586987316608429 train acc:0.875\n",
      "epoch:1 train loss:0.7355217337608337 train acc:0.6875\n",
      "epoch:1 train loss:0.5071494579315186 train acc:0.8125\n",
      "epoch:1 train loss:0.38286542892456055 train acc:0.875\n",
      "epoch:1 train loss:0.7071011066436768 train acc:0.875\n",
      "epoch:1 train loss:0.7724607586860657 train acc:0.75\n",
      "epoch:1 train loss:0.5563128590583801 train acc:0.84375\n",
      "epoch:1 train loss:0.3245736360549927 train acc:0.875\n",
      "epoch:1 train loss:0.5744509100914001 train acc:0.84375\n",
      "epoch:1 train loss:0.544978141784668 train acc:0.78125\n",
      "epoch:1 train loss:0.6454764008522034 train acc:0.84375\n",
      "epoch:1 train loss:0.5293799042701721 train acc:0.8125\n",
      "epoch:1 train loss:0.4432032108306885 train acc:0.875\n",
      "epoch:1 train loss:0.9420057535171509 train acc:0.78125\n",
      "epoch:1 train loss:0.3169021010398865 train acc:0.875\n",
      "epoch:1 train loss:0.6132511496543884 train acc:0.8125\n",
      "epoch:1 train loss:0.7489586472511292 train acc:0.75\n",
      "epoch:1 train loss:0.47047972679138184 train acc:0.875\n",
      "epoch:1 train loss:0.4396715462207794 train acc:0.875\n",
      "epoch:1 train loss:0.681796669960022 train acc:0.75\n",
      "epoch:1 train loss:0.4493250548839569 train acc:0.84375\n",
      "epoch:1 train loss:0.5392761826515198 train acc:0.875\n",
      "epoch:1 train loss:0.6968438625335693 train acc:0.84375\n",
      "epoch:1 train loss:0.6349499821662903 train acc:0.8125\n",
      "epoch:1 train loss:0.4886854887008667 train acc:0.84375\n",
      "epoch:1 train loss:0.27436119318008423 train acc:0.90625\n",
      "epoch:1 train loss:0.46916788816452026 train acc:0.84375\n",
      "epoch:1 train loss:0.512824535369873 train acc:0.875\n",
      "epoch:1 train loss:0.5971047878265381 train acc:0.8125\n",
      "epoch:1 train loss:0.8674259185791016 train acc:0.71875\n",
      "epoch:1 train loss:0.2895511984825134 train acc:0.90625\n",
      "epoch:1 train loss:0.5382639765739441 train acc:0.875\n",
      "epoch:1 train loss:0.37728428840637207 train acc:0.90625\n",
      "epoch:1 train loss:0.8395837545394897 train acc:0.71875\n",
      "epoch:1 train loss:0.4472585916519165 train acc:0.875\n",
      "epoch:1 train loss:0.4176208972930908 train acc:0.90625\n",
      "epoch:1 train loss:0.5474256277084351 train acc:0.84375\n",
      "epoch:1 train loss:0.405571311712265 train acc:0.875\n",
      "epoch:1 train loss:0.6147946715354919 train acc:0.875\n",
      "epoch:1 train loss:0.6839691400527954 train acc:0.71875\n",
      "epoch:1 train loss:0.6337530016899109 train acc:0.75\n",
      "epoch:1 train loss:0.4842154383659363 train acc:0.8125\n",
      "epoch:1 train loss:0.22872449457645416 train acc:0.90625\n",
      "epoch:1 train loss:0.787691056728363 train acc:0.71875\n",
      "epoch:1 train loss:0.5356127023696899 train acc:0.8125\n",
      "epoch:1 train loss:0.6606848835945129 train acc:0.8125\n",
      "epoch:1 train loss:0.33303120732307434 train acc:0.90625\n",
      "epoch:1 train loss:0.857925295829773 train acc:0.6875\n",
      "epoch:1 train loss:0.40758073329925537 train acc:0.90625\n",
      "epoch:1 train loss:0.5213462114334106 train acc:0.84375\n",
      "epoch:1 train loss:0.5938931703567505 train acc:0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 train loss:0.4279870390892029 train acc:0.84375\n",
      "epoch:1 train loss:0.9652562141418457 train acc:0.78125\n",
      "epoch:1 train loss:0.55027174949646 train acc:0.875\n",
      "epoch:1 train loss:0.6169273257255554 train acc:0.84375\n",
      "epoch:1 train loss:0.5075658559799194 train acc:0.78125\n",
      "epoch:1 train loss:0.6409026384353638 train acc:0.71875\n",
      "epoch:1 train loss:0.40410709381103516 train acc:0.875\n",
      "epoch:1 train loss:0.7730976343154907 train acc:0.78125\n",
      "epoch:1 train loss:0.7940889596939087 train acc:0.75\n",
      "epoch:1 train loss:0.54334557056427 train acc:0.875\n",
      "epoch:1 train loss:0.20577239990234375 train acc:0.9375\n",
      "epoch:1 train loss:0.5469280481338501 train acc:0.8125\n",
      "epoch:1 train loss:0.8775299787521362 train acc:0.78125\n",
      "epoch:1 train loss:0.5251399278640747 train acc:0.90625\n",
      "epoch:1 train loss:0.9182649850845337 train acc:0.8125\n",
      "epoch:1 train loss:0.8646360039710999 train acc:0.78125\n",
      "epoch:1 train loss:0.4507145285606384 train acc:0.8125\n",
      "epoch:1 train loss:0.8655941486358643 train acc:0.65625\n",
      "epoch:1 train loss:0.5821419954299927 train acc:0.84375\n",
      "epoch:1 train loss:0.6063570976257324 train acc:0.8125\n",
      "epoch:1 train loss:0.34252357482910156 train acc:0.875\n",
      "epoch:1 train loss:0.583219051361084 train acc:0.8125\n",
      "epoch:1 train loss:0.475538045167923 train acc:0.875\n",
      "epoch:1 train loss:0.5520927309989929 train acc:0.84375\n",
      "epoch:1 train loss:0.47985345125198364 train acc:0.875\n",
      "epoch:1 train loss:0.6057889461517334 train acc:0.8125\n",
      "epoch:1 train loss:0.21049366891384125 train acc:0.96875\n",
      "epoch:1 train loss:0.7630468010902405 train acc:0.84375\n",
      "epoch:1 train loss:0.539916455745697 train acc:0.78125\n",
      "epoch:1 train loss:0.5652490854263306 train acc:0.84375\n",
      "epoch:1 train loss:0.5537341833114624 train acc:0.875\n",
      "epoch:1 train loss:0.5542484521865845 train acc:0.75\n",
      "epoch:1 train loss:0.7333282232284546 train acc:0.75\n",
      "epoch:1 train loss:0.3447079360485077 train acc:0.875\n",
      "epoch:1 train loss:0.213907390832901 train acc:0.96875\n",
      "epoch:1 train loss:0.5131580233573914 train acc:0.84375\n",
      "epoch:1 train loss:0.1666863113641739 train acc:0.96875\n",
      "epoch:1 train loss:0.7132235765457153 train acc:0.75\n",
      "epoch:1 train loss:0.4295389950275421 train acc:0.9375\n",
      "epoch:1 train loss:0.44539812207221985 train acc:0.875\n",
      "epoch:1 train loss:0.5533271431922913 train acc:0.8125\n",
      "epoch:1 train loss:0.7939976453781128 train acc:0.75\n",
      "epoch:1 train loss:0.9203252196311951 train acc:0.75\n",
      "epoch:1 train loss:0.6882563829421997 train acc:0.78125\n",
      "epoch:1 train loss:0.6543488502502441 train acc:0.8125\n",
      "epoch:1 train loss:0.2844473123550415 train acc:1.0\n",
      "epoch:1 train loss:0.4059007167816162 train acc:0.90625\n",
      "epoch:1 train loss:0.7294906377792358 train acc:0.8125\n",
      "epoch:1 train loss:0.5294406414031982 train acc:0.8125\n",
      "epoch:1 train loss:0.5318721532821655 train acc:0.84375\n",
      "epoch:1 train loss:0.3655575215816498 train acc:0.84375\n",
      "epoch:1 train loss:0.5104200839996338 train acc:0.875\n",
      "epoch:1 train loss:0.45996344089508057 train acc:0.84375\n",
      "epoch:1 train loss:0.3771159052848816 train acc:0.90625\n",
      "epoch:1 train loss:0.4182702898979187 train acc:0.8125\n",
      "epoch:1 train loss:0.4372008740901947 train acc:0.875\n",
      "epoch:1 train loss:0.4158225953578949 train acc:0.875\n",
      "epoch:1 train loss:0.39992964267730713 train acc:0.78125\n",
      "epoch:1 train loss:0.5999244451522827 train acc:0.84375\n",
      "epoch:1 train loss:0.3878462314605713 train acc:0.875\n",
      "epoch:1 train loss:0.7098642587661743 train acc:0.78125\n",
      "epoch:1 train loss:0.4001867175102234 train acc:0.90625\n",
      "epoch:1 train loss:0.2769886553287506 train acc:0.96875\n",
      "epoch:1 train loss:0.8596731424331665 train acc:0.78125\n",
      "epoch:1 train loss:0.49259528517723083 train acc:0.84375\n",
      "epoch:1 train loss:0.2818623185157776 train acc:0.90625\n",
      "epoch:1 train loss:0.240371972322464 train acc:0.875\n",
      "epoch:1 train loss:0.8688180446624756 train acc:0.78125\n",
      "epoch:1 train loss:0.3896322548389435 train acc:0.90625\n",
      "epoch:1 train loss:0.7811179757118225 train acc:0.71875\n",
      "epoch:1 train loss:0.5746185779571533 train acc:0.875\n",
      "epoch:1 train loss:0.13856729865074158 train acc:0.9375\n",
      "epoch:1 train loss:0.41058364510536194 train acc:0.84375\n",
      "epoch:1 train loss:0.452632874250412 train acc:0.875\n",
      "epoch:1 train loss:0.35478129982948303 train acc:0.90625\n",
      "epoch:1 train loss:0.6213310956954956 train acc:0.8125\n",
      "epoch:1 train loss:0.343647837638855 train acc:0.90625\n",
      "epoch:1 train loss:0.6303500533103943 train acc:0.875\n",
      "epoch:1 train loss:0.8354685306549072 train acc:0.71875\n",
      "epoch:1 train loss:0.31852802634239197 train acc:0.90625\n",
      "epoch:1 train loss:0.3816141188144684 train acc:0.78125\n",
      "epoch:1 train loss:0.6694878935813904 train acc:0.78125\n",
      "epoch:1 train loss:0.25575241446495056 train acc:0.9375\n",
      "epoch:1 train loss:0.3407488167285919 train acc:0.84375\n",
      "epoch:1 train loss:0.4497896134853363 train acc:0.875\n",
      "epoch:1 train loss:0.3633580803871155 train acc:0.875\n",
      "epoch:1 train loss:0.7571190595626831 train acc:0.78125\n",
      "epoch:1 train loss:0.4008697271347046 train acc:0.84375\n",
      "epoch:1 train loss:0.6152878999710083 train acc:0.8125\n",
      "epoch:1 train loss:0.37664663791656494 train acc:0.875\n",
      "epoch:1 train loss:0.4272565245628357 train acc:0.8125\n",
      "epoch:1 train loss:0.33768266439437866 train acc:0.90625\n",
      "epoch:1 train loss:0.4282863438129425 train acc:0.875\n",
      "epoch:1 train loss:0.5014163255691528 train acc:0.78125\n",
      "epoch:1 train loss:0.2628706991672516 train acc:0.90625\n",
      "epoch:1 train loss:0.4506847858428955 train acc:0.84375\n",
      "epoch:1 train loss:0.34821951389312744 train acc:0.84375\n",
      "epoch:1 train loss:0.6155598759651184 train acc:0.84375\n",
      "epoch:1 train loss:0.22083553671836853 train acc:0.96875\n",
      "epoch:1 train loss:0.4721413552761078 train acc:0.84375\n",
      "epoch:1 train loss:0.39981022477149963 train acc:0.9375\n",
      "epoch:1 train loss:0.381822407245636 train acc:0.875\n",
      "epoch:1 train loss:0.44647088646888733 train acc:0.84375\n",
      "epoch:1 train loss:0.5462149381637573 train acc:0.8125\n",
      "epoch:1 train loss:0.23282182216644287 train acc:0.96875\n",
      "epoch:1 train loss:0.4068538248538971 train acc:0.84375\n",
      "epoch:1 train loss:0.5793386697769165 train acc:0.6875\n",
      "epoch:1 train loss:0.4450579285621643 train acc:0.875\n",
      "epoch:1 train loss:0.42098096013069153 train acc:0.90625\n",
      "epoch:1 train loss:0.36073100566864014 train acc:0.90625\n",
      "epoch:1 train loss:0.5067999362945557 train acc:0.875\n",
      "epoch:1 train loss:0.5773953199386597 train acc:0.875\n",
      "epoch:1 train loss:0.9372279644012451 train acc:0.8125\n",
      "epoch:1 train loss:0.7790829539299011 train acc:0.84375\n",
      "epoch:1 train loss:0.5324223041534424 train acc:0.90625\n",
      "epoch:1 train loss:0.6605358123779297 train acc:0.84375\n",
      "epoch:1 train loss:0.5662345886230469 train acc:0.875\n",
      "epoch:1 train loss:0.4743598699569702 train acc:0.8125\n",
      "epoch:1 train loss:0.31555530428886414 train acc:0.90625\n",
      "epoch:1 train loss:0.5491822957992554 train acc:0.78125\n",
      "epoch:1 train loss:0.302068293094635 train acc:0.90625\n",
      "epoch:1 train loss:0.5784955024719238 train acc:0.875\n",
      "epoch:1 train loss:0.4250730872154236 train acc:0.875\n",
      "epoch:1 train loss:0.5809019207954407 train acc:0.8125\n",
      "epoch:1 train loss:0.4056832492351532 train acc:0.84375\n",
      "epoch:1 train loss:0.672372579574585 train acc:0.78125\n",
      "epoch:1 train loss:0.3493505120277405 train acc:0.875\n",
      "epoch:1 train loss:0.5182392597198486 train acc:0.84375\n",
      "epoch:1 train loss:0.3087472915649414 train acc:0.90625\n",
      "epoch:1 train loss:0.35093963146209717 train acc:0.96875\n",
      "epoch:1 train loss:0.6001535654067993 train acc:0.84375\n",
      "epoch:1 train loss:0.789544939994812 train acc:0.71875\n",
      "epoch:1 train loss:0.5975381135940552 train acc:0.875\n",
      "epoch:1 train loss:0.36248376965522766 train acc:0.875\n",
      "epoch:1 train loss:0.2536936104297638 train acc:0.90625\n",
      "epoch:1 train loss:0.456010639667511 train acc:0.875\n",
      "epoch:1 train loss:0.6568412780761719 train acc:0.78125\n",
      "epoch:1 train loss:0.7830115556716919 train acc:0.71875\n",
      "epoch:1 train loss:0.631303608417511 train acc:0.8125\n",
      "epoch:1 train loss:0.45313072204589844 train acc:0.875\n",
      "epoch:1 train loss:0.5309507846832275 train acc:0.78125\n",
      "epoch:1 train loss:0.6749525666236877 train acc:0.78125\n",
      "epoch:1 train loss:0.45288166403770447 train acc:0.90625\n",
      "epoch:1 train loss:0.5785247087478638 train acc:0.84375\n",
      "epoch:1 train loss:0.30776068568229675 train acc:0.90625\n",
      "epoch:1 train loss:0.6178416609764099 train acc:0.78125\n",
      "epoch:1 train loss:0.7472062110900879 train acc:0.75\n",
      "epoch:1 train loss:0.3915209174156189 train acc:0.875\n",
      "epoch:1 train loss:0.7136587500572205 train acc:0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 train loss:0.5835822820663452 train acc:0.78125\n",
      "epoch:1 train loss:0.4335326850414276 train acc:0.84375\n",
      "epoch:1 train loss:0.2424081563949585 train acc:0.9375\n",
      "epoch:1 train loss:0.3922238349914551 train acc:0.84375\n",
      "epoch:1 train loss:0.3844373822212219 train acc:0.90625\n",
      "epoch:1 train loss:0.28517478704452515 train acc:0.96875\n",
      "epoch:1 train loss:0.6553899049758911 train acc:0.75\n",
      "epoch:1 train loss:0.5927942991256714 train acc:0.84375\n",
      "epoch:1 train loss:0.8510934114456177 train acc:0.78125\n",
      "epoch:1 train loss:0.4009471833705902 train acc:0.84375\n",
      "epoch:1 train loss:0.38687583804130554 train acc:0.84375\n",
      "epoch:1 train loss:0.5488660335540771 train acc:0.84375\n",
      "epoch:1 train loss:0.7210047245025635 train acc:0.75\n",
      "epoch:1 train loss:0.8172731399536133 train acc:0.71875\n",
      "epoch:1 train loss:0.40061530470848083 train acc:0.84375\n",
      "epoch:1 train loss:0.37977176904678345 train acc:0.84375\n",
      "epoch:1 train loss:0.6104789972305298 train acc:0.84375\n",
      "epoch:1 train loss:0.7295547723770142 train acc:0.75\n",
      "epoch:1 train loss:0.5755851864814758 train acc:0.75\n",
      "epoch:1 train loss:0.47383302450180054 train acc:0.875\n",
      "epoch:1 train loss:0.4147805869579315 train acc:0.84375\n",
      "epoch:1 train loss:0.39297911524772644 train acc:0.84375\n",
      "epoch:1 train loss:0.6783288717269897 train acc:0.75\n",
      "epoch:1 train loss:0.5373334288597107 train acc:0.875\n",
      "epoch:1 train loss:0.4942290782928467 train acc:0.84375\n",
      "epoch:1 train loss:0.42916059494018555 train acc:0.78125\n",
      "epoch:1 train loss:0.33346420526504517 train acc:0.96875\n",
      "epoch:1 train loss:0.45449942350387573 train acc:0.90625\n",
      "epoch:1 train loss:0.303703635931015 train acc:0.875\n",
      "epoch:1 train loss:0.21931898593902588 train acc:0.90625\n",
      "epoch:1 train loss:0.2600354254245758 train acc:0.96875\n",
      "epoch:1 train loss:0.832982063293457 train acc:0.75\n",
      "epoch:1 train loss:0.4996090531349182 train acc:0.875\n",
      "epoch:1 train loss:0.4653344452381134 train acc:0.90625\n",
      "epoch:1 train loss:0.25080406665802 train acc:0.90625\n",
      "epoch:1 train loss:0.5378746390342712 train acc:0.8125\n",
      "epoch:1 train loss:0.4522496461868286 train acc:0.84375\n",
      "epoch:1 train loss:0.49559083580970764 train acc:0.84375\n",
      "epoch:1 train loss:0.4230099618434906 train acc:0.875\n",
      "epoch:1 train loss:0.4774898886680603 train acc:0.84375\n",
      "epoch:1 train loss:0.49109938740730286 train acc:0.90625\n",
      "epoch:1 train loss:0.2382182478904724 train acc:0.96875\n",
      "epoch:1 train loss:0.3396875262260437 train acc:0.875\n",
      "epoch:1 train loss:0.395681768655777 train acc:0.9375\n",
      "epoch:1 train loss:0.5877951383590698 train acc:0.8125\n",
      "epoch:1 train loss:0.655944287776947 train acc:0.8125\n",
      "epoch:1 train loss:0.3769916892051697 train acc:0.875\n",
      "epoch:1 train loss:0.31435447931289673 train acc:0.90625\n",
      "epoch:1 train loss:0.20439542829990387 train acc:0.90625\n",
      "epoch:1 train loss:0.4239530861377716 train acc:0.875\n",
      "epoch:1 train loss:0.7934156656265259 train acc:0.78125\n",
      "epoch:1 train loss:0.5510836243629456 train acc:0.8125\n",
      "epoch:1 train loss:0.4948466122150421 train acc:0.875\n",
      "epoch:1 train loss:0.2097628116607666 train acc:0.96875\n",
      "epoch:1 train loss:0.23776890337467194 train acc:0.96875\n",
      "epoch:1 train loss:0.31047624349594116 train acc:0.90625\n",
      "epoch:1 train loss:0.6141679883003235 train acc:0.8125\n",
      "epoch:1 train loss:0.3071690797805786 train acc:0.90625\n",
      "epoch:1 train loss:0.35216718912124634 train acc:0.875\n",
      "epoch:1 train loss:0.5751277208328247 train acc:0.78125\n",
      "epoch:1 train loss:0.696570098400116 train acc:0.78125\n",
      "epoch:1 train loss:0.43792518973350525 train acc:0.90625\n",
      "epoch:1 train loss:0.3655990958213806 train acc:0.90625\n",
      "epoch:1 train loss:0.7286977767944336 train acc:0.78125\n",
      "epoch:1 train loss:0.4024731516838074 train acc:0.90625\n",
      "epoch:1 train loss:0.82353276014328 train acc:0.75\n",
      "epoch:1 train loss:0.6849151849746704 train acc:0.78125\n",
      "epoch:1 train loss:0.31029969453811646 train acc:0.96875\n",
      "epoch:1 train loss:0.6616842150688171 train acc:0.78125\n",
      "epoch:1 train loss:0.3123544454574585 train acc:0.9375\n",
      "epoch:1 train loss:0.5165194272994995 train acc:0.8999999761581421\n",
      "epoch:2 train loss:0.40331709384918213 train acc:0.9375\n",
      "epoch:2 train loss:0.22863832116127014 train acc:0.9375\n",
      "epoch:2 train loss:0.4168134033679962 train acc:0.875\n",
      "epoch:2 train loss:0.29307496547698975 train acc:0.90625\n",
      "epoch:2 train loss:0.3477019965648651 train acc:0.875\n",
      "epoch:2 train loss:0.20544850826263428 train acc:0.96875\n",
      "epoch:2 train loss:0.2891755700111389 train acc:0.90625\n",
      "epoch:2 train loss:0.23362430930137634 train acc:0.9375\n",
      "epoch:2 train loss:0.35461556911468506 train acc:0.9375\n",
      "epoch:2 train loss:0.4227920174598694 train acc:0.90625\n",
      "epoch:2 train loss:0.2833441495895386 train acc:0.875\n",
      "epoch:2 train loss:0.4647495448589325 train acc:0.875\n",
      "epoch:2 train loss:0.5016040802001953 train acc:0.9375\n",
      "epoch:2 train loss:0.33944034576416016 train acc:0.9375\n",
      "epoch:2 train loss:0.3394768536090851 train acc:0.90625\n",
      "epoch:2 train loss:0.41053253412246704 train acc:0.875\n",
      "epoch:2 train loss:0.3941609561443329 train acc:0.90625\n",
      "epoch:2 train loss:0.45476508140563965 train acc:0.84375\n",
      "epoch:2 train loss:0.5539767742156982 train acc:0.75\n",
      "epoch:2 train loss:0.39742928743362427 train acc:0.84375\n",
      "epoch:2 train loss:0.5342716574668884 train acc:0.9375\n",
      "epoch:2 train loss:0.6150580644607544 train acc:0.8125\n",
      "epoch:2 train loss:0.5833384394645691 train acc:0.8125\n",
      "epoch:2 train loss:0.2084161937236786 train acc:0.875\n",
      "epoch:2 train loss:0.45532992482185364 train acc:0.875\n",
      "epoch:2 train loss:0.639304518699646 train acc:0.8125\n",
      "epoch:2 train loss:0.19868804514408112 train acc:0.96875\n",
      "epoch:2 train loss:0.28969672322273254 train acc:0.9375\n",
      "epoch:2 train loss:0.3979106545448303 train acc:0.78125\n",
      "epoch:2 train loss:0.6179218292236328 train acc:0.71875\n",
      "epoch:2 train loss:0.20127373933792114 train acc:0.9375\n",
      "epoch:2 train loss:0.43169593811035156 train acc:0.875\n",
      "epoch:2 train loss:0.3043045103549957 train acc:0.9375\n",
      "epoch:2 train loss:0.2818785011768341 train acc:0.90625\n",
      "epoch:2 train loss:0.5163817405700684 train acc:0.90625\n",
      "epoch:2 train loss:0.2683413326740265 train acc:0.90625\n",
      "epoch:2 train loss:0.4528708755970001 train acc:0.84375\n",
      "epoch:2 train loss:0.32480308413505554 train acc:0.875\n",
      "epoch:2 train loss:0.3606806695461273 train acc:0.875\n",
      "epoch:2 train loss:0.3871036767959595 train acc:0.84375\n",
      "epoch:2 train loss:0.46885940432548523 train acc:0.8125\n",
      "epoch:2 train loss:0.20500388741493225 train acc:0.9375\n",
      "epoch:2 train loss:0.6754838824272156 train acc:0.75\n",
      "epoch:2 train loss:0.9329720139503479 train acc:0.6875\n",
      "epoch:2 train loss:0.2504481077194214 train acc:0.90625\n",
      "epoch:2 train loss:0.7416133880615234 train acc:0.875\n",
      "epoch:2 train loss:0.30754557251930237 train acc:0.875\n",
      "epoch:2 train loss:0.3119661808013916 train acc:0.9375\n",
      "epoch:2 train loss:0.36772850155830383 train acc:0.875\n",
      "epoch:2 train loss:0.5457761287689209 train acc:0.78125\n",
      "epoch:2 train loss:0.25917333364486694 train acc:0.90625\n",
      "epoch:2 train loss:0.5829704999923706 train acc:0.8125\n",
      "epoch:2 train loss:0.4508856236934662 train acc:0.84375\n",
      "epoch:2 train loss:0.25571489334106445 train acc:0.90625\n",
      "epoch:2 train loss:0.3463878631591797 train acc:0.90625\n",
      "epoch:2 train loss:0.38151639699935913 train acc:0.875\n",
      "epoch:2 train loss:0.6943825483322144 train acc:0.875\n",
      "epoch:2 train loss:0.5417089462280273 train acc:0.84375\n",
      "epoch:2 train loss:0.1747519075870514 train acc:0.9375\n",
      "epoch:2 train loss:0.5275533199310303 train acc:0.84375\n",
      "epoch:2 train loss:0.3917466700077057 train acc:0.9375\n",
      "epoch:2 train loss:0.2450554519891739 train acc:0.9375\n",
      "epoch:2 train loss:0.7693548202514648 train acc:0.75\n",
      "epoch:2 train loss:0.5872690677642822 train acc:0.84375\n",
      "epoch:2 train loss:0.2884661555290222 train acc:0.875\n",
      "epoch:2 train loss:0.2917769253253937 train acc:0.90625\n",
      "epoch:2 train loss:0.9037542939186096 train acc:0.8125\n",
      "epoch:2 train loss:0.5368039608001709 train acc:0.875\n",
      "epoch:2 train loss:0.2808642089366913 train acc:0.9375\n",
      "epoch:2 train loss:0.380180299282074 train acc:0.90625\n",
      "epoch:2 train loss:0.46512100100517273 train acc:0.90625\n",
      "epoch:2 train loss:0.30697816610336304 train acc:0.875\n",
      "epoch:2 train loss:0.5369083881378174 train acc:0.84375\n",
      "epoch:2 train loss:0.4930594861507416 train acc:0.875\n",
      "epoch:2 train loss:0.2098323106765747 train acc:0.96875\n",
      "epoch:2 train loss:0.40607479214668274 train acc:0.84375\n",
      "epoch:2 train loss:0.5745867490768433 train acc:0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 train loss:0.49814069271087646 train acc:0.84375\n",
      "epoch:2 train loss:0.2963029742240906 train acc:0.90625\n",
      "epoch:2 train loss:0.14680488407611847 train acc:0.96875\n",
      "epoch:2 train loss:0.6505212783813477 train acc:0.71875\n",
      "epoch:2 train loss:0.6657940745353699 train acc:0.8125\n",
      "epoch:2 train loss:0.1528392881155014 train acc:0.96875\n",
      "epoch:2 train loss:0.4459685683250427 train acc:0.875\n",
      "epoch:2 train loss:0.21923595666885376 train acc:0.9375\n",
      "epoch:2 train loss:0.3781384825706482 train acc:0.875\n",
      "epoch:2 train loss:0.23211149871349335 train acc:0.9375\n",
      "epoch:2 train loss:0.6541532874107361 train acc:0.84375\n",
      "epoch:2 train loss:0.5655068159103394 train acc:0.875\n",
      "epoch:2 train loss:0.6344137191772461 train acc:0.78125\n",
      "epoch:2 train loss:0.37175658345222473 train acc:0.84375\n",
      "epoch:2 train loss:0.6462269425392151 train acc:0.78125\n",
      "epoch:2 train loss:0.4482007920742035 train acc:0.875\n",
      "epoch:2 train loss:0.3129025101661682 train acc:0.90625\n",
      "epoch:2 train loss:0.4280792474746704 train acc:0.875\n",
      "epoch:2 train loss:0.35385406017303467 train acc:0.875\n",
      "epoch:2 train loss:0.53379225730896 train acc:0.84375\n",
      "epoch:2 train loss:0.49798649549484253 train acc:0.8125\n",
      "epoch:2 train loss:0.33200252056121826 train acc:0.90625\n",
      "epoch:2 train loss:0.5126626491546631 train acc:0.8125\n",
      "epoch:2 train loss:0.4130125045776367 train acc:0.84375\n",
      "epoch:2 train loss:0.5296471118927002 train acc:0.8125\n",
      "epoch:2 train loss:0.40691572427749634 train acc:0.90625\n",
      "epoch:2 train loss:0.4522947669029236 train acc:0.84375\n",
      "epoch:2 train loss:0.43513330817222595 train acc:0.90625\n",
      "epoch:2 train loss:0.4081149697303772 train acc:0.875\n",
      "epoch:2 train loss:0.45117032527923584 train acc:0.84375\n",
      "epoch:2 train loss:0.35085004568099976 train acc:0.875\n",
      "epoch:2 train loss:0.2919636368751526 train acc:0.875\n",
      "epoch:2 train loss:0.41419920325279236 train acc:0.90625\n",
      "epoch:2 train loss:0.5643597841262817 train acc:0.78125\n",
      "epoch:2 train loss:0.5372724533081055 train acc:0.84375\n",
      "epoch:2 train loss:0.8053930997848511 train acc:0.75\n",
      "epoch:2 train loss:0.5335158109664917 train acc:0.84375\n",
      "epoch:2 train loss:0.5895010232925415 train acc:0.84375\n",
      "epoch:2 train loss:0.32822591066360474 train acc:0.90625\n",
      "epoch:2 train loss:0.2549588084220886 train acc:0.9375\n",
      "epoch:2 train loss:0.29184383153915405 train acc:0.90625\n",
      "epoch:2 train loss:0.6373618841171265 train acc:0.84375\n",
      "epoch:2 train loss:0.7689592838287354 train acc:0.875\n",
      "epoch:2 train loss:0.4729541540145874 train acc:0.8125\n",
      "epoch:2 train loss:0.5768625736236572 train acc:0.8125\n",
      "epoch:2 train loss:0.5391849875450134 train acc:0.8125\n",
      "epoch:2 train loss:0.25254684686660767 train acc:0.9375\n",
      "epoch:2 train loss:0.46126875281333923 train acc:0.8125\n",
      "epoch:2 train loss:0.5451397895812988 train acc:0.84375\n",
      "epoch:2 train loss:0.3225904107093811 train acc:0.90625\n",
      "epoch:2 train loss:0.23628346621990204 train acc:0.96875\n",
      "epoch:2 train loss:0.5393928289413452 train acc:0.78125\n",
      "epoch:2 train loss:0.3867282271385193 train acc:0.90625\n",
      "epoch:2 train loss:0.40309181809425354 train acc:0.875\n",
      "epoch:2 train loss:0.3351685404777527 train acc:0.9375\n",
      "epoch:2 train loss:0.4742932617664337 train acc:0.8125\n",
      "epoch:2 train loss:0.2553194463253021 train acc:0.90625\n",
      "epoch:2 train loss:0.5500966310501099 train acc:0.875\n",
      "epoch:2 train loss:0.3625384271144867 train acc:0.875\n",
      "epoch:2 train loss:0.5346237421035767 train acc:0.8125\n",
      "epoch:2 train loss:0.42303958535194397 train acc:0.84375\n",
      "epoch:2 train loss:0.3779408633708954 train acc:0.9375\n",
      "epoch:2 train loss:0.4253631830215454 train acc:0.875\n",
      "epoch:2 train loss:0.24428844451904297 train acc:0.96875\n",
      "epoch:2 train loss:0.48983892798423767 train acc:0.84375\n",
      "epoch:2 train loss:0.5347070097923279 train acc:0.8125\n",
      "epoch:2 train loss:0.4114173650741577 train acc:0.84375\n",
      "epoch:2 train loss:0.5469765067100525 train acc:0.875\n",
      "epoch:2 train loss:0.31557509303092957 train acc:0.875\n",
      "epoch:2 train loss:0.217191681265831 train acc:0.90625\n",
      "epoch:2 train loss:0.321515291929245 train acc:0.875\n",
      "epoch:2 train loss:0.32516056299209595 train acc:0.90625\n",
      "epoch:2 train loss:0.2790376842021942 train acc:0.9375\n",
      "epoch:2 train loss:0.8353634476661682 train acc:0.78125\n",
      "epoch:2 train loss:0.4051979184150696 train acc:0.90625\n",
      "epoch:2 train loss:0.31685343384742737 train acc:0.90625\n",
      "epoch:2 train loss:0.6417809724807739 train acc:0.8125\n",
      "epoch:2 train loss:0.7115985751152039 train acc:0.8125\n",
      "epoch:2 train loss:0.2842715382575989 train acc:0.90625\n",
      "epoch:2 train loss:0.5453555583953857 train acc:0.875\n",
      "epoch:2 train loss:0.3803391456604004 train acc:0.90625\n",
      "epoch:2 train loss:0.15647350251674652 train acc:0.96875\n",
      "epoch:2 train loss:0.26101914048194885 train acc:0.90625\n",
      "epoch:2 train loss:0.7020409107208252 train acc:0.84375\n",
      "epoch:2 train loss:0.500647783279419 train acc:0.875\n",
      "epoch:2 train loss:0.31874361634254456 train acc:0.90625\n",
      "epoch:2 train loss:0.4719996452331543 train acc:0.875\n",
      "epoch:2 train loss:0.639726459980011 train acc:0.84375\n",
      "epoch:2 train loss:0.5784223675727844 train acc:0.75\n",
      "epoch:2 train loss:0.6124036312103271 train acc:0.84375\n",
      "epoch:2 train loss:0.3383152484893799 train acc:0.875\n",
      "epoch:2 train loss:1.0706439018249512 train acc:0.59375\n",
      "epoch:2 train loss:0.7369803190231323 train acc:0.71875\n",
      "epoch:2 train loss:0.5827620625495911 train acc:0.84375\n",
      "epoch:2 train loss:0.5097696185112 train acc:0.875\n",
      "epoch:2 train loss:0.32446086406707764 train acc:0.90625\n",
      "epoch:2 train loss:0.5953201651573181 train acc:0.84375\n",
      "epoch:2 train loss:0.37656915187835693 train acc:0.90625\n",
      "epoch:2 train loss:0.47259050607681274 train acc:0.875\n",
      "epoch:2 train loss:0.4616004526615143 train acc:0.8125\n",
      "epoch:2 train loss:0.6946231722831726 train acc:0.84375\n",
      "epoch:2 train loss:0.4713713526725769 train acc:0.875\n",
      "epoch:2 train loss:0.7589272856712341 train acc:0.75\n",
      "epoch:2 train loss:0.39472538232803345 train acc:0.84375\n",
      "epoch:2 train loss:0.20768459141254425 train acc:0.96875\n",
      "epoch:2 train loss:0.4329496920108795 train acc:0.90625\n",
      "epoch:2 train loss:0.49781787395477295 train acc:0.9375\n",
      "epoch:2 train loss:0.6496669054031372 train acc:0.75\n",
      "epoch:2 train loss:0.34104639291763306 train acc:0.90625\n",
      "epoch:2 train loss:0.5081874132156372 train acc:0.8125\n",
      "epoch:2 train loss:0.3979356586933136 train acc:0.875\n",
      "epoch:2 train loss:0.297762393951416 train acc:0.90625\n",
      "epoch:2 train loss:0.28835931420326233 train acc:0.9375\n",
      "epoch:2 train loss:0.41564130783081055 train acc:0.90625\n",
      "epoch:2 train loss:0.2689124643802643 train acc:0.9375\n",
      "epoch:2 train loss:0.5906415581703186 train acc:0.6875\n",
      "epoch:2 train loss:0.4693670868873596 train acc:0.90625\n",
      "epoch:2 train loss:0.4792844355106354 train acc:0.8125\n",
      "epoch:2 train loss:0.2658328711986542 train acc:1.0\n",
      "epoch:2 train loss:0.8064773082733154 train acc:0.78125\n",
      "epoch:2 train loss:0.5634512901306152 train acc:0.75\n",
      "epoch:2 train loss:0.24821512401103973 train acc:0.9375\n",
      "epoch:2 train loss:0.342970073223114 train acc:0.875\n",
      "epoch:2 train loss:0.2805582284927368 train acc:0.875\n",
      "epoch:2 train loss:0.3844720125198364 train acc:0.875\n",
      "epoch:2 train loss:0.20151621103286743 train acc:0.90625\n",
      "epoch:2 train loss:0.3662017583847046 train acc:0.90625\n",
      "epoch:2 train loss:0.3942459225654602 train acc:0.875\n",
      "epoch:2 train loss:0.08354005217552185 train acc:0.96875\n",
      "epoch:2 train loss:0.5933160185813904 train acc:0.8125\n",
      "epoch:2 train loss:0.48908716440200806 train acc:0.875\n",
      "epoch:2 train loss:0.5446144342422485 train acc:0.78125\n",
      "epoch:2 train loss:0.2025458812713623 train acc:0.9375\n",
      "epoch:2 train loss:0.39702489972114563 train acc:0.90625\n",
      "epoch:2 train loss:0.31892433762550354 train acc:0.875\n",
      "epoch:2 train loss:0.6259251832962036 train acc:0.8125\n",
      "epoch:2 train loss:0.32958537340164185 train acc:0.875\n",
      "epoch:2 train loss:0.24041545391082764 train acc:0.9375\n",
      "epoch:2 train loss:0.22257651388645172 train acc:0.96875\n",
      "epoch:2 train loss:0.3739200234413147 train acc:0.90625\n",
      "epoch:2 train loss:0.5077188611030579 train acc:0.8125\n",
      "epoch:2 train loss:0.45063352584838867 train acc:0.8125\n",
      "epoch:2 train loss:0.4082781970500946 train acc:0.8125\n",
      "epoch:2 train loss:0.5079549551010132 train acc:0.78125\n",
      "epoch:2 train loss:0.1866689771413803 train acc:0.9375\n",
      "epoch:2 train loss:0.36177897453308105 train acc:0.875\n",
      "epoch:2 train loss:0.5369604229927063 train acc:0.84375\n",
      "epoch:2 train loss:0.28175511956214905 train acc:0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 train loss:0.7312176823616028 train acc:0.8125\n",
      "epoch:2 train loss:0.2861403822898865 train acc:0.9375\n",
      "epoch:2 train loss:0.3966115415096283 train acc:0.90625\n",
      "epoch:2 train loss:0.14123094081878662 train acc:0.96875\n",
      "epoch:2 train loss:0.4482728838920593 train acc:0.90625\n",
      "epoch:2 train loss:0.6540812253952026 train acc:0.8125\n",
      "epoch:2 train loss:0.37195074558258057 train acc:0.90625\n",
      "epoch:2 train loss:0.3916793167591095 train acc:0.875\n",
      "epoch:2 train loss:0.5190736055374146 train acc:0.8125\n",
      "epoch:2 train loss:0.5022579431533813 train acc:0.8125\n",
      "epoch:2 train loss:0.33537518978118896 train acc:0.90625\n",
      "epoch:2 train loss:0.4722782075405121 train acc:0.875\n",
      "epoch:2 train loss:0.3128160834312439 train acc:0.90625\n",
      "epoch:2 train loss:0.4260095953941345 train acc:0.875\n",
      "epoch:2 train loss:0.3153820037841797 train acc:0.90625\n",
      "epoch:2 train loss:0.8137791752815247 train acc:0.8125\n",
      "epoch:2 train loss:0.3858526051044464 train acc:0.90625\n",
      "epoch:2 train loss:0.3373565375804901 train acc:0.90625\n",
      "epoch:2 train loss:0.6145708560943604 train acc:0.84375\n",
      "epoch:2 train loss:0.4773367643356323 train acc:0.90625\n",
      "epoch:2 train loss:0.2979932427406311 train acc:0.9375\n",
      "epoch:2 train loss:0.3000078797340393 train acc:0.84375\n",
      "epoch:2 train loss:0.4205781817436218 train acc:0.875\n",
      "epoch:2 train loss:0.2933041751384735 train acc:0.90625\n",
      "epoch:2 train loss:0.1896810084581375 train acc:0.96875\n",
      "epoch:2 train loss:0.3998122811317444 train acc:0.875\n",
      "epoch:2 train loss:0.41959255933761597 train acc:0.75\n",
      "epoch:2 train loss:0.3370186686515808 train acc:0.90625\n",
      "epoch:2 train loss:0.31601983308792114 train acc:0.90625\n",
      "epoch:2 train loss:0.29728659987449646 train acc:0.90625\n",
      "epoch:2 train loss:0.33090198040008545 train acc:0.90625\n",
      "epoch:2 train loss:0.4762423038482666 train acc:0.78125\n",
      "epoch:2 train loss:0.23951943218708038 train acc:0.90625\n",
      "epoch:2 train loss:0.2621407210826874 train acc:0.9375\n",
      "epoch:2 train loss:0.5886645317077637 train acc:0.84375\n",
      "epoch:2 train loss:0.18155629932880402 train acc:0.96875\n",
      "epoch:2 train loss:0.5457415580749512 train acc:0.84375\n",
      "epoch:2 train loss:0.34859520196914673 train acc:0.9375\n",
      "epoch:2 train loss:0.23200808465480804 train acc:0.96875\n",
      "epoch:2 train loss:0.34015169739723206 train acc:0.875\n",
      "epoch:2 train loss:0.18852129578590393 train acc:0.96875\n",
      "epoch:2 train loss:0.16478760540485382 train acc:0.9375\n",
      "epoch:2 train loss:0.32616686820983887 train acc:0.90625\n",
      "epoch:2 train loss:0.3304913640022278 train acc:0.90625\n",
      "epoch:2 train loss:0.6989686489105225 train acc:0.78125\n",
      "epoch:2 train loss:0.6859952211380005 train acc:0.84375\n",
      "epoch:2 train loss:0.4706158936023712 train acc:0.8125\n",
      "epoch:2 train loss:0.3298478424549103 train acc:0.9375\n",
      "epoch:2 train loss:0.9614659547805786 train acc:0.84375\n",
      "epoch:2 train loss:0.47890299558639526 train acc:0.84375\n",
      "epoch:2 train loss:0.405201256275177 train acc:0.875\n",
      "epoch:2 train loss:0.28244948387145996 train acc:0.96875\n",
      "epoch:2 train loss:0.10895679146051407 train acc:0.96875\n",
      "epoch:2 train loss:0.4204413592815399 train acc:0.875\n",
      "epoch:2 train loss:0.2968277931213379 train acc:0.9375\n",
      "epoch:2 train loss:0.8136475682258606 train acc:0.84375\n",
      "epoch:2 train loss:0.3554007411003113 train acc:0.9375\n",
      "epoch:2 train loss:0.5686953067779541 train acc:0.84375\n",
      "epoch:2 train loss:0.3033016324043274 train acc:0.875\n",
      "epoch:2 train loss:0.32494890689849854 train acc:0.875\n",
      "epoch:2 train loss:0.49479949474334717 train acc:0.875\n",
      "epoch:2 train loss:0.24005289375782013 train acc:1.0\n",
      "epoch:2 train loss:0.373190313577652 train acc:0.90625\n",
      "epoch:2 train loss:0.41894158720970154 train acc:0.875\n",
      "epoch:2 train loss:0.42880910634994507 train acc:0.84375\n",
      "epoch:2 train loss:0.2755199074745178 train acc:0.875\n",
      "epoch:2 train loss:0.30934256315231323 train acc:0.875\n",
      "epoch:2 train loss:0.15935395658016205 train acc:0.96875\n",
      "epoch:2 train loss:0.2980669438838959 train acc:0.90625\n",
      "epoch:2 train loss:0.6584315299987793 train acc:0.8125\n",
      "epoch:2 train loss:0.16167806088924408 train acc:0.9375\n",
      "epoch:2 train loss:0.32806017994880676 train acc:0.90625\n",
      "epoch:2 train loss:0.312578022480011 train acc:0.90625\n",
      "epoch:2 train loss:0.36734968423843384 train acc:0.90625\n",
      "epoch:2 train loss:0.4410499334335327 train acc:0.8125\n",
      "epoch:2 train loss:0.4046403169631958 train acc:0.875\n",
      "epoch:2 train loss:0.5257040858268738 train acc:0.78125\n",
      "epoch:2 train loss:0.48541462421417236 train acc:0.90625\n",
      "epoch:2 train loss:0.49187996983528137 train acc:0.84375\n",
      "epoch:2 train loss:0.5013506412506104 train acc:0.84375\n",
      "epoch:2 train loss:0.26626354455947876 train acc:0.90625\n",
      "epoch:2 train loss:0.4820210635662079 train acc:0.875\n",
      "epoch:2 train loss:0.5783865451812744 train acc:0.8125\n",
      "epoch:2 train loss:0.28242653608322144 train acc:0.9375\n",
      "epoch:2 train loss:0.33526116609573364 train acc:0.875\n",
      "epoch:2 train loss:0.4017288088798523 train acc:0.875\n",
      "epoch:2 train loss:0.5996377468109131 train acc:0.84375\n",
      "epoch:2 train loss:0.24944373965263367 train acc:0.9375\n",
      "epoch:2 train loss:0.6968098282814026 train acc:0.78125\n",
      "epoch:2 train loss:0.5026196837425232 train acc:0.8125\n",
      "epoch:2 train loss:0.36679619550704956 train acc:0.84375\n",
      "epoch:2 train loss:0.3713679909706116 train acc:0.9375\n",
      "epoch:2 train loss:0.8492552638053894 train acc:0.8125\n",
      "epoch:2 train loss:0.678464412689209 train acc:0.75\n",
      "epoch:2 train loss:0.47666800022125244 train acc:0.8125\n",
      "epoch:2 train loss:0.3055203855037689 train acc:0.90625\n",
      "epoch:2 train loss:0.32029232382774353 train acc:0.96875\n",
      "epoch:2 train loss:0.31872498989105225 train acc:0.9375\n",
      "epoch:2 train loss:0.3192656636238098 train acc:0.90625\n",
      "epoch:2 train loss:0.45915746688842773 train acc:0.84375\n",
      "epoch:2 train loss:0.3549250364303589 train acc:0.90625\n",
      "epoch:2 train loss:0.42387276887893677 train acc:0.90625\n",
      "epoch:2 train loss:0.6428073644638062 train acc:0.78125\n",
      "epoch:2 train loss:0.33507513999938965 train acc:0.90625\n",
      "epoch:2 train loss:0.5168445110321045 train acc:0.8125\n",
      "epoch:2 train loss:0.3485453724861145 train acc:0.90625\n",
      "epoch:2 train loss:1.0319864749908447 train acc:0.84375\n",
      "epoch:2 train loss:0.17952601611614227 train acc:0.9375\n",
      "epoch:2 train loss:0.21111339330673218 train acc:0.8999999761581421\n",
      "epoch:3 train loss:0.4134812653064728 train acc:0.84375\n",
      "epoch:3 train loss:0.1308736354112625 train acc:0.96875\n",
      "epoch:3 train loss:0.4546585977077484 train acc:0.84375\n",
      "epoch:3 train loss:0.6221768856048584 train acc:0.78125\n",
      "epoch:3 train loss:0.5672727227210999 train acc:0.8125\n",
      "epoch:3 train loss:0.3043220341205597 train acc:0.875\n",
      "epoch:3 train loss:0.29470905661582947 train acc:0.90625\n",
      "epoch:3 train loss:0.4952373206615448 train acc:0.84375\n",
      "epoch:3 train loss:0.4634808897972107 train acc:0.84375\n",
      "epoch:3 train loss:0.3029938340187073 train acc:0.875\n",
      "epoch:3 train loss:0.39272427558898926 train acc:0.90625\n",
      "epoch:3 train loss:0.40626776218414307 train acc:0.8125\n",
      "epoch:3 train loss:0.42685115337371826 train acc:0.84375\n",
      "epoch:3 train loss:0.2303600162267685 train acc:0.9375\n",
      "epoch:3 train loss:0.13692337274551392 train acc:0.96875\n",
      "epoch:3 train loss:0.5739739537239075 train acc:0.84375\n",
      "epoch:3 train loss:0.29027724266052246 train acc:0.90625\n",
      "epoch:3 train loss:0.2511633634567261 train acc:0.90625\n",
      "epoch:3 train loss:0.2905908524990082 train acc:0.90625\n",
      "epoch:3 train loss:0.5078229904174805 train acc:0.78125\n",
      "epoch:3 train loss:0.25150081515312195 train acc:0.8125\n",
      "epoch:3 train loss:0.12157934904098511 train acc:0.96875\n",
      "epoch:3 train loss:0.5174804329872131 train acc:0.8125\n",
      "epoch:3 train loss:0.8038367033004761 train acc:0.8125\n",
      "epoch:3 train loss:0.48996734619140625 train acc:0.875\n",
      "epoch:3 train loss:0.5570265650749207 train acc:0.8125\n",
      "epoch:3 train loss:0.3962228298187256 train acc:0.90625\n",
      "epoch:3 train loss:0.4944143295288086 train acc:0.875\n",
      "epoch:3 train loss:0.6122779846191406 train acc:0.78125\n",
      "epoch:3 train loss:0.3789156377315521 train acc:0.90625\n",
      "epoch:3 train loss:0.4915387034416199 train acc:0.84375\n",
      "epoch:3 train loss:0.355182945728302 train acc:0.8125\n",
      "epoch:3 train loss:0.3700370788574219 train acc:0.9375\n",
      "epoch:3 train loss:0.32032543420791626 train acc:0.875\n",
      "epoch:3 train loss:0.7976099848747253 train acc:0.8125\n",
      "epoch:3 train loss:0.2896091639995575 train acc:0.90625\n",
      "epoch:3 train loss:0.47136834263801575 train acc:0.84375\n",
      "epoch:3 train loss:0.48724472522735596 train acc:0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 train loss:0.2153378129005432 train acc:0.9375\n",
      "epoch:3 train loss:0.9858177304267883 train acc:0.71875\n",
      "epoch:3 train loss:0.6575354337692261 train acc:0.8125\n",
      "epoch:3 train loss:0.42944127321243286 train acc:0.875\n",
      "epoch:3 train loss:0.5675594806671143 train acc:0.78125\n",
      "epoch:3 train loss:0.4361785054206848 train acc:0.78125\n",
      "epoch:3 train loss:0.2843566834926605 train acc:0.90625\n",
      "epoch:3 train loss:0.5881817936897278 train acc:0.78125\n",
      "epoch:3 train loss:0.3517662584781647 train acc:0.875\n",
      "epoch:3 train loss:0.4715240001678467 train acc:0.875\n",
      "epoch:3 train loss:0.3191898763179779 train acc:0.90625\n",
      "epoch:3 train loss:0.24225962162017822 train acc:0.96875\n",
      "epoch:3 train loss:0.35841885209083557 train acc:0.90625\n",
      "epoch:3 train loss:0.673534631729126 train acc:0.84375\n",
      "epoch:3 train loss:0.5500706434249878 train acc:0.875\n",
      "epoch:3 train loss:0.2372121661901474 train acc:0.90625\n",
      "epoch:3 train loss:0.1814219355583191 train acc:0.96875\n",
      "epoch:3 train loss:0.10190393030643463 train acc:0.96875\n",
      "epoch:3 train loss:0.21758784353733063 train acc:0.9375\n",
      "epoch:3 train loss:0.5982955694198608 train acc:0.71875\n",
      "epoch:3 train loss:0.7774598598480225 train acc:0.71875\n",
      "epoch:3 train loss:0.597762942314148 train acc:0.84375\n",
      "epoch:3 train loss:0.16760262846946716 train acc:1.0\n",
      "epoch:3 train loss:0.3026345670223236 train acc:0.875\n",
      "epoch:3 train loss:0.190699502825737 train acc:0.90625\n",
      "epoch:3 train loss:0.26867228746414185 train acc:0.90625\n",
      "epoch:3 train loss:0.39167848229408264 train acc:0.875\n",
      "epoch:3 train loss:0.18678437173366547 train acc:0.96875\n",
      "epoch:3 train loss:0.37170979380607605 train acc:0.875\n",
      "epoch:3 train loss:0.42890435457229614 train acc:0.90625\n",
      "epoch:3 train loss:0.3777291178703308 train acc:0.84375\n",
      "epoch:3 train loss:0.35124069452285767 train acc:0.875\n",
      "epoch:3 train loss:0.38672158122062683 train acc:0.84375\n",
      "epoch:3 train loss:0.7265228033065796 train acc:0.75\n",
      "epoch:3 train loss:0.512222170829773 train acc:0.875\n",
      "epoch:3 train loss:0.21891389787197113 train acc:0.9375\n",
      "epoch:3 train loss:0.40120428800582886 train acc:0.875\n",
      "epoch:3 train loss:0.43887653946876526 train acc:0.875\n",
      "epoch:3 train loss:0.20511262118816376 train acc:0.96875\n",
      "epoch:3 train loss:0.4270697832107544 train acc:0.875\n",
      "epoch:3 train loss:0.4806923270225525 train acc:0.9375\n",
      "epoch:3 train loss:0.3805326223373413 train acc:0.84375\n",
      "epoch:3 train loss:0.5206784605979919 train acc:0.8125\n",
      "epoch:3 train loss:0.7036250829696655 train acc:0.8125\n",
      "epoch:3 train loss:0.3751366138458252 train acc:0.84375\n",
      "epoch:3 train loss:0.2429027110338211 train acc:0.90625\n",
      "epoch:3 train loss:0.3277871310710907 train acc:0.90625\n",
      "epoch:3 train loss:0.3700534999370575 train acc:0.9375\n",
      "epoch:3 train loss:0.663203775882721 train acc:0.84375\n",
      "epoch:3 train loss:0.3765147626399994 train acc:0.90625\n",
      "epoch:3 train loss:0.4068494141101837 train acc:0.875\n",
      "epoch:3 train loss:0.4547564685344696 train acc:0.875\n",
      "epoch:3 train loss:0.18017250299453735 train acc:0.96875\n",
      "epoch:3 train loss:0.3407900631427765 train acc:0.875\n",
      "epoch:3 train loss:0.39330434799194336 train acc:0.90625\n",
      "epoch:3 train loss:0.4131596088409424 train acc:0.84375\n",
      "epoch:3 train loss:0.5359148383140564 train acc:0.84375\n",
      "epoch:3 train loss:0.3644550144672394 train acc:0.90625\n",
      "epoch:3 train loss:0.29188013076782227 train acc:0.875\n",
      "epoch:3 train loss:0.5868960618972778 train acc:0.84375\n",
      "epoch:3 train loss:0.30362972617149353 train acc:0.90625\n",
      "epoch:3 train loss:0.3684737980365753 train acc:0.84375\n",
      "epoch:3 train loss:0.4402095675468445 train acc:0.84375\n",
      "epoch:3 train loss:0.20749852061271667 train acc:0.96875\n",
      "epoch:3 train loss:0.5471326112747192 train acc:0.90625\n",
      "epoch:3 train loss:0.13334988057613373 train acc:0.96875\n",
      "epoch:3 train loss:0.2498427778482437 train acc:0.90625\n",
      "epoch:3 train loss:0.2018355280160904 train acc:0.96875\n",
      "epoch:3 train loss:0.3527795076370239 train acc:0.8125\n",
      "epoch:3 train loss:0.24146877229213715 train acc:0.90625\n",
      "epoch:3 train loss:0.4044455587863922 train acc:0.90625\n",
      "epoch:3 train loss:0.3077906668186188 train acc:0.875\n",
      "epoch:3 train loss:0.46077653765678406 train acc:0.875\n",
      "epoch:3 train loss:0.3784824311733246 train acc:0.875\n",
      "epoch:3 train loss:0.7612636089324951 train acc:0.75\n",
      "epoch:3 train loss:0.5417746305465698 train acc:0.78125\n",
      "epoch:3 train loss:0.18339933454990387 train acc:0.96875\n",
      "epoch:3 train loss:0.323941171169281 train acc:0.90625\n",
      "epoch:3 train loss:0.44555267691612244 train acc:0.90625\n",
      "epoch:3 train loss:0.6376661062240601 train acc:0.84375\n",
      "epoch:3 train loss:0.5776085257530212 train acc:0.84375\n",
      "epoch:3 train loss:0.3418200612068176 train acc:0.875\n",
      "epoch:3 train loss:0.38278764486312866 train acc:0.9375\n",
      "epoch:3 train loss:0.25201138854026794 train acc:0.90625\n",
      "epoch:3 train loss:0.29509237408638 train acc:0.9375\n",
      "epoch:3 train loss:0.28252044320106506 train acc:0.96875\n",
      "epoch:3 train loss:0.2971409559249878 train acc:0.875\n",
      "epoch:3 train loss:0.3836439251899719 train acc:0.90625\n",
      "epoch:3 train loss:0.35738858580589294 train acc:0.875\n",
      "epoch:3 train loss:0.5155618190765381 train acc:0.84375\n",
      "epoch:3 train loss:0.1384080946445465 train acc:0.9375\n",
      "epoch:3 train loss:0.2966507077217102 train acc:0.84375\n",
      "epoch:3 train loss:0.5672585964202881 train acc:0.8125\n",
      "epoch:3 train loss:0.2686331868171692 train acc:0.90625\n",
      "epoch:3 train loss:0.33889299631118774 train acc:0.90625\n",
      "epoch:3 train loss:0.6601110100746155 train acc:0.75\n",
      "epoch:3 train loss:0.8163878917694092 train acc:0.8125\n",
      "epoch:3 train loss:0.19988907873630524 train acc:0.9375\n",
      "epoch:3 train loss:0.525676429271698 train acc:0.8125\n",
      "epoch:3 train loss:0.5980634689331055 train acc:0.875\n",
      "epoch:3 train loss:0.37150847911834717 train acc:0.875\n",
      "epoch:3 train loss:0.4925401508808136 train acc:0.8125\n",
      "epoch:3 train loss:0.2958604693412781 train acc:0.84375\n",
      "epoch:3 train loss:0.7286888360977173 train acc:0.71875\n",
      "epoch:3 train loss:0.1935800313949585 train acc:0.9375\n",
      "epoch:3 train loss:0.5206016302108765 train acc:0.8125\n",
      "epoch:3 train loss:0.4723933935165405 train acc:0.84375\n",
      "epoch:3 train loss:0.2113288938999176 train acc:0.9375\n",
      "epoch:3 train loss:0.5684700012207031 train acc:0.8125\n",
      "epoch:3 train loss:0.47598591446876526 train acc:0.875\n",
      "epoch:3 train loss:0.7548100352287292 train acc:0.78125\n",
      "epoch:3 train loss:0.6030302047729492 train acc:0.78125\n",
      "epoch:3 train loss:0.5080834627151489 train acc:0.8125\n",
      "epoch:3 train loss:0.6823757886886597 train acc:0.75\n",
      "epoch:3 train loss:0.23473723232746124 train acc:0.90625\n",
      "epoch:3 train loss:0.14312998950481415 train acc:0.96875\n",
      "epoch:3 train loss:0.38334783911705017 train acc:0.84375\n",
      "epoch:3 train loss:0.29725319147109985 train acc:0.875\n",
      "epoch:3 train loss:0.13265809416770935 train acc:1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-73b42870a4a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Research/Test/app/functions.py\u001b[0m in \u001b[0;36mcal_acc\u001b[0;34m(logits, label)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(args.train_log,'w') as f:\n",
    "    f.truncate()\n",
    "\n",
    "train_acc_arr=[]\n",
    "val_acc_arr=[]\n",
    "for epoch in range(args.epoch):\n",
    "    opt = get_finetune_optimizer(args, model)\n",
    "\n",
    "    train_result = []\n",
    "    train_label = []\n",
    "    val_result = []\n",
    "    val_label = []\n",
    "    for step, (img_id, img, label, bbox) in enumerate(dataloader):\n",
    "        img = img.cuda()\n",
    "        label = label.cuda()\n",
    "\n",
    "        logits, cam = model.forward(img)\n",
    "        loss = loss_func(logits, label)\n",
    "        acc = cal_acc(logits, label)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        print('epoch:{} train loss:{} train acc:{}'.format(epoch, loss, acc))\n",
    "\n",
    "        train_result.extend(torch.argmax(logits, dim=-1).cpu().data.numpy())\n",
    "        train_label.extend(label.cpu().data.numpy())\n",
    "\n",
    "    train_acc_arr.append(np.mean(np.array(train_result) == np.array(train_label)))\n",
    "\n",
    "    # validation\n",
    "    dataset.to_val()\n",
    "    val_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    model.use_hook()\n",
    "    for step, (img_id, img, label, bbox) in enumerate(val_dataloader):\n",
    "        img = img.cuda()\n",
    "        label = label.cuda()\n",
    "\n",
    "        logits, cam = model.forward(img)\n",
    "        val_result.extend(torch.argmax(logits, dim=-1).cpu().data.numpy())\n",
    "        val_label.extend(label.cpu().data.numpy())\n",
    "\n",
    "        if step == 0:\n",
    "            save_each_layer_cams_by_id(args, model)\n",
    "\n",
    "    val_acc_arr.append(np.mean(np.array(val_result) == np.array(val_label)))\n",
    "\n",
    "    #plot\n",
    "    plot_train_process([train_acc_arr,val_acc_arr])\n",
    "\n",
    "    dataset.to_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
