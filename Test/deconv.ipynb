{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import easydict\n",
    "import ast\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functions.cus_plot import plot_train_process,plot_different_figs\n",
    "from functions.helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=easydict.EasyDict({\n",
    "    'batch_size':16,\n",
    "    'epoch':50,\n",
    "    'data_root_path':'d:/Data/CUB/CUB_200_2011/',\n",
    "    'csv_path':'save/data/data.csv',\n",
    "    'lr':0.001,\n",
    "    'train_log':'save/train_log.txt',\n",
    "    'train_model':'save/model/deconv_vgg16.pt',\n",
    "    'train_img':'save/img/train_process.png',\n",
    "    'class_nums':200,\n",
    "    'train_img_size':224,\n",
    "    'weight_decay':0.0001,\n",
    "    'nesterov':False,\n",
    "    'check_path':'save/train/check.pkl',\n",
    "    'continue_train':False\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self, args, mode='train'):\n",
    "        self.data_csv = pd.read_csv(args.csv_path)\n",
    "        self.mode = mode\n",
    "        self.args = args\n",
    "        self.img_size = args.train_img_size\n",
    "        self.class_nums=args.class_nums\n",
    "\n",
    "        self.train_csv=self.data_csv[self.data_csv['is_train']==1]\n",
    "        self.val_csv=self.data_csv[self.data_csv['is_train']==0]\n",
    "        self.train_csv.reset_index(drop=True,inplace=True)\n",
    "        self.val_csv.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "        if self.mode=='train':\n",
    "            self.cur_csv=self.train_csv\n",
    "        else:\n",
    "            self.cur_csv=self.val_csv\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.cur_csv.loc[index]\n",
    "\n",
    "        img_id = item['id']\n",
    "        path = item['path']\n",
    "        label = item['cls']\n",
    "        bbox = item['bbox']\n",
    "\n",
    "        raw_img = Image.open(self.args.data_root_path + path).convert('RGB')\n",
    "        img = self.image_transform(img_size=self.img_size, mode=self.mode)(raw_img)\n",
    "\n",
    "        return img_id, img, label, bbox\n",
    "\n",
    "    def to_train(self):\n",
    "        self.mode = 'train'\n",
    "        self.cur_csv = self.train_csv\n",
    "\n",
    "    def to_val(self):\n",
    "        self.mode = 'val'\n",
    "        self.cur_csv = self.val_csv\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cur_csv)\n",
    "    \n",
    "    @staticmethod\n",
    "    def image_transform(img_size, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], mode='train'):\n",
    "        if mode == 'train':\n",
    "            horizontal_flip = 0.5\n",
    "            vertical_flip = 0.5\n",
    "\n",
    "            t = [\n",
    "                transforms.RandomResizedCrop(size=img_size),\n",
    "                transforms.RandomHorizontalFlip(horizontal_flip),\n",
    "                transforms.RandomVerticalFlip(vertical_flip),\n",
    "                transforms.ColorJitter(saturation=0.4, brightness=0.4, hue=0.05),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            t = [\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ]\n",
    "\n",
    "        return transforms.Compose([v for v in t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deconv_vgg(nn.Module):\n",
    "    def __init__(self, pre_trained_vgg, args, inference=False, freeze_vgg=False):\n",
    "        super(Deconv_vgg, self).__init__()\n",
    "        self.inference = inference\n",
    "        self.freeze_vgg = freeze_vgg\n",
    "        self.class_nums=args.class_nums\n",
    "\n",
    "        self.features = pre_trained_vgg.features\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1, dilation=1),  # fc6\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1, dilation=1),  # fc6\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(1024, self.class_nums, kernel_size=1, padding=0)  # fc8\n",
    "        )\n",
    "        \n",
    "        self.deconv=nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=200,out_channels=1024,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=1024,out_channels=1024,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=512,out_channels=512,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=512,out_channels=200,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        if self.freeze_vgg:\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.inference:\n",
    "            x.requires_grad_()\n",
    "            x.retain_grad()\n",
    "\n",
    "        base = self.features(x)\n",
    "        avg_pool = F.avg_pool2d(base, kernel_size=3, stride=1, padding=1)\n",
    "        cam = self.cls(avg_pool)\n",
    "        \n",
    "        deconv_cam=self.deconv(cam)\n",
    "        logits=torch.mean(torch.mean(deconv_cam,dim=2),dim=2)\n",
    "\n",
    "        if self.inference:\n",
    "            pass\n",
    "\n",
    "        return logits,deconv_cam\n",
    "    \n",
    "    def norm_cam_2_binary(self, bi_x_grad):\n",
    "        thd = float(np.percentile(np.sort(bi_x_grad.view(-1).cpu().data.numpy()), 80))\n",
    "        outline = torch.zeros(bi_x_grad.size())\n",
    "        high_pos = torch.gt(bi_x_grad, thd)\n",
    "        outline[high_pos.data] = 1.0\n",
    "        \n",
    "        return outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(pretrained=True,**kwargs):\n",
    "    pre_trained_model = models.vgg16(pretrained=pretrained)\n",
    "\n",
    "    model = Deconv_vgg(pre_trained_vgg=pre_trained_model, **kwargs)\n",
    "    model.cuda()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_func(args):\n",
    "    return torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finetune_optimizer(args, model):\n",
    "    lr = args.lr\n",
    "    weight_list = []\n",
    "    bias_list = []\n",
    "    last_weight_list = []\n",
    "    last_bias_list = []\n",
    "    \n",
    "    for name, value in model.named_parameters():\n",
    "        if 'cls' in name:\n",
    "            if 'weight' in name:\n",
    "                last_weight_list.append(value)\n",
    "            elif 'bias' in name:\n",
    "                last_bias_list.append(value)\n",
    "        else:\n",
    "            if 'weight' in name:\n",
    "                weight_list.append(value)\n",
    "            elif 'bias' in name:\n",
    "                bias_list.append(value)\n",
    "\n",
    "    opt = optim.SGD([{'params': weight_list, 'lr': lr / 10},\n",
    "                     {'params': bias_list, 'lr': lr / 5},\n",
    "                     {'params': last_weight_list, 'lr': lr},\n",
    "                     {'params': last_bias_list, 'lr': lr * 2}], momentum=0.9, weight_decay=args.weight_decay, nesterov=args.nesterov)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train loss:5.2978973388671875 train acc:0.0\n",
      "epoch:0 train loss:5.300558567047119 train acc:0.0\n",
      "epoch:0 train loss:5.302445411682129 train acc:0.0\n",
      "epoch:0 train loss:5.298092842102051 train acc:0.0\n",
      "epoch:0 train loss:5.2988057136535645 train acc:0.0\n",
      "epoch:0 train loss:5.296817302703857 train acc:0.0\n",
      "epoch:0 train loss:5.2963433265686035 train acc:0.0\n",
      "epoch:0 train loss:5.3005266189575195 train acc:0.0\n",
      "epoch:0 train loss:5.300187587738037 train acc:0.0\n",
      "epoch:0 train loss:5.300678730010986 train acc:0.0\n",
      "epoch:0 train loss:5.29926872253418 train acc:0.0\n",
      "epoch:0 train loss:5.2997026443481445 train acc:0.0\n",
      "epoch:0 train loss:5.297369003295898 train acc:0.0\n",
      "epoch:0 train loss:5.295777797698975 train acc:0.0\n",
      "epoch:0 train loss:5.300346851348877 train acc:0.0\n",
      "epoch:0 train loss:5.302643775939941 train acc:0.0\n",
      "epoch:0 train loss:5.300783157348633 train acc:0.0\n",
      "epoch:0 train loss:5.295696258544922 train acc:0.0\n",
      "epoch:0 train loss:5.300888538360596 train acc:0.0\n",
      "epoch:0 train loss:5.294212818145752 train acc:0.0\n",
      "epoch:0 train loss:5.298069000244141 train acc:0.0625\n",
      "epoch:0 train loss:5.296921253204346 train acc:0.0\n",
      "epoch:0 train loss:5.296062469482422 train acc:0.0\n",
      "epoch:0 train loss:5.2974958419799805 train acc:0.0\n",
      "epoch:0 train loss:5.2969489097595215 train acc:0.0\n",
      "epoch:0 train loss:5.298789978027344 train acc:0.0\n",
      "epoch:0 train loss:5.300076484680176 train acc:0.0625\n",
      "epoch:0 train loss:5.298399448394775 train acc:0.0\n",
      "epoch:0 train loss:5.2945876121521 train acc:0.0\n",
      "epoch:0 train loss:5.297635078430176 train acc:0.0\n",
      "epoch:0 train loss:5.294063091278076 train acc:0.0625\n",
      "epoch:0 train loss:5.295464038848877 train acc:0.0625\n",
      "epoch:0 train loss:5.298743724822998 train acc:0.0\n",
      "epoch:0 train loss:5.2988362312316895 train acc:0.0\n",
      "epoch:0 train loss:5.297203063964844 train acc:0.0\n",
      "epoch:0 train loss:5.298573970794678 train acc:0.0\n",
      "epoch:0 train loss:5.296572208404541 train acc:0.0\n",
      "epoch:0 train loss:5.299823760986328 train acc:0.0\n",
      "epoch:0 train loss:5.297412395477295 train acc:0.0\n",
      "epoch:0 train loss:5.300414562225342 train acc:0.0\n",
      "epoch:0 train loss:5.3003644943237305 train acc:0.0\n",
      "epoch:0 train loss:5.300101280212402 train acc:0.0\n",
      "epoch:0 train loss:5.3010430335998535 train acc:0.0\n",
      "epoch:0 train loss:5.298729419708252 train acc:0.0\n",
      "epoch:0 train loss:5.297915935516357 train acc:0.0\n",
      "epoch:0 train loss:5.299398899078369 train acc:0.0\n",
      "epoch:0 train loss:5.29580020904541 train acc:0.0\n",
      "epoch:0 train loss:5.299455165863037 train acc:0.0\n",
      "epoch:0 train loss:5.298237323760986 train acc:0.0\n",
      "epoch:0 train loss:5.300172328948975 train acc:0.0\n",
      "epoch:0 train loss:5.299269676208496 train acc:0.0\n",
      "epoch:0 train loss:5.296570301055908 train acc:0.0625\n",
      "epoch:0 train loss:5.296088218688965 train acc:0.0\n",
      "epoch:0 train loss:5.299571990966797 train acc:0.0\n",
      "epoch:0 train loss:5.297220230102539 train acc:0.0\n",
      "epoch:0 train loss:5.298431873321533 train acc:0.0\n",
      "epoch:0 train loss:5.298191070556641 train acc:0.0\n",
      "epoch:0 train loss:5.298334121704102 train acc:0.0\n",
      "epoch:0 train loss:5.298101902008057 train acc:0.0\n",
      "epoch:0 train loss:5.2989654541015625 train acc:0.0625\n",
      "epoch:0 train loss:5.295755386352539 train acc:0.0\n",
      "epoch:0 train loss:5.299212455749512 train acc:0.0\n",
      "epoch:0 train loss:5.299356460571289 train acc:0.0\n",
      "epoch:0 train loss:5.298768997192383 train acc:0.0\n",
      "epoch:0 train loss:5.295323848724365 train acc:0.0\n",
      "epoch:0 train loss:5.300264835357666 train acc:0.0\n",
      "epoch:0 train loss:5.301178455352783 train acc:0.0\n",
      "epoch:0 train loss:5.298633575439453 train acc:0.0\n",
      "epoch:0 train loss:5.297971248626709 train acc:0.0\n",
      "epoch:0 train loss:5.299004077911377 train acc:0.0625\n",
      "epoch:0 train loss:5.298379898071289 train acc:0.0\n",
      "epoch:0 train loss:5.296553611755371 train acc:0.0\n",
      "epoch:0 train loss:5.297082901000977 train acc:0.0\n",
      "epoch:0 train loss:5.29652738571167 train acc:0.0\n",
      "epoch:0 train loss:5.300086975097656 train acc:0.0\n",
      "epoch:0 train loss:5.300068378448486 train acc:0.0\n",
      "epoch:0 train loss:5.301044464111328 train acc:0.0\n",
      "epoch:0 train loss:5.297801971435547 train acc:0.0\n",
      "epoch:0 train loss:5.299602508544922 train acc:0.0\n",
      "epoch:0 train loss:5.297188758850098 train acc:0.0\n",
      "epoch:0 train loss:5.296490669250488 train acc:0.0\n",
      "epoch:0 train loss:5.300376892089844 train acc:0.0\n",
      "epoch:0 train loss:5.297646522521973 train acc:0.0\n",
      "epoch:0 train loss:5.296406269073486 train acc:0.0\n",
      "epoch:0 train loss:5.298831462860107 train acc:0.0\n",
      "epoch:0 train loss:5.297747611999512 train acc:0.0\n",
      "epoch:0 train loss:5.2979021072387695 train acc:0.0\n",
      "epoch:0 train loss:5.300867080688477 train acc:0.0\n",
      "epoch:0 train loss:5.301385402679443 train acc:0.0\n",
      "epoch:0 train loss:5.299491882324219 train acc:0.0\n",
      "epoch:0 train loss:5.298047065734863 train acc:0.0\n",
      "epoch:0 train loss:5.293433666229248 train acc:0.0\n",
      "epoch:0 train loss:5.293169975280762 train acc:0.0625\n",
      "epoch:0 train loss:5.296931266784668 train acc:0.0625\n",
      "epoch:0 train loss:5.300405502319336 train acc:0.0\n",
      "epoch:0 train loss:5.297448635101318 train acc:0.0\n",
      "epoch:0 train loss:5.297624588012695 train acc:0.0\n",
      "epoch:0 train loss:5.295531272888184 train acc:0.0\n",
      "epoch:0 train loss:5.298985004425049 train acc:0.0\n",
      "epoch:0 train loss:5.300070762634277 train acc:0.0\n",
      "epoch:0 train loss:5.2995076179504395 train acc:0.0\n",
      "epoch:0 train loss:5.297804832458496 train acc:0.0\n",
      "epoch:0 train loss:5.298177719116211 train acc:0.0\n",
      "epoch:0 train loss:5.299106121063232 train acc:0.0\n",
      "epoch:0 train loss:5.2988200187683105 train acc:0.0\n",
      "epoch:0 train loss:5.299971580505371 train acc:0.0\n",
      "epoch:0 train loss:5.296647548675537 train acc:0.0\n",
      "epoch:0 train loss:5.301636695861816 train acc:0.0\n",
      "epoch:0 train loss:5.298269748687744 train acc:0.0625\n",
      "epoch:0 train loss:5.297813892364502 train acc:0.0\n",
      "epoch:0 train loss:5.296137809753418 train acc:0.0\n",
      "epoch:0 train loss:5.298360347747803 train acc:0.0\n",
      "epoch:0 train loss:5.298250198364258 train acc:0.0\n",
      "epoch:0 train loss:5.29737663269043 train acc:0.0\n",
      "epoch:0 train loss:5.298384666442871 train acc:0.0625\n",
      "epoch:0 train loss:5.2984538078308105 train acc:0.0\n",
      "epoch:0 train loss:5.298346996307373 train acc:0.0\n",
      "epoch:0 train loss:5.298542499542236 train acc:0.0\n",
      "epoch:0 train loss:5.299664497375488 train acc:0.0\n",
      "epoch:0 train loss:5.299441814422607 train acc:0.0\n",
      "epoch:0 train loss:5.300187587738037 train acc:0.0\n",
      "epoch:0 train loss:5.2996602058410645 train acc:0.0\n",
      "epoch:0 train loss:5.297400951385498 train acc:0.0\n",
      "epoch:0 train loss:5.297370910644531 train acc:0.0625\n",
      "epoch:0 train loss:5.296988487243652 train acc:0.0\n",
      "epoch:0 train loss:5.295618057250977 train acc:0.0\n",
      "epoch:0 train loss:5.298879623413086 train acc:0.0\n",
      "epoch:0 train loss:5.2977681159973145 train acc:0.0\n",
      "epoch:0 train loss:5.296146869659424 train acc:0.0\n",
      "epoch:0 train loss:5.298910617828369 train acc:0.0\n",
      "epoch:0 train loss:5.294921875 train acc:0.0\n",
      "epoch:0 train loss:5.300808906555176 train acc:0.0\n",
      "epoch:0 train loss:5.300924301147461 train acc:0.0\n",
      "epoch:0 train loss:5.298672676086426 train acc:0.0\n",
      "epoch:0 train loss:5.295437335968018 train acc:0.0625\n",
      "epoch:0 train loss:5.30190896987915 train acc:0.0\n",
      "epoch:0 train loss:5.297101020812988 train acc:0.0\n",
      "epoch:0 train loss:5.300926685333252 train acc:0.0\n",
      "epoch:0 train loss:5.299155235290527 train acc:0.0\n",
      "epoch:0 train loss:5.295005798339844 train acc:0.0\n",
      "epoch:0 train loss:5.298912048339844 train acc:0.0\n",
      "epoch:0 train loss:5.300928592681885 train acc:0.0\n",
      "epoch:0 train loss:5.299966812133789 train acc:0.0\n",
      "epoch:0 train loss:5.295759201049805 train acc:0.0\n",
      "epoch:0 train loss:5.296077728271484 train acc:0.0\n",
      "epoch:0 train loss:5.295320987701416 train acc:0.0\n",
      "epoch:0 train loss:5.298425197601318 train acc:0.0\n",
      "epoch:0 train loss:5.299635410308838 train acc:0.0\n",
      "epoch:0 train loss:5.299583911895752 train acc:0.0\n",
      "epoch:0 train loss:5.301011085510254 train acc:0.0\n",
      "epoch:0 train loss:5.300330638885498 train acc:0.0\n",
      "epoch:0 train loss:5.299470901489258 train acc:0.0\n",
      "epoch:0 train loss:5.2966156005859375 train acc:0.0\n",
      "epoch:0 train loss:5.300188064575195 train acc:0.0\n",
      "epoch:0 train loss:5.300465106964111 train acc:0.0\n",
      "epoch:0 train loss:5.2967939376831055 train acc:0.0\n",
      "epoch:0 train loss:5.298529624938965 train acc:0.0\n",
      "epoch:0 train loss:5.300298690795898 train acc:0.0\n",
      "epoch:0 train loss:5.298152446746826 train acc:0.0\n",
      "epoch:0 train loss:5.297104358673096 train acc:0.0625\n",
      "epoch:0 train loss:5.2987446784973145 train acc:0.0\n",
      "epoch:0 train loss:5.299232482910156 train acc:0.0\n",
      "epoch:0 train loss:5.29555082321167 train acc:0.0\n",
      "epoch:0 train loss:5.298104763031006 train acc:0.0\n",
      "epoch:0 train loss:5.300014495849609 train acc:0.0\n",
      "epoch:0 train loss:5.300508975982666 train acc:0.0\n",
      "epoch:0 train loss:5.298663139343262 train acc:0.0\n",
      "epoch:0 train loss:5.302402019500732 train acc:0.0\n",
      "epoch:0 train loss:5.300262928009033 train acc:0.0\n",
      "epoch:0 train loss:5.298287391662598 train acc:0.0\n",
      "epoch:0 train loss:5.296308517456055 train acc:0.0\n",
      "epoch:0 train loss:5.294464588165283 train acc:0.0\n",
      "epoch:0 train loss:5.2968316078186035 train acc:0.0\n",
      "epoch:0 train loss:5.300177574157715 train acc:0.0\n",
      "epoch:0 train loss:5.299481391906738 train acc:0.0\n",
      "epoch:0 train loss:5.298175811767578 train acc:0.0\n",
      "epoch:0 train loss:5.298794269561768 train acc:0.0\n",
      "epoch:0 train loss:5.298084735870361 train acc:0.0\n",
      "epoch:0 train loss:5.2947540283203125 train acc:0.0\n",
      "epoch:0 train loss:5.297222137451172 train acc:0.0625\n",
      "epoch:0 train loss:5.299914836883545 train acc:0.0\n",
      "epoch:0 train loss:5.29273796081543 train acc:0.0625\n",
      "epoch:0 train loss:5.298586368560791 train acc:0.0\n",
      "epoch:0 train loss:5.298553943634033 train acc:0.0\n",
      "epoch:0 train loss:5.294708251953125 train acc:0.0\n",
      "epoch:0 train loss:5.298823356628418 train acc:0.0\n",
      "epoch:0 train loss:5.2971720695495605 train acc:0.0\n",
      "epoch:0 train loss:5.300063610076904 train acc:0.0\n",
      "epoch:0 train loss:5.300815105438232 train acc:0.0\n",
      "epoch:0 train loss:5.3005218505859375 train acc:0.0\n",
      "epoch:0 train loss:5.299967288970947 train acc:0.0\n",
      "epoch:0 train loss:5.297393321990967 train acc:0.0\n",
      "epoch:0 train loss:5.295909881591797 train acc:0.0\n",
      "epoch:0 train loss:5.2981181144714355 train acc:0.0\n",
      "epoch:0 train loss:5.296514511108398 train acc:0.0\n",
      "epoch:0 train loss:5.2977752685546875 train acc:0.0625\n",
      "epoch:0 train loss:5.293949127197266 train acc:0.0\n",
      "epoch:0 train loss:5.299557685852051 train acc:0.0\n",
      "epoch:0 train loss:5.299732685089111 train acc:0.0\n",
      "epoch:0 train loss:5.294375419616699 train acc:0.0\n",
      "epoch:0 train loss:5.298935413360596 train acc:0.0\n",
      "epoch:0 train loss:5.297521114349365 train acc:0.0625\n",
      "epoch:0 train loss:5.296595573425293 train acc:0.0\n",
      "epoch:0 train loss:5.2995123863220215 train acc:0.0\n",
      "epoch:0 train loss:5.2984747886657715 train acc:0.0625\n",
      "epoch:0 train loss:5.299949645996094 train acc:0.0\n",
      "epoch:0 train loss:5.295634746551514 train acc:0.0625\n",
      "epoch:0 train loss:5.300041675567627 train acc:0.0\n",
      "epoch:0 train loss:5.2995147705078125 train acc:0.0\n",
      "epoch:0 train loss:5.298281669616699 train acc:0.0\n",
      "epoch:0 train loss:5.2995405197143555 train acc:0.0\n",
      "epoch:0 train loss:5.298282623291016 train acc:0.0625\n",
      "epoch:0 train loss:5.296996593475342 train acc:0.0\n",
      "epoch:0 train loss:5.300378799438477 train acc:0.0\n",
      "epoch:0 train loss:5.29802942276001 train acc:0.0\n",
      "epoch:0 train loss:5.294204235076904 train acc:0.0\n",
      "epoch:0 train loss:5.301430702209473 train acc:0.0\n",
      "epoch:0 train loss:5.297177791595459 train acc:0.0\n",
      "epoch:0 train loss:5.296602249145508 train acc:0.0\n",
      "epoch:0 train loss:5.297173023223877 train acc:0.0\n",
      "epoch:0 train loss:5.29840612411499 train acc:0.0\n",
      "epoch:0 train loss:5.2979583740234375 train acc:0.0\n",
      "epoch:0 train loss:5.299600601196289 train acc:0.0\n",
      "epoch:0 train loss:5.30009651184082 train acc:0.0\n",
      "epoch:0 train loss:5.292675495147705 train acc:0.0625\n",
      "epoch:0 train loss:5.297392845153809 train acc:0.0\n",
      "epoch:0 train loss:5.296473979949951 train acc:0.0625\n",
      "epoch:0 train loss:5.2994771003723145 train acc:0.0\n",
      "epoch:0 train loss:5.298225402832031 train acc:0.0\n",
      "epoch:0 train loss:5.299530982971191 train acc:0.0\n",
      "epoch:0 train loss:5.300537586212158 train acc:0.0\n",
      "epoch:0 train loss:5.29939603805542 train acc:0.0\n",
      "epoch:0 train loss:5.297638893127441 train acc:0.0\n",
      "epoch:0 train loss:5.300856590270996 train acc:0.0\n",
      "epoch:0 train loss:5.295676231384277 train acc:0.0\n",
      "epoch:0 train loss:5.29823112487793 train acc:0.0\n",
      "epoch:0 train loss:5.299179553985596 train acc:0.0625\n",
      "epoch:0 train loss:5.298101425170898 train acc:0.0\n",
      "epoch:0 train loss:5.29792594909668 train acc:0.0\n",
      "epoch:0 train loss:5.297418117523193 train acc:0.0\n",
      "epoch:0 train loss:5.301206588745117 train acc:0.0\n",
      "epoch:0 train loss:5.298841953277588 train acc:0.0\n",
      "epoch:0 train loss:5.2976603507995605 train acc:0.0\n",
      "epoch:0 train loss:5.299807548522949 train acc:0.0\n",
      "epoch:0 train loss:5.2957353591918945 train acc:0.0\n",
      "epoch:0 train loss:5.298678874969482 train acc:0.0\n",
      "epoch:0 train loss:5.297303676605225 train acc:0.0\n",
      "epoch:0 train loss:5.295798301696777 train acc:0.0\n",
      "epoch:0 train loss:5.297425746917725 train acc:0.0\n",
      "epoch:0 train loss:5.295433521270752 train acc:0.0\n",
      "epoch:0 train loss:5.299047470092773 train acc:0.0\n",
      "epoch:0 train loss:5.3013129234313965 train acc:0.0\n",
      "epoch:0 train loss:5.301370143890381 train acc:0.0\n",
      "epoch:0 train loss:5.295148849487305 train acc:0.0\n",
      "epoch:0 train loss:5.2974324226379395 train acc:0.0\n",
      "epoch:0 train loss:5.299638748168945 train acc:0.0\n",
      "epoch:0 train loss:5.302186965942383 train acc:0.0\n",
      "epoch:0 train loss:5.29763126373291 train acc:0.0\n",
      "epoch:0 train loss:5.300375461578369 train acc:0.0\n",
      "epoch:0 train loss:5.298012733459473 train acc:0.0\n",
      "epoch:0 train loss:5.299293518066406 train acc:0.0\n",
      "epoch:0 train loss:5.297743797302246 train acc:0.0\n",
      "epoch:0 train loss:5.300518989562988 train acc:0.0\n",
      "epoch:0 train loss:5.296496868133545 train acc:0.0\n",
      "epoch:0 train loss:5.298664093017578 train acc:0.0\n",
      "epoch:0 train loss:5.301103115081787 train acc:0.0\n",
      "epoch:0 train loss:5.294832229614258 train acc:0.0\n",
      "epoch:0 train loss:5.299068927764893 train acc:0.0\n",
      "epoch:0 train loss:5.298964500427246 train acc:0.0\n",
      "epoch:0 train loss:5.295821666717529 train acc:0.0625\n",
      "epoch:0 train loss:5.2985053062438965 train acc:0.0\n",
      "epoch:0 train loss:5.295221328735352 train acc:0.0\n",
      "epoch:0 train loss:5.301274299621582 train acc:0.0\n",
      "epoch:0 train loss:5.298659324645996 train acc:0.0\n",
      "epoch:0 train loss:5.298767566680908 train acc:0.0\n",
      "epoch:0 train loss:5.298028469085693 train acc:0.0\n",
      "epoch:0 train loss:5.300096035003662 train acc:0.0\n",
      "epoch:0 train loss:5.29469108581543 train acc:0.0\n",
      "epoch:0 train loss:5.301671981811523 train acc:0.0\n",
      "epoch:0 train loss:5.297835350036621 train acc:0.0\n",
      "epoch:0 train loss:5.296889305114746 train acc:0.0\n",
      "epoch:0 train loss:5.299673557281494 train acc:0.0\n",
      "epoch:0 train loss:5.29783821105957 train acc:0.0\n",
      "epoch:0 train loss:5.296368598937988 train acc:0.0\n",
      "epoch:0 train loss:5.298028945922852 train acc:0.0\n",
      "epoch:0 train loss:5.299507141113281 train acc:0.0\n",
      "epoch:0 train loss:5.299381256103516 train acc:0.0\n",
      "epoch:0 train loss:5.29688024520874 train acc:0.0\n",
      "epoch:0 train loss:5.294833183288574 train acc:0.0\n",
      "epoch:0 train loss:5.298056125640869 train acc:0.0\n",
      "epoch:0 train loss:5.296085357666016 train acc:0.0\n",
      "epoch:0 train loss:5.29567289352417 train acc:0.0\n",
      "epoch:0 train loss:5.298831939697266 train acc:0.0\n",
      "epoch:0 train loss:5.299450874328613 train acc:0.0\n",
      "epoch:0 train loss:5.299262046813965 train acc:0.0\n",
      "epoch:0 train loss:5.302554607391357 train acc:0.0\n",
      "epoch:0 train loss:5.29887580871582 train acc:0.0\n",
      "epoch:0 train loss:5.298249244689941 train acc:0.0\n",
      "epoch:0 train loss:5.300572395324707 train acc:0.0\n",
      "epoch:0 train loss:5.294881343841553 train acc:0.0\n",
      "epoch:0 train loss:5.299249172210693 train acc:0.0\n",
      "epoch:0 train loss:5.299245834350586 train acc:0.0\n",
      "epoch:0 train loss:5.29983377456665 train acc:0.0\n",
      "epoch:0 train loss:5.297525882720947 train acc:0.0\n",
      "epoch:0 train loss:5.298696994781494 train acc:0.0\n",
      "epoch:0 train loss:5.300149917602539 train acc:0.0\n",
      "epoch:0 train loss:5.298154354095459 train acc:0.0\n",
      "epoch:0 train loss:5.302404880523682 train acc:0.0\n",
      "epoch:0 train loss:5.299911975860596 train acc:0.0\n",
      "epoch:0 train loss:5.298704147338867 train acc:0.0\n",
      "epoch:0 train loss:5.299527168273926 train acc:0.0\n",
      "epoch:0 train loss:5.299243450164795 train acc:0.0625\n",
      "epoch:0 train loss:5.296544551849365 train acc:0.0\n",
      "epoch:0 train loss:5.29880952835083 train acc:0.0\n",
      "epoch:0 train loss:5.295367240905762 train acc:0.0\n",
      "epoch:0 train loss:5.29597282409668 train acc:0.0\n",
      "epoch:0 train loss:5.301627159118652 train acc:0.0\n",
      "epoch:0 train loss:5.299335479736328 train acc:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train loss:5.2984113693237305 train acc:0.0\n",
      "epoch:0 train loss:5.3000168800354 train acc:0.0\n",
      "epoch:0 train loss:5.298556804656982 train acc:0.0\n",
      "epoch:0 train loss:5.2960944175720215 train acc:0.0\n",
      "epoch:0 train loss:5.2956414222717285 train acc:0.0625\n",
      "epoch:0 train loss:5.300796985626221 train acc:0.0\n",
      "epoch:0 train loss:5.295852184295654 train acc:0.0625\n",
      "epoch:0 train loss:5.296573162078857 train acc:0.0\n",
      "epoch:0 train loss:5.297729015350342 train acc:0.0\n",
      "epoch:0 train loss:5.29569149017334 train acc:0.0\n",
      "epoch:0 train loss:5.29852294921875 train acc:0.0\n",
      "epoch:0 train loss:5.302338600158691 train acc:0.0\n",
      "epoch:0 train loss:5.300187110900879 train acc:0.0\n",
      "epoch:0 train loss:5.300409317016602 train acc:0.0\n",
      "epoch:0 train loss:5.2977166175842285 train acc:0.0\n",
      "epoch:0 train loss:5.298351764678955 train acc:0.0\n",
      "epoch:0 train loss:5.298081398010254 train acc:0.0\n",
      "epoch:0 train loss:5.2973175048828125 train acc:0.0\n",
      "epoch:0 train loss:5.2958292961120605 train acc:0.0\n",
      "epoch:0 train loss:5.299206256866455 train acc:0.0\n",
      "epoch:0 train loss:5.3018012046813965 train acc:0.0\n",
      "epoch:0 train loss:5.297972202301025 train acc:0.0625\n",
      "epoch:0 train loss:5.299863338470459 train acc:0.0\n",
      "epoch:0 train loss:5.295986175537109 train acc:0.0\n",
      "epoch:0 train loss:5.300601482391357 train acc:0.0\n",
      "epoch:0 train loss:5.2966156005859375 train acc:0.0\n",
      "epoch:0 train loss:5.298141956329346 train acc:0.0\n",
      "epoch:0 train loss:5.301164150238037 train acc:0.0\n",
      "epoch:0 train loss:5.301597595214844 train acc:0.0\n",
      "epoch:0 train loss:5.2986931800842285 train acc:0.0\n",
      "epoch:0 train loss:5.300679683685303 train acc:0.0\n",
      "epoch:0 train loss:5.294986724853516 train acc:0.0\n",
      "epoch:0 train loss:5.300368309020996 train acc:0.0\n",
      "epoch:0 train loss:5.2989606857299805 train acc:0.0\n",
      "epoch:0 train loss:5.297697067260742 train acc:0.0\n",
      "epoch:0 train loss:5.298947811126709 train acc:0.0\n",
      "epoch:0 train loss:5.297525405883789 train acc:0.0\n",
      "epoch:0 train loss:5.300968647003174 train acc:0.0\n",
      "epoch:0 train loss:5.296256065368652 train acc:0.0625\n",
      "epoch:0 train loss:5.298542499542236 train acc:0.0\n",
      "epoch:0 train loss:5.296908855438232 train acc:0.0\n",
      "epoch:0 train loss:5.297962188720703 train acc:0.0\n",
      "epoch:0 train loss:5.298959732055664 train acc:0.0\n",
      "epoch:0 train loss:5.300835132598877 train acc:0.0\n",
      "epoch:0 train loss:5.297701835632324 train acc:0.0\n",
      "epoch:0 train loss:5.302248001098633 train acc:0.0\n",
      "epoch:0 train loss:5.299257755279541 train acc:0.0\n",
      "epoch:0 train loss:5.298226356506348 train acc:0.0\n",
      "epoch:0 train loss:5.29878044128418 train acc:0.0\n",
      "epoch:0 train loss:5.29798698425293 train acc:0.0\n",
      "epoch:0 train loss:5.3010382652282715 train acc:0.0\n",
      "epoch:0 train loss:5.298747539520264 train acc:0.0\n",
      "epoch:0 train loss:5.298700332641602 train acc:0.0\n",
      "epoch:0 train loss:5.300011157989502 train acc:0.0\n",
      "epoch:0 train loss:5.2988810539245605 train acc:0.0\n",
      "epoch:0 train loss:5.298073768615723 train acc:0.0\n",
      "epoch:0 train loss:5.295624732971191 train acc:0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637281edf5bf4dd8b3f2a6d59c41d756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=363), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\App\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2351: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:1 train loss:5.298020362854004 train acc:0.0\n",
      "epoch:1 train loss:5.298901557922363 train acc:0.0\n",
      "epoch:1 train loss:5.299846649169922 train acc:0.0\n",
      "epoch:1 train loss:5.298932075500488 train acc:0.0\n",
      "epoch:1 train loss:5.299930095672607 train acc:0.0\n",
      "epoch:1 train loss:5.298301696777344 train acc:0.0625\n",
      "epoch:1 train loss:5.299336910247803 train acc:0.0\n",
      "epoch:1 train loss:5.298492431640625 train acc:0.0\n",
      "epoch:1 train loss:5.297505855560303 train acc:0.0\n",
      "epoch:1 train loss:5.30026912689209 train acc:0.0\n",
      "epoch:1 train loss:5.2978010177612305 train acc:0.0\n",
      "epoch:1 train loss:5.300914287567139 train acc:0.0\n",
      "epoch:1 train loss:5.299271583557129 train acc:0.0\n",
      "epoch:1 train loss:5.2985520362854 train acc:0.0\n",
      "epoch:1 train loss:5.300684452056885 train acc:0.0\n",
      "epoch:1 train loss:5.296081066131592 train acc:0.0625\n",
      "epoch:1 train loss:5.29858922958374 train acc:0.0\n",
      "epoch:1 train loss:5.297869682312012 train acc:0.0\n",
      "epoch:1 train loss:5.296184539794922 train acc:0.0\n",
      "epoch:1 train loss:5.297938823699951 train acc:0.0\n",
      "epoch:1 train loss:5.298234462738037 train acc:0.0625\n",
      "epoch:1 train loss:5.29543924331665 train acc:0.0\n",
      "epoch:1 train loss:5.295248031616211 train acc:0.0\n",
      "epoch:1 train loss:5.2983622550964355 train acc:0.0\n",
      "epoch:1 train loss:5.298261642456055 train acc:0.0625\n",
      "epoch:1 train loss:5.2975640296936035 train acc:0.0625\n",
      "epoch:1 train loss:5.299108505249023 train acc:0.0625\n",
      "epoch:1 train loss:5.297574043273926 train acc:0.0\n",
      "epoch:1 train loss:5.298046112060547 train acc:0.0\n",
      "epoch:1 train loss:5.296988487243652 train acc:0.0\n",
      "epoch:1 train loss:5.298951625823975 train acc:0.0\n",
      "epoch:1 train loss:5.301934719085693 train acc:0.0\n",
      "epoch:1 train loss:5.299829006195068 train acc:0.0\n",
      "epoch:1 train loss:5.29990816116333 train acc:0.0\n",
      "epoch:1 train loss:5.296474456787109 train acc:0.0\n",
      "epoch:1 train loss:5.298974990844727 train acc:0.0\n",
      "epoch:1 train loss:5.298035621643066 train acc:0.0\n",
      "epoch:1 train loss:5.299623966217041 train acc:0.0\n",
      "epoch:1 train loss:5.301449775695801 train acc:0.0\n",
      "epoch:1 train loss:5.295989036560059 train acc:0.0\n",
      "epoch:1 train loss:5.292357444763184 train acc:0.0\n",
      "epoch:1 train loss:5.299985885620117 train acc:0.0625\n",
      "epoch:1 train loss:5.295691967010498 train acc:0.0625\n",
      "epoch:1 train loss:5.295439720153809 train acc:0.0\n",
      "epoch:1 train loss:5.300060272216797 train acc:0.0\n",
      "epoch:1 train loss:5.30073356628418 train acc:0.0\n",
      "epoch:1 train loss:5.295031547546387 train acc:0.0\n",
      "epoch:1 train loss:5.299484729766846 train acc:0.0\n",
      "epoch:1 train loss:5.295531749725342 train acc:0.0\n",
      "epoch:1 train loss:5.297669410705566 train acc:0.0\n",
      "epoch:1 train loss:5.299103260040283 train acc:0.0\n",
      "epoch:1 train loss:5.296927452087402 train acc:0.0\n",
      "epoch:1 train loss:5.297139644622803 train acc:0.0625\n",
      "epoch:1 train loss:5.300212383270264 train acc:0.0\n",
      "epoch:1 train loss:5.299172878265381 train acc:0.0625\n",
      "epoch:1 train loss:5.297398567199707 train acc:0.0\n",
      "epoch:1 train loss:5.30055570602417 train acc:0.0\n",
      "epoch:1 train loss:5.295903205871582 train acc:0.0\n",
      "epoch:1 train loss:5.301269054412842 train acc:0.0\n",
      "epoch:1 train loss:5.2984700202941895 train acc:0.0\n",
      "epoch:1 train loss:5.302040100097656 train acc:0.0\n",
      "epoch:1 train loss:5.295907974243164 train acc:0.0\n",
      "epoch:1 train loss:5.298956871032715 train acc:0.0\n",
      "epoch:1 train loss:5.2992377281188965 train acc:0.0\n",
      "epoch:1 train loss:5.30029821395874 train acc:0.0\n",
      "epoch:1 train loss:5.299201488494873 train acc:0.0\n",
      "epoch:1 train loss:5.298075199127197 train acc:0.0\n",
      "epoch:1 train loss:5.298773765563965 train acc:0.0\n",
      "epoch:1 train loss:5.298861503601074 train acc:0.0\n",
      "epoch:1 train loss:5.297810077667236 train acc:0.0\n",
      "epoch:1 train loss:5.300321102142334 train acc:0.0\n",
      "epoch:1 train loss:5.296196460723877 train acc:0.0\n",
      "epoch:1 train loss:5.298382759094238 train acc:0.0\n",
      "epoch:1 train loss:5.298001766204834 train acc:0.0\n",
      "epoch:1 train loss:5.297790050506592 train acc:0.0\n",
      "epoch:1 train loss:5.294239521026611 train acc:0.0\n",
      "epoch:1 train loss:5.298489570617676 train acc:0.0625\n",
      "epoch:1 train loss:5.300562858581543 train acc:0.0\n",
      "epoch:1 train loss:5.300426483154297 train acc:0.0\n",
      "epoch:1 train loss:5.301146030426025 train acc:0.0\n",
      "epoch:1 train loss:5.299859523773193 train acc:0.0\n",
      "epoch:1 train loss:5.296572685241699 train acc:0.0\n",
      "epoch:1 train loss:5.2974138259887695 train acc:0.0\n",
      "epoch:1 train loss:5.294486045837402 train acc:0.0\n",
      "epoch:1 train loss:5.297402858734131 train acc:0.0\n",
      "epoch:1 train loss:5.294907569885254 train acc:0.0\n",
      "epoch:1 train loss:5.301114082336426 train acc:0.0\n",
      "epoch:1 train loss:5.295979976654053 train acc:0.0\n",
      "epoch:1 train loss:5.299593925476074 train acc:0.0\n",
      "epoch:1 train loss:5.298243999481201 train acc:0.0\n",
      "epoch:1 train loss:5.301403999328613 train acc:0.0\n",
      "epoch:1 train loss:5.301278591156006 train acc:0.0\n",
      "epoch:1 train loss:5.298305988311768 train acc:0.0\n",
      "epoch:1 train loss:5.2952775955200195 train acc:0.0\n",
      "epoch:1 train loss:5.2970733642578125 train acc:0.0\n",
      "epoch:1 train loss:5.299668312072754 train acc:0.0\n",
      "epoch:1 train loss:5.301730155944824 train acc:0.0\n",
      "epoch:1 train loss:5.298350811004639 train acc:0.0\n",
      "epoch:1 train loss:5.300509452819824 train acc:0.0\n",
      "epoch:1 train loss:5.297111511230469 train acc:0.0\n",
      "epoch:1 train loss:5.298820495605469 train acc:0.0\n",
      "epoch:1 train loss:5.300233364105225 train acc:0.0\n",
      "epoch:1 train loss:5.297564506530762 train acc:0.0\n",
      "epoch:1 train loss:5.298266887664795 train acc:0.0\n",
      "epoch:1 train loss:5.295711517333984 train acc:0.0\n",
      "epoch:1 train loss:5.299651145935059 train acc:0.0\n",
      "epoch:1 train loss:5.301587104797363 train acc:0.0\n",
      "epoch:1 train loss:5.298651695251465 train acc:0.0\n",
      "epoch:1 train loss:5.297351360321045 train acc:0.0\n",
      "epoch:1 train loss:5.299209117889404 train acc:0.0\n",
      "epoch:1 train loss:5.297029495239258 train acc:0.0\n",
      "epoch:1 train loss:5.299544334411621 train acc:0.0\n",
      "epoch:1 train loss:5.298569202423096 train acc:0.0\n",
      "epoch:1 train loss:5.2987542152404785 train acc:0.0\n",
      "epoch:1 train loss:5.299558639526367 train acc:0.0\n",
      "epoch:1 train loss:5.2963409423828125 train acc:0.0\n",
      "epoch:1 train loss:5.295978546142578 train acc:0.0\n",
      "epoch:1 train loss:5.295795917510986 train acc:0.0\n",
      "epoch:1 train loss:5.300732612609863 train acc:0.0\n",
      "epoch:1 train loss:5.297739028930664 train acc:0.0\n",
      "epoch:1 train loss:5.296098232269287 train acc:0.0\n",
      "epoch:1 train loss:5.296280860900879 train acc:0.0\n",
      "epoch:1 train loss:5.30018949508667 train acc:0.0\n",
      "epoch:1 train loss:5.296100616455078 train acc:0.0\n",
      "epoch:1 train loss:5.301101207733154 train acc:0.0\n",
      "epoch:1 train loss:5.297699928283691 train acc:0.0\n",
      "epoch:1 train loss:5.297065258026123 train acc:0.0\n",
      "epoch:1 train loss:5.297585964202881 train acc:0.0\n",
      "epoch:1 train loss:5.3019561767578125 train acc:0.0\n",
      "epoch:1 train loss:5.297132968902588 train acc:0.0\n",
      "epoch:1 train loss:5.30015754699707 train acc:0.0\n",
      "epoch:1 train loss:5.296592712402344 train acc:0.0\n",
      "epoch:1 train loss:5.296265602111816 train acc:0.0\n",
      "epoch:1 train loss:5.295748710632324 train acc:0.0\n",
      "epoch:1 train loss:5.301085948944092 train acc:0.0625\n",
      "epoch:1 train loss:5.298691749572754 train acc:0.0\n",
      "epoch:1 train loss:5.297297954559326 train acc:0.0\n",
      "epoch:1 train loss:5.300850868225098 train acc:0.0\n",
      "epoch:1 train loss:5.300295352935791 train acc:0.0\n",
      "epoch:1 train loss:5.297873020172119 train acc:0.0\n",
      "epoch:1 train loss:5.29977560043335 train acc:0.0\n",
      "epoch:1 train loss:5.298773765563965 train acc:0.0\n",
      "epoch:1 train loss:5.299126625061035 train acc:0.0625\n",
      "epoch:1 train loss:5.300448894500732 train acc:0.0\n",
      "epoch:1 train loss:5.298920631408691 train acc:0.0\n",
      "epoch:1 train loss:5.299151420593262 train acc:0.0625\n",
      "epoch:1 train loss:5.298949718475342 train acc:0.0\n",
      "epoch:1 train loss:5.298198223114014 train acc:0.0625\n",
      "epoch:1 train loss:5.294488906860352 train acc:0.0\n",
      "epoch:1 train loss:5.300968170166016 train acc:0.0\n",
      "epoch:1 train loss:5.296858310699463 train acc:0.0\n",
      "epoch:1 train loss:5.298407554626465 train acc:0.0\n",
      "epoch:1 train loss:5.298010349273682 train acc:0.0\n",
      "epoch:1 train loss:5.2969465255737305 train acc:0.0\n",
      "epoch:1 train loss:5.298285484313965 train acc:0.0\n",
      "epoch:1 train loss:5.301480770111084 train acc:0.0\n",
      "epoch:1 train loss:5.299269676208496 train acc:0.0\n",
      "epoch:1 train loss:5.295043468475342 train acc:0.0\n",
      "epoch:1 train loss:5.295506477355957 train acc:0.0\n",
      "epoch:1 train loss:5.299193859100342 train acc:0.0\n",
      "epoch:1 train loss:5.297848224639893 train acc:0.0\n",
      "epoch:1 train loss:5.300507068634033 train acc:0.0\n",
      "epoch:1 train loss:5.296163558959961 train acc:0.0\n",
      "epoch:1 train loss:5.300252914428711 train acc:0.0\n",
      "epoch:1 train loss:5.297642230987549 train acc:0.0\n",
      "epoch:1 train loss:5.297505855560303 train acc:0.0\n",
      "epoch:1 train loss:5.299744606018066 train acc:0.0\n",
      "epoch:1 train loss:5.296006202697754 train acc:0.0\n",
      "epoch:1 train loss:5.298642158508301 train acc:0.0\n",
      "epoch:1 train loss:5.299428939819336 train acc:0.0\n",
      "epoch:1 train loss:5.299618244171143 train acc:0.0\n",
      "epoch:1 train loss:5.292037487030029 train acc:0.0\n",
      "epoch:1 train loss:5.296770095825195 train acc:0.0\n",
      "epoch:1 train loss:5.295640468597412 train acc:0.0\n",
      "epoch:1 train loss:5.299668788909912 train acc:0.0\n",
      "epoch:1 train loss:5.295914173126221 train acc:0.0\n",
      "epoch:1 train loss:5.298144340515137 train acc:0.0\n",
      "epoch:1 train loss:5.301978588104248 train acc:0.0\n",
      "epoch:1 train loss:5.300080299377441 train acc:0.0\n",
      "epoch:1 train loss:5.30045747756958 train acc:0.0\n",
      "epoch:1 train loss:5.297467231750488 train acc:0.0\n",
      "epoch:1 train loss:5.297366619110107 train acc:0.0\n",
      "epoch:1 train loss:5.298372268676758 train acc:0.0\n",
      "epoch:1 train loss:5.298512935638428 train acc:0.0\n",
      "epoch:1 train loss:5.297952651977539 train acc:0.0\n",
      "epoch:1 train loss:5.298279285430908 train acc:0.0\n",
      "epoch:1 train loss:5.298895835876465 train acc:0.0\n",
      "epoch:1 train loss:5.300198078155518 train acc:0.0\n",
      "epoch:1 train loss:5.296533107757568 train acc:0.0\n",
      "epoch:1 train loss:5.2968831062316895 train acc:0.0\n",
      "epoch:1 train loss:5.2983808517456055 train acc:0.0\n",
      "epoch:1 train loss:5.301777362823486 train acc:0.0\n",
      "epoch:1 train loss:5.2950663566589355 train acc:0.0\n",
      "epoch:1 train loss:5.2969207763671875 train acc:0.0\n",
      "epoch:1 train loss:5.298069000244141 train acc:0.0\n",
      "epoch:1 train loss:5.300278663635254 train acc:0.0\n",
      "epoch:1 train loss:5.300250053405762 train acc:0.0\n",
      "epoch:1 train loss:5.299660682678223 train acc:0.0\n",
      "epoch:1 train loss:5.299116134643555 train acc:0.0\n",
      "epoch:1 train loss:5.296935558319092 train acc:0.0\n",
      "epoch:1 train loss:5.298502445220947 train acc:0.0\n",
      "epoch:1 train loss:5.299921035766602 train acc:0.0\n",
      "epoch:1 train loss:5.298023223876953 train acc:0.0\n",
      "epoch:1 train loss:5.296844959259033 train acc:0.0625\n",
      "epoch:1 train loss:5.301529884338379 train acc:0.0\n",
      "epoch:1 train loss:5.298516273498535 train acc:0.0\n",
      "epoch:1 train loss:5.299169063568115 train acc:0.0\n",
      "epoch:1 train loss:5.297814846038818 train acc:0.0\n",
      "epoch:1 train loss:5.299134254455566 train acc:0.0\n",
      "epoch:1 train loss:5.298973083496094 train acc:0.0\n",
      "epoch:1 train loss:5.2995734214782715 train acc:0.0\n",
      "epoch:1 train loss:5.2979278564453125 train acc:0.0\n",
      "epoch:1 train loss:5.299568176269531 train acc:0.0\n",
      "epoch:1 train loss:5.300002098083496 train acc:0.0\n",
      "epoch:1 train loss:5.298370838165283 train acc:0.0\n",
      "epoch:1 train loss:5.2992167472839355 train acc:0.0\n",
      "epoch:1 train loss:5.295811653137207 train acc:0.0\n",
      "epoch:1 train loss:5.296767711639404 train acc:0.0\n",
      "epoch:1 train loss:5.300396919250488 train acc:0.0\n",
      "epoch:1 train loss:5.296773433685303 train acc:0.0\n",
      "epoch:1 train loss:5.297598361968994 train acc:0.0\n",
      "epoch:1 train loss:5.300694465637207 train acc:0.0\n",
      "epoch:1 train loss:5.293792724609375 train acc:0.0625\n",
      "epoch:1 train loss:5.297154903411865 train acc:0.0\n",
      "epoch:1 train loss:5.30225944519043 train acc:0.0\n",
      "epoch:1 train loss:5.297075271606445 train acc:0.0\n",
      "epoch:1 train loss:5.2931389808654785 train acc:0.0\n",
      "epoch:1 train loss:5.298366546630859 train acc:0.0\n",
      "epoch:1 train loss:5.298858642578125 train acc:0.0\n",
      "epoch:1 train loss:5.298308372497559 train acc:0.0625\n",
      "epoch:1 train loss:5.297688961029053 train acc:0.0\n",
      "epoch:1 train loss:5.295870304107666 train acc:0.0\n",
      "epoch:1 train loss:5.299277305603027 train acc:0.0\n",
      "epoch:1 train loss:5.300614833831787 train acc:0.0\n",
      "epoch:1 train loss:5.298334121704102 train acc:0.0\n",
      "epoch:1 train loss:5.298061847686768 train acc:0.0\n",
      "epoch:1 train loss:5.297584533691406 train acc:0.0\n",
      "epoch:1 train loss:5.298818588256836 train acc:0.0\n",
      "epoch:1 train loss:5.297918796539307 train acc:0.0\n",
      "epoch:1 train loss:5.293932914733887 train acc:0.0\n",
      "epoch:1 train loss:5.29899787902832 train acc:0.0\n",
      "epoch:1 train loss:5.298535346984863 train acc:0.0\n",
      "epoch:1 train loss:5.299432754516602 train acc:0.0\n",
      "epoch:1 train loss:5.296257495880127 train acc:0.0\n",
      "epoch:1 train loss:5.2986321449279785 train acc:0.0\n",
      "epoch:1 train loss:5.302082538604736 train acc:0.0\n",
      "epoch:1 train loss:5.297323226928711 train acc:0.0\n",
      "epoch:1 train loss:5.301374912261963 train acc:0.0\n",
      "epoch:1 train loss:5.302104473114014 train acc:0.0\n",
      "epoch:1 train loss:5.296571254730225 train acc:0.0\n",
      "epoch:1 train loss:5.295449733734131 train acc:0.0\n",
      "epoch:1 train loss:5.299384117126465 train acc:0.0\n",
      "epoch:1 train loss:5.302197456359863 train acc:0.0\n",
      "epoch:1 train loss:5.295094966888428 train acc:0.0\n",
      "epoch:1 train loss:5.294436931610107 train acc:0.0625\n",
      "epoch:1 train loss:5.2991862297058105 train acc:0.0\n",
      "epoch:1 train loss:5.2996826171875 train acc:0.0\n",
      "epoch:1 train loss:5.299093246459961 train acc:0.0\n",
      "epoch:1 train loss:5.29809045791626 train acc:0.0\n",
      "epoch:1 train loss:5.296761989593506 train acc:0.0\n",
      "epoch:1 train loss:5.300606727600098 train acc:0.0\n",
      "epoch:1 train loss:5.296491622924805 train acc:0.0\n",
      "epoch:1 train loss:5.2995500564575195 train acc:0.0\n",
      "epoch:1 train loss:5.298269748687744 train acc:0.0\n",
      "epoch:1 train loss:5.29623556137085 train acc:0.0\n",
      "epoch:1 train loss:5.301047325134277 train acc:0.0\n",
      "epoch:1 train loss:5.300198554992676 train acc:0.0\n",
      "epoch:1 train loss:5.299696445465088 train acc:0.0\n",
      "epoch:1 train loss:5.297729015350342 train acc:0.0\n",
      "epoch:1 train loss:5.298314094543457 train acc:0.0\n",
      "epoch:1 train loss:5.299315452575684 train acc:0.0\n",
      "epoch:1 train loss:5.298862457275391 train acc:0.0\n",
      "epoch:1 train loss:5.29397439956665 train acc:0.0625\n",
      "epoch:1 train loss:5.29628324508667 train acc:0.0\n",
      "epoch:1 train loss:5.298404693603516 train acc:0.0\n",
      "epoch:1 train loss:5.298473834991455 train acc:0.0\n",
      "epoch:1 train loss:5.300088882446289 train acc:0.0\n",
      "epoch:1 train loss:5.298903465270996 train acc:0.0\n",
      "epoch:1 train loss:5.296938896179199 train acc:0.0\n",
      "epoch:1 train loss:5.299137115478516 train acc:0.0\n",
      "epoch:1 train loss:5.296587944030762 train acc:0.0\n",
      "epoch:1 train loss:5.297032356262207 train acc:0.0\n",
      "epoch:1 train loss:5.297064304351807 train acc:0.0\n",
      "epoch:1 train loss:5.299318313598633 train acc:0.0\n",
      "epoch:1 train loss:5.2971577644348145 train acc:0.0\n",
      "epoch:1 train loss:5.296722412109375 train acc:0.0\n",
      "epoch:1 train loss:5.299428939819336 train acc:0.0625\n",
      "epoch:1 train loss:5.296130657196045 train acc:0.0\n",
      "epoch:1 train loss:5.296805381774902 train acc:0.0\n",
      "epoch:1 train loss:5.297003746032715 train acc:0.0\n",
      "epoch:1 train loss:5.300319671630859 train acc:0.0\n",
      "epoch:1 train loss:5.301241397857666 train acc:0.0\n",
      "epoch:1 train loss:5.297977924346924 train acc:0.0\n",
      "epoch:1 train loss:5.30027961730957 train acc:0.0\n",
      "epoch:1 train loss:5.3004150390625 train acc:0.0\n",
      "epoch:1 train loss:5.299421310424805 train acc:0.0\n",
      "epoch:1 train loss:5.2965779304504395 train acc:0.0\n",
      "epoch:1 train loss:5.300622940063477 train acc:0.0\n",
      "epoch:1 train loss:5.296534061431885 train acc:0.0\n",
      "epoch:1 train loss:5.296415328979492 train acc:0.0\n",
      "epoch:1 train loss:5.302684783935547 train acc:0.0\n",
      "epoch:1 train loss:5.294492721557617 train acc:0.0\n",
      "epoch:1 train loss:5.299134731292725 train acc:0.0\n",
      "epoch:1 train loss:5.29718017578125 train acc:0.0\n",
      "epoch:1 train loss:5.301175117492676 train acc:0.0\n",
      "epoch:1 train loss:5.298073768615723 train acc:0.0\n",
      "epoch:1 train loss:5.296655178070068 train acc:0.0\n",
      "epoch:1 train loss:5.294774532318115 train acc:0.0\n",
      "epoch:1 train loss:5.299820423126221 train acc:0.0\n",
      "epoch:1 train loss:5.299522876739502 train acc:0.0\n",
      "epoch:1 train loss:5.297342300415039 train acc:0.0\n",
      "epoch:1 train loss:5.3006591796875 train acc:0.0625\n",
      "epoch:1 train loss:5.295561790466309 train acc:0.0\n",
      "epoch:1 train loss:5.295445919036865 train acc:0.0\n",
      "epoch:1 train loss:5.295825481414795 train acc:0.0\n",
      "epoch:1 train loss:5.298276901245117 train acc:0.0\n",
      "epoch:1 train loss:5.300745964050293 train acc:0.0\n",
      "epoch:1 train loss:5.299375534057617 train acc:0.0\n",
      "epoch:1 train loss:5.298390865325928 train acc:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 train loss:5.299102306365967 train acc:0.0\n",
      "epoch:1 train loss:5.2978949546813965 train acc:0.0\n",
      "epoch:1 train loss:5.302248001098633 train acc:0.0\n",
      "epoch:1 train loss:5.2970733642578125 train acc:0.0\n",
      "epoch:1 train loss:5.296627044677734 train acc:0.0\n",
      "epoch:1 train loss:5.299118518829346 train acc:0.0\n",
      "epoch:1 train loss:5.300253391265869 train acc:0.0\n",
      "epoch:1 train loss:5.297296524047852 train acc:0.0\n",
      "epoch:1 train loss:5.298335075378418 train acc:0.0\n",
      "epoch:1 train loss:5.294458866119385 train acc:0.0\n",
      "epoch:1 train loss:5.295547962188721 train acc:0.0\n",
      "epoch:1 train loss:5.298795700073242 train acc:0.0\n",
      "epoch:1 train loss:5.300189971923828 train acc:0.0\n",
      "epoch:1 train loss:5.299688339233398 train acc:0.0\n",
      "epoch:1 train loss:5.293947219848633 train acc:0.0\n",
      "epoch:1 train loss:5.298079490661621 train acc:0.0\n",
      "epoch:1 train loss:5.299742221832275 train acc:0.0\n",
      "epoch:1 train loss:5.298617362976074 train acc:0.0625\n",
      "epoch:1 train loss:5.297658443450928 train acc:0.0\n",
      "epoch:1 train loss:5.299485206604004 train acc:0.0\n",
      "epoch:1 train loss:5.299807071685791 train acc:0.0\n",
      "epoch:1 train loss:5.301278591156006 train acc:0.0\n",
      "epoch:1 train loss:5.300565719604492 train acc:0.0\n",
      "epoch:1 train loss:5.298526763916016 train acc:0.0\n",
      "epoch:1 train loss:5.300076961517334 train acc:0.0\n",
      "epoch:1 train loss:5.299696922302246 train acc:0.0\n",
      "epoch:1 train loss:5.3023223876953125 train acc:0.0\n",
      "epoch:1 train loss:5.2987565994262695 train acc:0.0\n",
      "epoch:1 train loss:5.297693252563477 train acc:0.0\n",
      "epoch:1 train loss:5.300191402435303 train acc:0.0\n",
      "epoch:1 train loss:5.2984395027160645 train acc:0.0\n",
      "epoch:1 train loss:5.297761917114258 train acc:0.0\n",
      "epoch:1 train loss:5.298990249633789 train acc:0.0\n",
      "epoch:1 train loss:5.298137187957764 train acc:0.0\n",
      "epoch:1 train loss:5.2968292236328125 train acc:0.0\n",
      "epoch:1 train loss:5.299295425415039 train acc:0.0\n",
      "epoch:1 train loss:5.298280239105225 train acc:0.0\n",
      "epoch:1 train loss:5.296139717102051 train acc:0.0\n",
      "epoch:1 train loss:5.29879903793335 train acc:0.0\n",
      "epoch:1 train loss:5.298631191253662 train acc:0.0\n",
      "epoch:1 train loss:5.298189640045166 train acc:0.0\n",
      "epoch:1 train loss:5.299281120300293 train acc:0.0\n",
      "epoch:1 train loss:5.294297695159912 train acc:0.0\n",
      "epoch:1 train loss:5.299410343170166 train acc:0.0\n",
      "epoch:1 train loss:5.2969489097595215 train acc:0.0\n",
      "epoch:1 train loss:5.3015265464782715 train acc:0.0\n",
      "epoch:1 train loss:5.29929780960083 train acc:0.0\n",
      "epoch:1 train loss:5.298736095428467 train acc:0.0\n",
      "epoch:1 train loss:5.3006157875061035 train acc:0.0\n",
      "epoch:1 train loss:5.30164098739624 train acc:0.0625\n",
      "epoch:1 train loss:5.297569274902344 train acc:0.0\n",
      "epoch:1 train loss:5.300265312194824 train acc:0.0\n",
      "epoch:1 train loss:5.29754114151001 train acc:0.0\n",
      "epoch:1 train loss:5.2957258224487305 train acc:0.0625\n",
      "epoch:1 train loss:5.2981767654418945 train acc:0.0\n",
      "epoch:1 train loss:5.303704261779785 train acc:0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1400b69ccd8e4b11bb9b58abc49759f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=363), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:2 train loss:5.298397541046143 train acc:0.0\n",
      "epoch:2 train loss:5.2989630699157715 train acc:0.0\n",
      "epoch:2 train loss:5.299751281738281 train acc:0.0\n",
      "epoch:2 train loss:5.301172256469727 train acc:0.0\n",
      "epoch:2 train loss:5.299623489379883 train acc:0.0\n",
      "epoch:2 train loss:5.299569129943848 train acc:0.0\n",
      "epoch:2 train loss:5.298774719238281 train acc:0.0\n",
      "epoch:2 train loss:5.295257091522217 train acc:0.0\n",
      "epoch:2 train loss:5.298083305358887 train acc:0.0\n",
      "epoch:2 train loss:5.298882484436035 train acc:0.0\n",
      "epoch:2 train loss:5.294376850128174 train acc:0.0\n",
      "epoch:2 train loss:5.298440933227539 train acc:0.0\n",
      "epoch:2 train loss:5.297898292541504 train acc:0.0\n",
      "epoch:2 train loss:5.29632568359375 train acc:0.0\n",
      "epoch:2 train loss:5.29530143737793 train acc:0.0\n",
      "epoch:2 train loss:5.299814224243164 train acc:0.0\n",
      "epoch:2 train loss:5.296966075897217 train acc:0.0\n",
      "epoch:2 train loss:5.2960991859436035 train acc:0.0\n",
      "epoch:2 train loss:5.300508975982666 train acc:0.0\n",
      "epoch:2 train loss:5.302642822265625 train acc:0.0\n",
      "epoch:2 train loss:5.297395706176758 train acc:0.0\n",
      "epoch:2 train loss:5.297058582305908 train acc:0.0\n",
      "epoch:2 train loss:5.299560546875 train acc:0.0625\n",
      "epoch:2 train loss:5.295749187469482 train acc:0.0\n",
      "epoch:2 train loss:5.297844409942627 train acc:0.0\n",
      "epoch:2 train loss:5.3009514808654785 train acc:0.0\n",
      "epoch:2 train loss:5.301538944244385 train acc:0.0\n",
      "epoch:2 train loss:5.299134731292725 train acc:0.0\n",
      "epoch:2 train loss:5.298617362976074 train acc:0.0\n",
      "epoch:2 train loss:5.299217224121094 train acc:0.0\n",
      "epoch:2 train loss:5.299850940704346 train acc:0.0\n",
      "epoch:2 train loss:5.298524856567383 train acc:0.0\n",
      "epoch:2 train loss:5.29954195022583 train acc:0.0\n",
      "epoch:2 train loss:5.299126148223877 train acc:0.0\n",
      "epoch:2 train loss:5.297589302062988 train acc:0.0\n",
      "epoch:2 train loss:5.299138069152832 train acc:0.0\n",
      "epoch:2 train loss:5.298518657684326 train acc:0.0\n",
      "epoch:2 train loss:5.298070430755615 train acc:0.0\n",
      "epoch:2 train loss:5.300421714782715 train acc:0.0\n",
      "epoch:2 train loss:5.298349857330322 train acc:0.0625\n",
      "epoch:2 train loss:5.298959732055664 train acc:0.0\n",
      "epoch:2 train loss:5.300905227661133 train acc:0.0\n",
      "epoch:2 train loss:5.297778606414795 train acc:0.0625\n",
      "epoch:2 train loss:5.297708034515381 train acc:0.0\n",
      "epoch:2 train loss:5.296662330627441 train acc:0.0\n",
      "epoch:2 train loss:5.297110080718994 train acc:0.0625\n",
      "epoch:2 train loss:5.295922756195068 train acc:0.0\n",
      "epoch:2 train loss:5.299927234649658 train acc:0.0\n",
      "epoch:2 train loss:5.298793315887451 train acc:0.0625\n",
      "epoch:2 train loss:5.299377918243408 train acc:0.0\n",
      "epoch:2 train loss:5.297401428222656 train acc:0.0\n",
      "epoch:2 train loss:5.295083522796631 train acc:0.0\n",
      "epoch:2 train loss:5.296719551086426 train acc:0.0\n",
      "epoch:2 train loss:5.297200679779053 train acc:0.0\n",
      "epoch:2 train loss:5.295816421508789 train acc:0.0\n",
      "epoch:2 train loss:5.297504425048828 train acc:0.0\n",
      "epoch:2 train loss:5.297752857208252 train acc:0.0\n",
      "epoch:2 train loss:5.298627853393555 train acc:0.0\n",
      "epoch:2 train loss:5.298374652862549 train acc:0.0\n",
      "epoch:2 train loss:5.29810905456543 train acc:0.0\n",
      "epoch:2 train loss:5.296627044677734 train acc:0.0\n",
      "epoch:2 train loss:5.295531749725342 train acc:0.0\n",
      "epoch:2 train loss:5.300280570983887 train acc:0.0\n",
      "epoch:2 train loss:5.301586151123047 train acc:0.0\n",
      "epoch:2 train loss:5.300991535186768 train acc:0.0\n",
      "epoch:2 train loss:5.300321578979492 train acc:0.0\n",
      "epoch:2 train loss:5.29946231842041 train acc:0.0\n",
      "epoch:2 train loss:5.301215648651123 train acc:0.0\n",
      "epoch:2 train loss:5.298001289367676 train acc:0.0\n",
      "epoch:2 train loss:5.299510478973389 train acc:0.0\n",
      "epoch:2 train loss:5.2971625328063965 train acc:0.0\n",
      "epoch:2 train loss:5.294360160827637 train acc:0.0\n",
      "epoch:2 train loss:5.297109127044678 train acc:0.0\n",
      "epoch:2 train loss:5.297269821166992 train acc:0.0\n",
      "epoch:2 train loss:5.301964282989502 train acc:0.0\n",
      "epoch:2 train loss:5.298974990844727 train acc:0.0\n",
      "epoch:2 train loss:5.297440528869629 train acc:0.0\n",
      "epoch:2 train loss:5.298338413238525 train acc:0.0\n",
      "epoch:2 train loss:5.2979736328125 train acc:0.0\n",
      "epoch:2 train loss:5.295879364013672 train acc:0.0\n",
      "epoch:2 train loss:5.298205375671387 train acc:0.0\n",
      "epoch:2 train loss:5.2994384765625 train acc:0.0\n",
      "epoch:2 train loss:5.298824787139893 train acc:0.0\n",
      "epoch:2 train loss:5.296244144439697 train acc:0.0\n",
      "epoch:2 train loss:5.298712730407715 train acc:0.0\n",
      "epoch:2 train loss:5.292938709259033 train acc:0.0\n",
      "epoch:2 train loss:5.30185079574585 train acc:0.0\n",
      "epoch:2 train loss:5.296350479125977 train acc:0.0\n",
      "epoch:2 train loss:5.301091194152832 train acc:0.0\n",
      "epoch:2 train loss:5.29910135269165 train acc:0.0\n",
      "epoch:2 train loss:5.299848556518555 train acc:0.0\n",
      "epoch:2 train loss:5.299878120422363 train acc:0.0\n",
      "epoch:2 train loss:5.300301551818848 train acc:0.0\n",
      "epoch:2 train loss:5.295284748077393 train acc:0.0625\n",
      "epoch:2 train loss:5.298229694366455 train acc:0.0\n",
      "epoch:2 train loss:5.300247669219971 train acc:0.0\n",
      "epoch:2 train loss:5.300216197967529 train acc:0.0\n",
      "epoch:2 train loss:5.298268795013428 train acc:0.0\n",
      "epoch:2 train loss:5.29949951171875 train acc:0.0\n",
      "epoch:2 train loss:5.299522876739502 train acc:0.0\n",
      "epoch:2 train loss:5.297922134399414 train acc:0.0\n",
      "epoch:2 train loss:5.29752779006958 train acc:0.0\n",
      "epoch:2 train loss:5.2967681884765625 train acc:0.0\n",
      "epoch:2 train loss:5.29213285446167 train acc:0.0625\n",
      "epoch:2 train loss:5.2963972091674805 train acc:0.0\n",
      "epoch:2 train loss:5.29577112197876 train acc:0.0625\n",
      "epoch:2 train loss:5.298640727996826 train acc:0.0\n",
      "epoch:2 train loss:5.300901889801025 train acc:0.0\n",
      "epoch:2 train loss:5.294360160827637 train acc:0.0\n",
      "epoch:2 train loss:5.300349235534668 train acc:0.0\n",
      "epoch:2 train loss:5.29974889755249 train acc:0.0\n",
      "epoch:2 train loss:5.300950050354004 train acc:0.0\n",
      "epoch:2 train loss:5.298269748687744 train acc:0.0\n",
      "epoch:2 train loss:5.299805641174316 train acc:0.0625\n",
      "epoch:2 train loss:5.299365043640137 train acc:0.0\n",
      "epoch:2 train loss:5.294410705566406 train acc:0.0\n",
      "epoch:2 train loss:5.299131393432617 train acc:0.0\n",
      "epoch:2 train loss:5.296367645263672 train acc:0.0\n",
      "epoch:2 train loss:5.299625396728516 train acc:0.0\n",
      "epoch:2 train loss:5.297336101531982 train acc:0.0\n",
      "epoch:2 train loss:5.297761917114258 train acc:0.0625\n",
      "epoch:2 train loss:5.301183700561523 train acc:0.0\n",
      "epoch:2 train loss:5.299688816070557 train acc:0.0\n",
      "epoch:2 train loss:5.3001275062561035 train acc:0.0\n",
      "epoch:2 train loss:5.294253826141357 train acc:0.0625\n",
      "epoch:2 train loss:5.2954912185668945 train acc:0.0\n",
      "epoch:2 train loss:5.2977752685546875 train acc:0.0\n",
      "epoch:2 train loss:5.296496391296387 train acc:0.0\n",
      "epoch:2 train loss:5.298197269439697 train acc:0.0\n",
      "epoch:2 train loss:5.301758289337158 train acc:0.0\n",
      "epoch:2 train loss:5.297423839569092 train acc:0.0625\n",
      "epoch:2 train loss:5.299517631530762 train acc:0.0\n",
      "epoch:2 train loss:5.297513961791992 train acc:0.0\n",
      "epoch:2 train loss:5.2995758056640625 train acc:0.0\n",
      "epoch:2 train loss:5.297299861907959 train acc:0.0625\n",
      "epoch:2 train loss:5.298821449279785 train acc:0.0\n",
      "epoch:2 train loss:5.29992151260376 train acc:0.0\n",
      "epoch:2 train loss:5.297513961791992 train acc:0.0\n",
      "epoch:2 train loss:5.298049449920654 train acc:0.0\n",
      "epoch:2 train loss:5.296113967895508 train acc:0.0625\n",
      "epoch:2 train loss:5.299788475036621 train acc:0.0\n",
      "epoch:2 train loss:5.296850681304932 train acc:0.0\n",
      "epoch:2 train loss:5.298930644989014 train acc:0.0\n",
      "epoch:2 train loss:5.297313213348389 train acc:0.0\n",
      "epoch:2 train loss:5.299082279205322 train acc:0.0\n",
      "epoch:2 train loss:5.299269676208496 train acc:0.0\n",
      "epoch:2 train loss:5.2972917556762695 train acc:0.0\n",
      "epoch:2 train loss:5.301055908203125 train acc:0.0\n",
      "epoch:2 train loss:5.297994136810303 train acc:0.0\n",
      "epoch:2 train loss:5.294669151306152 train acc:0.0\n",
      "epoch:2 train loss:5.294251918792725 train acc:0.0\n",
      "epoch:2 train loss:5.300572395324707 train acc:0.0\n",
      "epoch:2 train loss:5.29755973815918 train acc:0.0\n",
      "epoch:2 train loss:5.2985734939575195 train acc:0.0\n",
      "epoch:2 train loss:5.298813819885254 train acc:0.0\n",
      "epoch:2 train loss:5.298521995544434 train acc:0.0\n",
      "epoch:2 train loss:5.298877239227295 train acc:0.0\n",
      "epoch:2 train loss:5.299659252166748 train acc:0.0\n",
      "epoch:2 train loss:5.298635482788086 train acc:0.0\n",
      "epoch:2 train loss:5.298487663269043 train acc:0.0\n",
      "epoch:2 train loss:5.298269748687744 train acc:0.0\n",
      "epoch:2 train loss:5.297288417816162 train acc:0.0\n",
      "epoch:2 train loss:5.298464775085449 train acc:0.0\n",
      "epoch:2 train loss:5.295752048492432 train acc:0.0\n",
      "epoch:2 train loss:5.301474571228027 train acc:0.0\n",
      "epoch:2 train loss:5.299481391906738 train acc:0.0\n",
      "epoch:2 train loss:5.297088146209717 train acc:0.0\n",
      "epoch:2 train loss:5.298978805541992 train acc:0.0\n",
      "epoch:2 train loss:5.2968621253967285 train acc:0.0\n",
      "epoch:2 train loss:5.29799747467041 train acc:0.0\n",
      "epoch:2 train loss:5.299978733062744 train acc:0.0\n",
      "epoch:2 train loss:5.296158790588379 train acc:0.0\n",
      "epoch:2 train loss:5.299249649047852 train acc:0.0\n",
      "epoch:2 train loss:5.296431541442871 train acc:0.0\n",
      "epoch:2 train loss:5.300274848937988 train acc:0.0\n",
      "epoch:2 train loss:5.300590991973877 train acc:0.0\n",
      "epoch:2 train loss:5.298467636108398 train acc:0.0\n",
      "epoch:2 train loss:5.2972259521484375 train acc:0.0\n",
      "epoch:2 train loss:5.2970170974731445 train acc:0.0\n",
      "epoch:2 train loss:5.294782638549805 train acc:0.0\n",
      "epoch:2 train loss:5.299561500549316 train acc:0.0\n",
      "epoch:2 train loss:5.299006462097168 train acc:0.0\n",
      "epoch:2 train loss:5.299589157104492 train acc:0.0\n",
      "epoch:2 train loss:5.297222137451172 train acc:0.0\n",
      "epoch:2 train loss:5.302419662475586 train acc:0.0\n",
      "epoch:2 train loss:5.298019886016846 train acc:0.0\n",
      "epoch:2 train loss:5.2966108322143555 train acc:0.0\n",
      "epoch:2 train loss:5.298519611358643 train acc:0.0\n",
      "epoch:2 train loss:5.299206733703613 train acc:0.0\n",
      "epoch:2 train loss:5.300360679626465 train acc:0.0\n",
      "epoch:2 train loss:5.296675682067871 train acc:0.125\n",
      "epoch:2 train loss:5.301671028137207 train acc:0.0\n",
      "epoch:2 train loss:5.297504901885986 train acc:0.0\n",
      "epoch:2 train loss:5.298189163208008 train acc:0.0\n",
      "epoch:2 train loss:5.299984455108643 train acc:0.0\n",
      "epoch:2 train loss:5.298697471618652 train acc:0.0\n",
      "epoch:2 train loss:5.296785354614258 train acc:0.0625\n",
      "epoch:2 train loss:5.299619197845459 train acc:0.0\n",
      "epoch:2 train loss:5.296761989593506 train acc:0.0\n",
      "epoch:2 train loss:5.299183368682861 train acc:0.0\n",
      "epoch:2 train loss:5.295800685882568 train acc:0.0\n",
      "epoch:2 train loss:5.297449111938477 train acc:0.0\n",
      "epoch:2 train loss:5.300037860870361 train acc:0.0\n",
      "epoch:2 train loss:5.295456886291504 train acc:0.0\n",
      "epoch:2 train loss:5.296089172363281 train acc:0.0\n",
      "epoch:2 train loss:5.300120830535889 train acc:0.0\n",
      "epoch:2 train loss:5.298694610595703 train acc:0.0\n",
      "epoch:2 train loss:5.297539710998535 train acc:0.0\n",
      "epoch:2 train loss:5.297517776489258 train acc:0.0\n",
      "epoch:2 train loss:5.299184799194336 train acc:0.0\n",
      "epoch:2 train loss:5.3005805015563965 train acc:0.0\n",
      "epoch:2 train loss:5.297525882720947 train acc:0.0\n",
      "epoch:2 train loss:5.29577112197876 train acc:0.0\n",
      "epoch:2 train loss:5.297236442565918 train acc:0.0\n",
      "epoch:2 train loss:5.297677516937256 train acc:0.0\n",
      "epoch:2 train loss:5.298267364501953 train acc:0.0\n",
      "epoch:2 train loss:5.297425270080566 train acc:0.0\n",
      "epoch:2 train loss:5.297797203063965 train acc:0.0\n",
      "epoch:2 train loss:5.301399230957031 train acc:0.0\n",
      "epoch:2 train loss:5.3009138107299805 train acc:0.0\n",
      "epoch:2 train loss:5.29683256149292 train acc:0.0\n",
      "epoch:2 train loss:5.300149440765381 train acc:0.0\n",
      "epoch:2 train loss:5.297645092010498 train acc:0.0\n",
      "epoch:2 train loss:5.300096035003662 train acc:0.0\n",
      "epoch:2 train loss:5.2986741065979 train acc:0.0\n",
      "epoch:2 train loss:5.296734809875488 train acc:0.0\n",
      "epoch:2 train loss:5.29901123046875 train acc:0.0\n",
      "epoch:2 train loss:5.296342849731445 train acc:0.0\n",
      "epoch:2 train loss:5.300220966339111 train acc:0.0\n",
      "epoch:2 train loss:5.297353267669678 train acc:0.0\n",
      "epoch:2 train loss:5.300496578216553 train acc:0.0\n",
      "epoch:2 train loss:5.298986434936523 train acc:0.0\n",
      "epoch:2 train loss:5.302645683288574 train acc:0.0\n",
      "epoch:2 train loss:5.299280166625977 train acc:0.0\n",
      "epoch:2 train loss:5.301474571228027 train acc:0.0\n",
      "epoch:2 train loss:5.298513412475586 train acc:0.0\n",
      "epoch:2 train loss:5.297909259796143 train acc:0.0\n",
      "epoch:2 train loss:5.300477981567383 train acc:0.0\n",
      "epoch:2 train loss:5.298475742340088 train acc:0.0\n",
      "epoch:2 train loss:5.300086498260498 train acc:0.0\n",
      "epoch:2 train loss:5.300403118133545 train acc:0.0\n",
      "epoch:2 train loss:5.298598289489746 train acc:0.0\n",
      "epoch:2 train loss:5.298076629638672 train acc:0.0\n",
      "epoch:2 train loss:5.296497344970703 train acc:0.0\n",
      "epoch:2 train loss:5.2986907958984375 train acc:0.0\n",
      "epoch:2 train loss:5.2981390953063965 train acc:0.0\n",
      "epoch:2 train loss:5.300711154937744 train acc:0.0\n",
      "epoch:2 train loss:5.296501636505127 train acc:0.0\n",
      "epoch:2 train loss:5.2983808517456055 train acc:0.0\n",
      "epoch:2 train loss:5.296492576599121 train acc:0.0\n",
      "epoch:2 train loss:5.29355525970459 train acc:0.0\n",
      "epoch:2 train loss:5.295053958892822 train acc:0.0\n",
      "epoch:2 train loss:5.29846715927124 train acc:0.0\n",
      "epoch:2 train loss:5.299939155578613 train acc:0.0\n",
      "epoch:2 train loss:5.296126365661621 train acc:0.0\n",
      "epoch:2 train loss:5.2999467849731445 train acc:0.0\n",
      "epoch:2 train loss:5.297327041625977 train acc:0.0\n",
      "epoch:2 train loss:5.297732353210449 train acc:0.0\n",
      "epoch:2 train loss:5.299971580505371 train acc:0.0\n",
      "epoch:2 train loss:5.296924114227295 train acc:0.0\n",
      "epoch:2 train loss:5.299775123596191 train acc:0.0\n",
      "epoch:2 train loss:5.296023845672607 train acc:0.0\n",
      "epoch:2 train loss:5.301305294036865 train acc:0.0\n",
      "epoch:2 train loss:5.294160842895508 train acc:0.0\n",
      "epoch:2 train loss:5.2983880043029785 train acc:0.0\n",
      "epoch:2 train loss:5.298985004425049 train acc:0.0\n",
      "epoch:2 train loss:5.299630641937256 train acc:0.0\n",
      "epoch:2 train loss:5.2982587814331055 train acc:0.0\n",
      "epoch:2 train loss:5.300658702850342 train acc:0.0\n",
      "epoch:2 train loss:5.295637130737305 train acc:0.0\n",
      "epoch:2 train loss:5.299304962158203 train acc:0.0\n",
      "epoch:2 train loss:5.294968605041504 train acc:0.0\n",
      "epoch:2 train loss:5.3003249168396 train acc:0.0\n",
      "epoch:2 train loss:5.298935413360596 train acc:0.0\n",
      "epoch:2 train loss:5.2995805740356445 train acc:0.0\n",
      "epoch:2 train loss:5.298990726470947 train acc:0.0\n",
      "epoch:2 train loss:5.300178050994873 train acc:0.0\n",
      "epoch:2 train loss:5.296416759490967 train acc:0.0625\n",
      "epoch:2 train loss:5.300275802612305 train acc:0.0\n",
      "epoch:2 train loss:5.300119876861572 train acc:0.0\n",
      "epoch:2 train loss:5.299317359924316 train acc:0.0\n",
      "epoch:2 train loss:5.301722049713135 train acc:0.0\n",
      "epoch:2 train loss:5.296188831329346 train acc:0.0\n",
      "epoch:2 train loss:5.296523094177246 train acc:0.0625\n",
      "epoch:2 train loss:5.296054840087891 train acc:0.0\n",
      "epoch:2 train loss:5.295901298522949 train acc:0.0\n",
      "epoch:2 train loss:5.2986369132995605 train acc:0.0\n",
      "epoch:2 train loss:5.299805641174316 train acc:0.0\n",
      "epoch:2 train loss:5.298174858093262 train acc:0.0\n",
      "epoch:2 train loss:5.29792594909668 train acc:0.0625\n",
      "epoch:2 train loss:5.297176837921143 train acc:0.0\n",
      "epoch:2 train loss:5.29901647567749 train acc:0.0\n",
      "epoch:2 train loss:5.296881198883057 train acc:0.0\n",
      "epoch:2 train loss:5.2966790199279785 train acc:0.0625\n",
      "epoch:2 train loss:5.299043655395508 train acc:0.0\n",
      "epoch:2 train loss:5.3002543449401855 train acc:0.0\n",
      "epoch:2 train loss:5.297073841094971 train acc:0.0\n",
      "epoch:2 train loss:5.2992377281188965 train acc:0.0\n",
      "epoch:2 train loss:5.295837879180908 train acc:0.0\n",
      "epoch:2 train loss:5.299072265625 train acc:0.0\n",
      "epoch:2 train loss:5.297329902648926 train acc:0.0\n",
      "epoch:2 train loss:5.298669338226318 train acc:0.0\n",
      "epoch:2 train loss:5.298679351806641 train acc:0.0\n",
      "epoch:2 train loss:5.298649787902832 train acc:0.0\n",
      "epoch:2 train loss:5.2975754737854 train acc:0.0\n",
      "epoch:2 train loss:5.296894550323486 train acc:0.0\n",
      "epoch:2 train loss:5.300384998321533 train acc:0.0\n",
      "epoch:2 train loss:5.298737049102783 train acc:0.0\n",
      "epoch:2 train loss:5.29896879196167 train acc:0.0\n",
      "epoch:2 train loss:5.299124717712402 train acc:0.0\n",
      "epoch:2 train loss:5.298659801483154 train acc:0.0\n",
      "epoch:2 train loss:5.296002388000488 train acc:0.0\n",
      "epoch:2 train loss:5.301990509033203 train acc:0.0\n",
      "epoch:2 train loss:5.298954010009766 train acc:0.0\n",
      "epoch:2 train loss:5.299493789672852 train acc:0.0\n",
      "epoch:2 train loss:5.295442581176758 train acc:0.0\n",
      "epoch:2 train loss:5.30098295211792 train acc:0.0\n",
      "epoch:2 train loss:5.299220085144043 train acc:0.0\n",
      "epoch:2 train loss:5.2980427742004395 train acc:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 train loss:5.297835826873779 train acc:0.0\n",
      "epoch:2 train loss:5.298617839813232 train acc:0.0\n",
      "epoch:2 train loss:5.296379089355469 train acc:0.125\n",
      "epoch:2 train loss:5.298405647277832 train acc:0.0\n",
      "epoch:2 train loss:5.298053741455078 train acc:0.0\n",
      "epoch:2 train loss:5.29617166519165 train acc:0.0625\n",
      "epoch:2 train loss:5.300352096557617 train acc:0.0\n",
      "epoch:2 train loss:5.296236515045166 train acc:0.0625\n",
      "epoch:2 train loss:5.299568176269531 train acc:0.0\n",
      "epoch:2 train loss:5.298504829406738 train acc:0.0\n",
      "epoch:2 train loss:5.297752380371094 train acc:0.0\n",
      "epoch:2 train loss:5.2988433837890625 train acc:0.0\n",
      "epoch:2 train loss:5.300076007843018 train acc:0.0\n",
      "epoch:2 train loss:5.29829740524292 train acc:0.0\n",
      "epoch:2 train loss:5.2971649169921875 train acc:0.0\n",
      "epoch:2 train loss:5.297399520874023 train acc:0.0\n",
      "epoch:2 train loss:5.299853801727295 train acc:0.0\n",
      "epoch:2 train loss:5.301611423492432 train acc:0.0\n",
      "epoch:2 train loss:5.295914649963379 train acc:0.0\n",
      "epoch:2 train loss:5.300102233886719 train acc:0.0\n",
      "epoch:2 train loss:5.2999701499938965 train acc:0.0\n",
      "epoch:2 train loss:5.299185752868652 train acc:0.0\n",
      "epoch:2 train loss:5.2984514236450195 train acc:0.0\n",
      "epoch:2 train loss:5.297125816345215 train acc:0.0\n",
      "epoch:2 train loss:5.301522254943848 train acc:0.0\n",
      "epoch:2 train loss:5.297675132751465 train acc:0.0\n",
      "epoch:2 train loss:5.299185276031494 train acc:0.0\n",
      "epoch:2 train loss:5.299384593963623 train acc:0.0\n",
      "epoch:2 train loss:5.29897403717041 train acc:0.0\n",
      "epoch:2 train loss:5.298839092254639 train acc:0.0\n",
      "epoch:2 train loss:5.294302463531494 train acc:0.0\n",
      "epoch:2 train loss:5.295687198638916 train acc:0.0\n",
      "epoch:2 train loss:5.296507358551025 train acc:0.0\n",
      "epoch:2 train loss:5.297736644744873 train acc:0.0\n",
      "epoch:2 train loss:5.30147647857666 train acc:0.0\n",
      "epoch:2 train loss:5.299025535583496 train acc:0.0\n",
      "epoch:2 train loss:5.2993316650390625 train acc:0.0\n",
      "epoch:2 train loss:5.2969536781311035 train acc:0.0\n",
      "epoch:2 train loss:5.298330783843994 train acc:0.0\n",
      "epoch:2 train loss:5.300466060638428 train acc:0.0\n",
      "epoch:2 train loss:5.298599720001221 train acc:0.0\n",
      "epoch:2 train loss:5.299438953399658 train acc:0.0\n",
      "epoch:2 train loss:5.299300193786621 train acc:0.0\n",
      "epoch:2 train loss:5.299881935119629 train acc:0.0\n",
      "epoch:2 train loss:5.298048496246338 train acc:0.0\n",
      "epoch:2 train loss:5.297205924987793 train acc:0.0\n",
      "epoch:2 train loss:5.299312114715576 train acc:0.0\n",
      "epoch:2 train loss:5.29979133605957 train acc:0.0\n",
      "epoch:2 train loss:5.297573566436768 train acc:0.0\n",
      "epoch:2 train loss:5.29872465133667 train acc:0.0\n",
      "epoch:2 train loss:5.299277305603027 train acc:0.0\n",
      "epoch:2 train loss:5.298552989959717 train acc:0.0\n",
      "epoch:2 train loss:5.2984619140625 train acc:0.0\n",
      "epoch:2 train loss:5.299710273742676 train acc:0.0\n",
      "epoch:2 train loss:5.299402713775635 train acc:0.0\n",
      "epoch:2 train loss:5.295279502868652 train acc:0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938e16c9682e4a6e900b8d134c82457a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=363), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:3 train loss:5.2953386306762695 train acc:0.0625\n",
      "epoch:3 train loss:5.295817852020264 train acc:0.0\n",
      "epoch:3 train loss:5.2983503341674805 train acc:0.0\n",
      "epoch:3 train loss:5.301715850830078 train acc:0.0\n",
      "epoch:3 train loss:5.30076265335083 train acc:0.0\n",
      "epoch:3 train loss:5.298306465148926 train acc:0.0\n",
      "epoch:3 train loss:5.300664901733398 train acc:0.0\n",
      "epoch:3 train loss:5.300920486450195 train acc:0.0\n",
      "epoch:3 train loss:5.29803991317749 train acc:0.0\n",
      "epoch:3 train loss:5.299860000610352 train acc:0.0\n",
      "epoch:3 train loss:5.300347805023193 train acc:0.0\n",
      "epoch:3 train loss:5.2979817390441895 train acc:0.0\n",
      "epoch:3 train loss:5.300286769866943 train acc:0.0\n",
      "epoch:3 train loss:5.298632621765137 train acc:0.0\n",
      "epoch:3 train loss:5.299382209777832 train acc:0.0\n",
      "epoch:3 train loss:5.299435615539551 train acc:0.0\n",
      "epoch:3 train loss:5.295721054077148 train acc:0.0\n",
      "epoch:3 train loss:5.301209926605225 train acc:0.0\n",
      "epoch:3 train loss:5.301753520965576 train acc:0.0\n",
      "epoch:3 train loss:5.2985758781433105 train acc:0.0\n",
      "epoch:3 train loss:5.296441555023193 train acc:0.0\n",
      "epoch:3 train loss:5.299447536468506 train acc:0.0\n",
      "epoch:3 train loss:5.296718597412109 train acc:0.125\n",
      "epoch:3 train loss:5.299755573272705 train acc:0.0\n",
      "epoch:3 train loss:5.296962738037109 train acc:0.0\n",
      "epoch:3 train loss:5.299257755279541 train acc:0.0\n",
      "epoch:3 train loss:5.299569129943848 train acc:0.0\n",
      "epoch:3 train loss:5.2959699630737305 train acc:0.0\n",
      "epoch:3 train loss:5.29982328414917 train acc:0.0\n",
      "epoch:3 train loss:5.2979631423950195 train acc:0.0\n",
      "epoch:3 train loss:5.3005900382995605 train acc:0.0\n",
      "epoch:3 train loss:5.297451496124268 train acc:0.0\n",
      "epoch:3 train loss:5.297935962677002 train acc:0.0625\n",
      "epoch:3 train loss:5.2985358238220215 train acc:0.0\n",
      "epoch:3 train loss:5.2953410148620605 train acc:0.0\n",
      "epoch:3 train loss:5.296405792236328 train acc:0.0\n",
      "epoch:3 train loss:5.301656246185303 train acc:0.0\n",
      "epoch:3 train loss:5.3019890785217285 train acc:0.0\n",
      "epoch:3 train loss:5.297455310821533 train acc:0.0\n",
      "epoch:3 train loss:5.295978546142578 train acc:0.0\n",
      "epoch:3 train loss:5.297682285308838 train acc:0.0\n",
      "epoch:3 train loss:5.292616844177246 train acc:0.0625\n",
      "epoch:3 train loss:5.296117782592773 train acc:0.0\n",
      "epoch:3 train loss:5.3020148277282715 train acc:0.0\n",
      "epoch:3 train loss:5.298819065093994 train acc:0.0\n",
      "epoch:3 train loss:5.299306392669678 train acc:0.0\n",
      "epoch:3 train loss:5.297832012176514 train acc:0.0\n",
      "epoch:3 train loss:5.299593925476074 train acc:0.0\n",
      "epoch:3 train loss:5.297900676727295 train acc:0.0\n",
      "epoch:3 train loss:5.298715591430664 train acc:0.0\n",
      "epoch:3 train loss:5.298259258270264 train acc:0.0\n",
      "epoch:3 train loss:5.298450469970703 train acc:0.0\n",
      "epoch:3 train loss:5.299185276031494 train acc:0.0\n",
      "epoch:3 train loss:5.299068450927734 train acc:0.0\n",
      "epoch:3 train loss:5.300426006317139 train acc:0.0\n",
      "epoch:3 train loss:5.298703193664551 train acc:0.0\n",
      "epoch:3 train loss:5.29875373840332 train acc:0.0\n",
      "epoch:3 train loss:5.297895908355713 train acc:0.0\n",
      "epoch:3 train loss:5.294464111328125 train acc:0.0\n",
      "epoch:3 train loss:5.298413276672363 train acc:0.0\n",
      "epoch:3 train loss:5.299358367919922 train acc:0.0\n",
      "epoch:3 train loss:5.297607898712158 train acc:0.0\n",
      "epoch:3 train loss:5.2986650466918945 train acc:0.0\n",
      "epoch:3 train loss:5.301443576812744 train acc:0.0\n",
      "epoch:3 train loss:5.298271656036377 train acc:0.0\n",
      "epoch:3 train loss:5.295836925506592 train acc:0.0\n",
      "epoch:3 train loss:5.297443866729736 train acc:0.0\n",
      "epoch:3 train loss:5.296605110168457 train acc:0.0\n",
      "epoch:3 train loss:5.299427032470703 train acc:0.0\n",
      "epoch:3 train loss:5.298750400543213 train acc:0.0\n",
      "epoch:3 train loss:5.301097393035889 train acc:0.0\n",
      "epoch:3 train loss:5.298429012298584 train acc:0.0\n",
      "epoch:3 train loss:5.2974348068237305 train acc:0.0\n",
      "epoch:3 train loss:5.299497127532959 train acc:0.0\n",
      "epoch:3 train loss:5.299271583557129 train acc:0.0\n",
      "epoch:3 train loss:5.296145439147949 train acc:0.0\n",
      "epoch:3 train loss:5.299096584320068 train acc:0.0\n",
      "epoch:3 train loss:5.294999122619629 train acc:0.0\n",
      "epoch:3 train loss:5.295912742614746 train acc:0.0\n",
      "epoch:3 train loss:5.301214694976807 train acc:0.0\n",
      "epoch:3 train loss:5.299744129180908 train acc:0.0\n",
      "epoch:3 train loss:5.298490524291992 train acc:0.0\n",
      "epoch:3 train loss:5.300398349761963 train acc:0.0\n",
      "epoch:3 train loss:5.299134731292725 train acc:0.0\n",
      "epoch:3 train loss:5.300715446472168 train acc:0.0\n",
      "epoch:3 train loss:5.299829006195068 train acc:0.0\n",
      "epoch:3 train loss:5.297956466674805 train acc:0.0\n",
      "epoch:3 train loss:5.301243782043457 train acc:0.0\n",
      "epoch:3 train loss:5.299858093261719 train acc:0.0\n",
      "epoch:3 train loss:5.300583362579346 train acc:0.0\n",
      "epoch:3 train loss:5.301052570343018 train acc:0.0\n",
      "epoch:3 train loss:5.299074649810791 train acc:0.0\n",
      "epoch:3 train loss:5.296352863311768 train acc:0.0\n",
      "epoch:3 train loss:5.301409721374512 train acc:0.0\n",
      "epoch:3 train loss:5.299241065979004 train acc:0.0\n",
      "epoch:3 train loss:5.299053192138672 train acc:0.0\n",
      "epoch:3 train loss:5.300004482269287 train acc:0.0\n",
      "epoch:3 train loss:5.300750255584717 train acc:0.0\n",
      "epoch:3 train loss:5.2968244552612305 train acc:0.0\n",
      "epoch:3 train loss:5.295271873474121 train acc:0.0\n",
      "epoch:3 train loss:5.29661750793457 train acc:0.0\n",
      "epoch:3 train loss:5.296346664428711 train acc:0.0\n",
      "epoch:3 train loss:5.299452304840088 train acc:0.0\n",
      "epoch:3 train loss:5.300209045410156 train acc:0.0\n",
      "epoch:3 train loss:5.301855087280273 train acc:0.0\n",
      "epoch:3 train loss:5.299651622772217 train acc:0.0\n",
      "epoch:3 train loss:5.297553062438965 train acc:0.0\n",
      "epoch:3 train loss:5.296855926513672 train acc:0.0\n",
      "epoch:3 train loss:5.2969865798950195 train acc:0.0\n",
      "epoch:3 train loss:5.29964542388916 train acc:0.0\n",
      "epoch:3 train loss:5.297396659851074 train acc:0.0\n",
      "epoch:3 train loss:5.296416759490967 train acc:0.0\n",
      "epoch:3 train loss:5.295942306518555 train acc:0.0\n",
      "epoch:3 train loss:5.2986884117126465 train acc:0.0\n",
      "epoch:3 train loss:5.300190448760986 train acc:0.0\n",
      "epoch:3 train loss:5.30147123336792 train acc:0.0\n",
      "epoch:3 train loss:5.298513412475586 train acc:0.0\n",
      "epoch:3 train loss:5.295571804046631 train acc:0.0\n",
      "epoch:3 train loss:5.298986434936523 train acc:0.0\n",
      "epoch:3 train loss:5.299520969390869 train acc:0.0\n",
      "epoch:3 train loss:5.3010573387146 train acc:0.0\n",
      "epoch:3 train loss:5.296760559082031 train acc:0.0\n",
      "epoch:3 train loss:5.299307823181152 train acc:0.0\n",
      "epoch:3 train loss:5.296988010406494 train acc:0.0\n",
      "epoch:3 train loss:5.300321102142334 train acc:0.0\n",
      "epoch:3 train loss:5.299649715423584 train acc:0.0\n",
      "epoch:3 train loss:5.2986674308776855 train acc:0.0\n",
      "epoch:3 train loss:5.29688024520874 train acc:0.0\n",
      "epoch:3 train loss:5.297476768493652 train acc:0.0\n",
      "epoch:3 train loss:5.298092365264893 train acc:0.0\n",
      "epoch:3 train loss:5.295409679412842 train acc:0.0\n",
      "epoch:3 train loss:5.29575252532959 train acc:0.0\n",
      "epoch:3 train loss:5.299642086029053 train acc:0.0\n",
      "epoch:3 train loss:5.298381805419922 train acc:0.0\n",
      "epoch:3 train loss:5.296236038208008 train acc:0.0\n",
      "epoch:3 train loss:5.301295757293701 train acc:0.0\n",
      "epoch:3 train loss:5.2959723472595215 train acc:0.0\n",
      "epoch:3 train loss:5.3002448081970215 train acc:0.0\n",
      "epoch:3 train loss:5.296740531921387 train acc:0.0\n",
      "epoch:3 train loss:5.2983174324035645 train acc:0.0\n",
      "epoch:3 train loss:5.298412322998047 train acc:0.0\n",
      "epoch:3 train loss:5.294498443603516 train acc:0.0\n",
      "epoch:3 train loss:5.299458026885986 train acc:0.0\n",
      "epoch:3 train loss:5.294392108917236 train acc:0.0\n",
      "epoch:3 train loss:5.297825336456299 train acc:0.0\n",
      "epoch:3 train loss:5.298011302947998 train acc:0.0625\n",
      "epoch:3 train loss:5.298648834228516 train acc:0.0\n",
      "epoch:3 train loss:5.30023193359375 train acc:0.0\n",
      "epoch:3 train loss:5.298673629760742 train acc:0.0\n",
      "epoch:3 train loss:5.298058032989502 train acc:0.0\n",
      "epoch:3 train loss:5.298947811126709 train acc:0.0\n",
      "epoch:3 train loss:5.296751022338867 train acc:0.0\n",
      "epoch:3 train loss:5.296989440917969 train acc:0.0\n",
      "epoch:3 train loss:5.2964091300964355 train acc:0.0625\n",
      "epoch:3 train loss:5.294753074645996 train acc:0.0\n",
      "epoch:3 train loss:5.297633171081543 train acc:0.0\n",
      "epoch:3 train loss:5.301079273223877 train acc:0.0\n",
      "epoch:3 train loss:5.297651290893555 train acc:0.0\n",
      "epoch:3 train loss:5.301334381103516 train acc:0.0\n",
      "epoch:3 train loss:5.295338153839111 train acc:0.0\n",
      "epoch:3 train loss:5.296061992645264 train acc:0.0\n",
      "epoch:3 train loss:5.298148155212402 train acc:0.0\n",
      "epoch:3 train loss:5.297384738922119 train acc:0.0\n",
      "epoch:3 train loss:5.29769229888916 train acc:0.0\n",
      "epoch:3 train loss:5.300564289093018 train acc:0.0\n",
      "epoch:3 train loss:5.296300888061523 train acc:0.0\n",
      "epoch:3 train loss:5.299544334411621 train acc:0.0\n",
      "epoch:3 train loss:5.300210475921631 train acc:0.0\n",
      "epoch:3 train loss:5.298041343688965 train acc:0.0\n",
      "epoch:3 train loss:5.2987446784973145 train acc:0.0\n",
      "epoch:3 train loss:5.301834583282471 train acc:0.0\n",
      "epoch:3 train loss:5.302137851715088 train acc:0.0\n",
      "epoch:3 train loss:5.2979841232299805 train acc:0.0\n",
      "epoch:3 train loss:5.299355506896973 train acc:0.0\n",
      "epoch:3 train loss:5.29782772064209 train acc:0.0\n",
      "epoch:3 train loss:5.2969560623168945 train acc:0.0\n",
      "epoch:3 train loss:5.298987865447998 train acc:0.0\n",
      "epoch:3 train loss:5.298444747924805 train acc:0.0\n",
      "epoch:3 train loss:5.298063278198242 train acc:0.0\n",
      "epoch:3 train loss:5.301229476928711 train acc:0.0\n",
      "epoch:3 train loss:5.297183990478516 train acc:0.0\n",
      "epoch:3 train loss:5.298003673553467 train acc:0.0\n",
      "epoch:3 train loss:5.299326419830322 train acc:0.0625\n",
      "epoch:3 train loss:5.29715633392334 train acc:0.0\n",
      "epoch:3 train loss:5.29776668548584 train acc:0.0\n",
      "epoch:3 train loss:5.296551704406738 train acc:0.0\n",
      "epoch:3 train loss:5.3008646965026855 train acc:0.0\n",
      "epoch:3 train loss:5.300293445587158 train acc:0.0\n",
      "epoch:3 train loss:5.299437999725342 train acc:0.0\n",
      "epoch:3 train loss:5.296446323394775 train acc:0.0\n",
      "epoch:3 train loss:5.298304557800293 train acc:0.0\n",
      "epoch:3 train loss:5.296652793884277 train acc:0.0\n",
      "epoch:3 train loss:5.298947811126709 train acc:0.0\n",
      "epoch:3 train loss:5.299454689025879 train acc:0.0\n",
      "epoch:3 train loss:5.298974514007568 train acc:0.0\n",
      "epoch:3 train loss:5.2988600730896 train acc:0.0\n",
      "epoch:3 train loss:5.2952728271484375 train acc:0.0\n",
      "epoch:3 train loss:5.298678398132324 train acc:0.0\n",
      "epoch:3 train loss:5.299849510192871 train acc:0.0\n",
      "epoch:3 train loss:5.2942728996276855 train acc:0.0\n",
      "epoch:3 train loss:5.29763650894165 train acc:0.0\n",
      "epoch:3 train loss:5.29658317565918 train acc:0.0\n",
      "epoch:3 train loss:5.3004302978515625 train acc:0.0\n",
      "epoch:3 train loss:5.298597812652588 train acc:0.0\n",
      "epoch:3 train loss:5.296051025390625 train acc:0.0\n",
      "epoch:3 train loss:5.296533107757568 train acc:0.0\n",
      "epoch:3 train loss:5.297496795654297 train acc:0.0\n",
      "epoch:3 train loss:5.298653602600098 train acc:0.0\n",
      "epoch:3 train loss:5.296902179718018 train acc:0.0\n",
      "epoch:3 train loss:5.299984931945801 train acc:0.0\n",
      "epoch:3 train loss:5.2989044189453125 train acc:0.0\n",
      "epoch:3 train loss:5.296909332275391 train acc:0.0625\n",
      "epoch:3 train loss:5.298511028289795 train acc:0.0\n",
      "epoch:3 train loss:5.2982892990112305 train acc:0.0\n",
      "epoch:3 train loss:5.297472953796387 train acc:0.0\n",
      "epoch:3 train loss:5.297140121459961 train acc:0.0\n",
      "epoch:3 train loss:5.2973952293396 train acc:0.0625\n",
      "epoch:3 train loss:5.297367572784424 train acc:0.0\n",
      "epoch:3 train loss:5.293796539306641 train acc:0.0625\n",
      "epoch:3 train loss:5.298299312591553 train acc:0.0625\n",
      "epoch:3 train loss:5.300483703613281 train acc:0.0\n",
      "epoch:3 train loss:5.2941508293151855 train acc:0.0\n",
      "epoch:3 train loss:5.295629978179932 train acc:0.0\n",
      "epoch:3 train loss:5.297325611114502 train acc:0.0625\n",
      "epoch:3 train loss:5.296781539916992 train acc:0.0\n",
      "epoch:3 train loss:5.296561241149902 train acc:0.0625\n",
      "epoch:3 train loss:5.297178268432617 train acc:0.0625\n",
      "epoch:3 train loss:5.297919273376465 train acc:0.0\n",
      "epoch:3 train loss:5.301909923553467 train acc:0.0\n",
      "epoch:3 train loss:5.29850435256958 train acc:0.0\n",
      "epoch:3 train loss:5.2999348640441895 train acc:0.0\n",
      "epoch:3 train loss:5.297448635101318 train acc:0.0\n",
      "epoch:3 train loss:5.3001790046691895 train acc:0.0\n",
      "epoch:3 train loss:5.298772811889648 train acc:0.0\n",
      "epoch:3 train loss:5.299950122833252 train acc:0.0\n",
      "epoch:3 train loss:5.296291828155518 train acc:0.0\n",
      "epoch:3 train loss:5.298598289489746 train acc:0.0\n",
      "epoch:3 train loss:5.296233654022217 train acc:0.0\n",
      "epoch:3 train loss:5.297248840332031 train acc:0.0\n",
      "epoch:3 train loss:5.298338890075684 train acc:0.0\n",
      "epoch:3 train loss:5.298091888427734 train acc:0.0\n",
      "epoch:3 train loss:5.300544261932373 train acc:0.0\n",
      "epoch:3 train loss:5.298830032348633 train acc:0.0\n",
      "epoch:3 train loss:5.297306060791016 train acc:0.0\n",
      "epoch:3 train loss:5.299688339233398 train acc:0.0\n",
      "epoch:3 train loss:5.2992658615112305 train acc:0.0\n",
      "epoch:3 train loss:5.297480583190918 train acc:0.0\n",
      "epoch:3 train loss:5.295474529266357 train acc:0.0\n",
      "epoch:3 train loss:5.302009582519531 train acc:0.0\n",
      "epoch:3 train loss:5.3009796142578125 train acc:0.0\n",
      "epoch:3 train loss:5.297761917114258 train acc:0.0\n",
      "epoch:3 train loss:5.300762176513672 train acc:0.0\n",
      "epoch:3 train loss:5.300859451293945 train acc:0.0\n",
      "epoch:3 train loss:5.298519611358643 train acc:0.0\n",
      "epoch:3 train loss:5.300180435180664 train acc:0.0\n",
      "epoch:3 train loss:5.2983269691467285 train acc:0.0\n",
      "epoch:3 train loss:5.296153545379639 train acc:0.0\n",
      "epoch:3 train loss:5.294693946838379 train acc:0.0\n",
      "epoch:3 train loss:5.300336837768555 train acc:0.0\n",
      "epoch:3 train loss:5.298092365264893 train acc:0.0\n",
      "epoch:3 train loss:5.2987589836120605 train acc:0.0\n",
      "epoch:3 train loss:5.297882556915283 train acc:0.0\n",
      "epoch:3 train loss:5.294547080993652 train acc:0.0625\n",
      "epoch:3 train loss:5.299896717071533 train acc:0.0\n",
      "epoch:3 train loss:5.296276092529297 train acc:0.0\n",
      "epoch:3 train loss:5.302178859710693 train acc:0.0\n",
      "epoch:3 train loss:5.299158096313477 train acc:0.0\n",
      "epoch:3 train loss:5.298269748687744 train acc:0.0\n",
      "epoch:3 train loss:5.295851707458496 train acc:0.0\n",
      "epoch:3 train loss:5.298288822174072 train acc:0.0\n",
      "epoch:3 train loss:5.297292232513428 train acc:0.0\n",
      "epoch:3 train loss:5.299461364746094 train acc:0.0\n",
      "epoch:3 train loss:5.30064582824707 train acc:0.0\n",
      "epoch:3 train loss:5.29925537109375 train acc:0.0\n",
      "epoch:3 train loss:5.29978084564209 train acc:0.0\n",
      "epoch:3 train loss:5.296255588531494 train acc:0.0\n",
      "epoch:3 train loss:5.299002170562744 train acc:0.0\n",
      "epoch:3 train loss:5.300162315368652 train acc:0.0\n",
      "epoch:3 train loss:5.299338340759277 train acc:0.0\n",
      "epoch:3 train loss:5.296682834625244 train acc:0.0\n",
      "epoch:3 train loss:5.298306465148926 train acc:0.0\n",
      "epoch:3 train loss:5.298303604125977 train acc:0.0\n",
      "epoch:3 train loss:5.297177791595459 train acc:0.0\n",
      "epoch:3 train loss:5.298917770385742 train acc:0.0\n",
      "epoch:3 train loss:5.298473834991455 train acc:0.0\n",
      "epoch:3 train loss:5.297224998474121 train acc:0.0\n",
      "epoch:3 train loss:5.297844886779785 train acc:0.0\n",
      "epoch:3 train loss:5.300864219665527 train acc:0.0\n",
      "epoch:3 train loss:5.298105716705322 train acc:0.0\n",
      "epoch:3 train loss:5.297990798950195 train acc:0.0625\n",
      "epoch:3 train loss:5.2998199462890625 train acc:0.0\n",
      "epoch:3 train loss:5.300478458404541 train acc:0.0\n",
      "epoch:3 train loss:5.296800136566162 train acc:0.0\n",
      "epoch:3 train loss:5.30061149597168 train acc:0.0\n",
      "epoch:3 train loss:5.3003644943237305 train acc:0.0\n",
      "epoch:3 train loss:5.298234462738037 train acc:0.0\n",
      "epoch:3 train loss:5.2982048988342285 train acc:0.0\n",
      "epoch:3 train loss:5.298952102661133 train acc:0.0\n",
      "epoch:3 train loss:5.299973964691162 train acc:0.0\n",
      "epoch:3 train loss:5.297107219696045 train acc:0.0\n",
      "epoch:3 train loss:5.2975358963012695 train acc:0.0\n",
      "epoch:3 train loss:5.29856014251709 train acc:0.0\n",
      "epoch:3 train loss:5.302273273468018 train acc:0.0\n",
      "epoch:3 train loss:5.298793315887451 train acc:0.0\n",
      "epoch:3 train loss:5.295541763305664 train acc:0.0\n",
      "epoch:3 train loss:5.294748306274414 train acc:0.0\n",
      "epoch:3 train loss:5.295447826385498 train acc:0.0\n",
      "epoch:3 train loss:5.299550533294678 train acc:0.0\n",
      "epoch:3 train loss:5.299558639526367 train acc:0.0\n",
      "epoch:3 train loss:5.297386169433594 train acc:0.0\n",
      "epoch:3 train loss:5.297285079956055 train acc:0.0\n",
      "epoch:3 train loss:5.297064781188965 train acc:0.0\n",
      "epoch:3 train loss:5.299843788146973 train acc:0.0\n",
      "epoch:3 train loss:5.296872138977051 train acc:0.0\n",
      "epoch:3 train loss:5.295862674713135 train acc:0.0\n",
      "epoch:3 train loss:5.3014044761657715 train acc:0.0\n",
      "epoch:3 train loss:5.299442768096924 train acc:0.0\n",
      "epoch:3 train loss:5.2968244552612305 train acc:0.0\n",
      "epoch:3 train loss:5.300759792327881 train acc:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 train loss:5.298903465270996 train acc:0.0\n",
      "epoch:3 train loss:5.2996039390563965 train acc:0.0\n",
      "epoch:3 train loss:5.295580863952637 train acc:0.0\n",
      "epoch:3 train loss:5.295234203338623 train acc:0.0\n",
      "epoch:3 train loss:5.298060894012451 train acc:0.0\n",
      "epoch:3 train loss:5.299359321594238 train acc:0.0\n",
      "epoch:3 train loss:5.298300266265869 train acc:0.0\n",
      "epoch:3 train loss:5.2968573570251465 train acc:0.0\n",
      "epoch:3 train loss:5.296949863433838 train acc:0.0625\n",
      "epoch:3 train loss:5.295220375061035 train acc:0.0625\n",
      "epoch:3 train loss:5.297420978546143 train acc:0.0\n",
      "epoch:3 train loss:5.299088001251221 train acc:0.0\n",
      "epoch:3 train loss:5.298323154449463 train acc:0.0\n",
      "epoch:3 train loss:5.297237873077393 train acc:0.0\n",
      "epoch:3 train loss:5.298551559448242 train acc:0.0\n",
      "epoch:3 train loss:5.295932769775391 train acc:0.0\n",
      "epoch:3 train loss:5.296392440795898 train acc:0.0\n",
      "epoch:3 train loss:5.300436973571777 train acc:0.0\n",
      "epoch:3 train loss:5.299074649810791 train acc:0.0\n",
      "epoch:3 train loss:5.297601699829102 train acc:0.0\n",
      "epoch:3 train loss:5.2983503341674805 train acc:0.0\n",
      "epoch:3 train loss:5.299906253814697 train acc:0.0\n",
      "epoch:3 train loss:5.297232151031494 train acc:0.0625\n",
      "epoch:3 train loss:5.2998504638671875 train acc:0.0\n",
      "epoch:3 train loss:5.3014726638793945 train acc:0.0\n",
      "epoch:3 train loss:5.296713352203369 train acc:0.0\n",
      "epoch:3 train loss:5.300839900970459 train acc:0.0\n",
      "epoch:3 train loss:5.299400329589844 train acc:0.0\n",
      "epoch:3 train loss:5.296382427215576 train acc:0.0\n",
      "epoch:3 train loss:5.301455020904541 train acc:0.0\n",
      "epoch:3 train loss:5.29806661605835 train acc:0.0\n",
      "epoch:3 train loss:5.296970844268799 train acc:0.0\n",
      "epoch:3 train loss:5.2990851402282715 train acc:0.0\n",
      "epoch:3 train loss:5.299264907836914 train acc:0.0\n",
      "epoch:3 train loss:5.2970075607299805 train acc:0.0\n",
      "epoch:3 train loss:5.297794818878174 train acc:0.0\n",
      "epoch:3 train loss:5.297050952911377 train acc:0.0\n",
      "epoch:3 train loss:5.299424648284912 train acc:0.0\n",
      "epoch:3 train loss:5.296292781829834 train acc:0.0\n",
      "epoch:3 train loss:5.297403335571289 train acc:0.0\n",
      "epoch:3 train loss:5.299187183380127 train acc:0.0\n",
      "epoch:3 train loss:5.2982354164123535 train acc:0.0\n",
      "epoch:3 train loss:5.295252323150635 train acc:0.0\n",
      "epoch:3 train loss:5.299776554107666 train acc:0.0\n",
      "epoch:3 train loss:5.299282550811768 train acc:0.0\n",
      "epoch:3 train loss:5.299250602722168 train acc:0.0\n",
      "epoch:3 train loss:5.299878120422363 train acc:0.0\n",
      "epoch:3 train loss:5.301684856414795 train acc:0.0\n",
      "epoch:3 train loss:5.29946231842041 train acc:0.0\n",
      "epoch:3 train loss:5.302698135375977 train acc:0.0\n",
      "epoch:3 train loss:5.2971625328063965 train acc:0.0625\n",
      "epoch:3 train loss:5.301445007324219 train acc:0.0\n",
      "epoch:3 train loss:5.294246673583984 train acc:0.0\n",
      "epoch:3 train loss:5.2991437911987305 train acc:0.0\n",
      "epoch:3 train loss:5.29802131652832 train acc:0.0\n",
      "epoch:3 train loss:5.295937538146973 train acc:0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a612423c46f44f2914ac4bc7ae088fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=363), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:4 train loss:5.301733016967773 train acc:0.0\n",
      "epoch:4 train loss:5.296097755432129 train acc:0.0\n",
      "epoch:4 train loss:5.297606468200684 train acc:0.0625\n",
      "epoch:4 train loss:5.300663948059082 train acc:0.0\n",
      "epoch:4 train loss:5.298459529876709 train acc:0.0\n",
      "epoch:4 train loss:5.298678398132324 train acc:0.0\n",
      "epoch:4 train loss:5.300393581390381 train acc:0.0\n",
      "epoch:4 train loss:5.301620006561279 train acc:0.0\n",
      "epoch:4 train loss:5.298038959503174 train acc:0.0\n",
      "epoch:4 train loss:5.295742511749268 train acc:0.0\n",
      "epoch:4 train loss:5.298316478729248 train acc:0.0\n",
      "epoch:4 train loss:5.299464225769043 train acc:0.0\n",
      "epoch:4 train loss:5.2978434562683105 train acc:0.0\n",
      "epoch:4 train loss:5.296684265136719 train acc:0.0\n",
      "epoch:4 train loss:5.300664901733398 train acc:0.0\n",
      "epoch:4 train loss:5.300563812255859 train acc:0.0\n",
      "epoch:4 train loss:5.296679496765137 train acc:0.0\n",
      "epoch:4 train loss:5.294901371002197 train acc:0.0\n",
      "epoch:4 train loss:5.299020290374756 train acc:0.0\n",
      "epoch:4 train loss:5.299187660217285 train acc:0.0\n",
      "epoch:4 train loss:5.299745559692383 train acc:0.0\n",
      "epoch:4 train loss:5.298823833465576 train acc:0.0\n",
      "epoch:4 train loss:5.296455383300781 train acc:0.0\n",
      "epoch:4 train loss:5.297051429748535 train acc:0.0\n",
      "epoch:4 train loss:5.297573089599609 train acc:0.0\n",
      "epoch:4 train loss:5.300963878631592 train acc:0.0\n",
      "epoch:4 train loss:5.297107696533203 train acc:0.0\n",
      "epoch:4 train loss:5.2956976890563965 train acc:0.0\n",
      "epoch:4 train loss:5.296917915344238 train acc:0.0\n",
      "epoch:4 train loss:5.299041748046875 train acc:0.0\n",
      "epoch:4 train loss:5.2985429763793945 train acc:0.0\n",
      "epoch:4 train loss:5.3000054359436035 train acc:0.0\n",
      "epoch:4 train loss:5.295963287353516 train acc:0.0\n",
      "epoch:4 train loss:5.295330047607422 train acc:0.0\n",
      "epoch:4 train loss:5.30000114440918 train acc:0.0\n",
      "epoch:4 train loss:5.3009161949157715 train acc:0.0\n",
      "epoch:4 train loss:5.301774978637695 train acc:0.0\n",
      "epoch:4 train loss:5.29633903503418 train acc:0.0\n",
      "epoch:4 train loss:5.300487995147705 train acc:0.0\n",
      "epoch:4 train loss:5.297500133514404 train acc:0.0\n",
      "epoch:4 train loss:5.300816535949707 train acc:0.0\n",
      "epoch:4 train loss:5.299922943115234 train acc:0.0\n",
      "epoch:4 train loss:5.3000030517578125 train acc:0.0\n",
      "epoch:4 train loss:5.300002098083496 train acc:0.0\n",
      "epoch:4 train loss:5.299777507781982 train acc:0.0\n",
      "epoch:4 train loss:5.299665451049805 train acc:0.0\n",
      "epoch:4 train loss:5.299872875213623 train acc:0.0\n",
      "epoch:4 train loss:5.296209812164307 train acc:0.0\n",
      "epoch:4 train loss:5.298455715179443 train acc:0.0\n",
      "epoch:4 train loss:5.299046993255615 train acc:0.0\n",
      "epoch:4 train loss:5.302149772644043 train acc:0.0\n",
      "epoch:4 train loss:5.30034875869751 train acc:0.0\n",
      "epoch:4 train loss:5.29718017578125 train acc:0.0\n",
      "epoch:4 train loss:5.297297477722168 train acc:0.0\n",
      "epoch:4 train loss:5.298630714416504 train acc:0.0\n",
      "epoch:4 train loss:5.299286842346191 train acc:0.0\n",
      "epoch:4 train loss:5.299643039703369 train acc:0.0\n",
      "epoch:4 train loss:5.300362586975098 train acc:0.0\n",
      "epoch:4 train loss:5.301370620727539 train acc:0.0\n",
      "epoch:4 train loss:5.2962117195129395 train acc:0.0625\n",
      "epoch:4 train loss:5.3004150390625 train acc:0.0\n",
      "epoch:4 train loss:5.29633092880249 train acc:0.0\n",
      "epoch:4 train loss:5.296632289886475 train acc:0.0\n",
      "epoch:4 train loss:5.299289226531982 train acc:0.0\n",
      "epoch:4 train loss:5.2969255447387695 train acc:0.0\n",
      "epoch:4 train loss:5.300127983093262 train acc:0.0\n",
      "epoch:4 train loss:5.2999982833862305 train acc:0.0\n",
      "epoch:4 train loss:5.298896789550781 train acc:0.0\n",
      "epoch:4 train loss:5.296980381011963 train acc:0.0\n",
      "epoch:4 train loss:5.2947001457214355 train acc:0.0\n",
      "epoch:4 train loss:5.298517227172852 train acc:0.0\n",
      "epoch:4 train loss:5.300932884216309 train acc:0.0\n",
      "epoch:4 train loss:5.296230316162109 train acc:0.0\n",
      "epoch:4 train loss:5.298910140991211 train acc:0.0\n",
      "epoch:4 train loss:5.298705577850342 train acc:0.0\n",
      "epoch:4 train loss:5.297802925109863 train acc:0.0\n",
      "epoch:4 train loss:5.3004841804504395 train acc:0.0\n",
      "epoch:4 train loss:5.29940938949585 train acc:0.0\n",
      "epoch:4 train loss:5.300034523010254 train acc:0.0\n",
      "epoch:4 train loss:5.297420978546143 train acc:0.0\n",
      "epoch:4 train loss:5.294484615325928 train acc:0.0\n",
      "epoch:4 train loss:5.297407150268555 train acc:0.0\n",
      "epoch:4 train loss:5.298805236816406 train acc:0.0\n",
      "epoch:4 train loss:5.297296524047852 train acc:0.0\n",
      "epoch:4 train loss:5.299292087554932 train acc:0.0\n",
      "epoch:4 train loss:5.297073841094971 train acc:0.0\n",
      "epoch:4 train loss:5.300945281982422 train acc:0.0\n",
      "epoch:4 train loss:5.299500465393066 train acc:0.0\n",
      "epoch:4 train loss:5.2952399253845215 train acc:0.0\n",
      "epoch:4 train loss:5.298713207244873 train acc:0.0\n",
      "epoch:4 train loss:5.296464920043945 train acc:0.0\n",
      "epoch:4 train loss:5.299923896789551 train acc:0.0\n",
      "epoch:4 train loss:5.297159671783447 train acc:0.0\n",
      "epoch:4 train loss:5.298928737640381 train acc:0.0625\n",
      "epoch:4 train loss:5.298893928527832 train acc:0.0\n",
      "epoch:4 train loss:5.298360824584961 train acc:0.0\n",
      "epoch:4 train loss:5.2985005378723145 train acc:0.0\n",
      "epoch:4 train loss:5.297428607940674 train acc:0.0\n",
      "epoch:4 train loss:5.297195911407471 train acc:0.0\n",
      "epoch:4 train loss:5.30086088180542 train acc:0.0\n",
      "epoch:4 train loss:5.298673629760742 train acc:0.0\n",
      "epoch:4 train loss:5.298115253448486 train acc:0.0\n",
      "epoch:4 train loss:5.298236846923828 train acc:0.0\n",
      "epoch:4 train loss:5.2960052490234375 train acc:0.0\n",
      "epoch:4 train loss:5.299356937408447 train acc:0.0\n",
      "epoch:4 train loss:5.2999043464660645 train acc:0.0\n",
      "epoch:4 train loss:5.300739765167236 train acc:0.0\n",
      "epoch:4 train loss:5.298585891723633 train acc:0.0\n",
      "epoch:4 train loss:5.297534465789795 train acc:0.0\n",
      "epoch:4 train loss:5.300329685211182 train acc:0.0\n",
      "epoch:4 train loss:5.294615268707275 train acc:0.0\n",
      "epoch:4 train loss:5.295926094055176 train acc:0.0\n",
      "epoch:4 train loss:5.297412395477295 train acc:0.0\n",
      "epoch:4 train loss:5.299374103546143 train acc:0.0625\n",
      "epoch:4 train loss:5.296535968780518 train acc:0.0\n",
      "epoch:4 train loss:5.297826290130615 train acc:0.0\n",
      "epoch:4 train loss:5.300765514373779 train acc:0.0\n",
      "epoch:4 train loss:5.295160293579102 train acc:0.0\n",
      "epoch:4 train loss:5.300461769104004 train acc:0.0\n",
      "epoch:4 train loss:5.300967216491699 train acc:0.0\n",
      "epoch:4 train loss:5.297820568084717 train acc:0.0\n",
      "epoch:4 train loss:5.299428462982178 train acc:0.0625\n",
      "epoch:4 train loss:5.299367904663086 train acc:0.0\n",
      "epoch:4 train loss:5.296710968017578 train acc:0.0\n",
      "epoch:4 train loss:5.299363136291504 train acc:0.0\n",
      "epoch:4 train loss:5.298425197601318 train acc:0.0\n",
      "epoch:4 train loss:5.300276756286621 train acc:0.0\n",
      "epoch:4 train loss:5.301066875457764 train acc:0.0\n",
      "epoch:4 train loss:5.2986159324646 train acc:0.0\n",
      "epoch:4 train loss:5.297377586364746 train acc:0.0\n",
      "epoch:4 train loss:5.293529033660889 train acc:0.0\n",
      "epoch:4 train loss:5.297359466552734 train acc:0.0\n",
      "epoch:4 train loss:5.2998366355896 train acc:0.0\n",
      "epoch:4 train loss:5.299906253814697 train acc:0.0\n",
      "epoch:4 train loss:5.296790599822998 train acc:0.0625\n",
      "epoch:4 train loss:5.297724723815918 train acc:0.0\n",
      "epoch:4 train loss:5.2990899085998535 train acc:0.0\n",
      "epoch:4 train loss:5.297950267791748 train acc:0.0\n",
      "epoch:4 train loss:5.296875953674316 train acc:0.0\n",
      "epoch:4 train loss:5.299700736999512 train acc:0.0\n",
      "epoch:4 train loss:5.299552917480469 train acc:0.0\n",
      "epoch:4 train loss:5.296464920043945 train acc:0.0\n",
      "epoch:4 train loss:5.2959303855896 train acc:0.0\n",
      "epoch:4 train loss:5.2970099449157715 train acc:0.0\n",
      "epoch:4 train loss:5.299088478088379 train acc:0.0\n",
      "epoch:4 train loss:5.298898696899414 train acc:0.0\n",
      "epoch:4 train loss:5.298995494842529 train acc:0.0\n",
      "epoch:4 train loss:5.300483703613281 train acc:0.0\n",
      "epoch:4 train loss:5.301110744476318 train acc:0.0\n",
      "epoch:4 train loss:5.296336650848389 train acc:0.0625\n",
      "epoch:4 train loss:5.29771614074707 train acc:0.0\n",
      "epoch:4 train loss:5.299545764923096 train acc:0.0\n",
      "epoch:4 train loss:5.29908561706543 train acc:0.0\n",
      "epoch:4 train loss:5.299874782562256 train acc:0.0\n",
      "epoch:4 train loss:5.2969794273376465 train acc:0.0\n",
      "epoch:4 train loss:5.298037052154541 train acc:0.0\n",
      "epoch:4 train loss:5.297252655029297 train acc:0.0625\n",
      "epoch:4 train loss:5.29920768737793 train acc:0.0\n",
      "epoch:4 train loss:5.299926280975342 train acc:0.0\n",
      "epoch:4 train loss:5.297068119049072 train acc:0.0\n",
      "epoch:4 train loss:5.29947566986084 train acc:0.0\n",
      "epoch:4 train loss:5.300596714019775 train acc:0.0\n",
      "epoch:4 train loss:5.298360824584961 train acc:0.0625\n",
      "epoch:4 train loss:5.30194091796875 train acc:0.0\n",
      "epoch:4 train loss:5.300569534301758 train acc:0.0\n",
      "epoch:4 train loss:5.30013370513916 train acc:0.0625\n",
      "epoch:4 train loss:5.29890775680542 train acc:0.0\n",
      "epoch:4 train loss:5.298778057098389 train acc:0.0\n",
      "epoch:4 train loss:5.29680871963501 train acc:0.0\n",
      "epoch:4 train loss:5.2994232177734375 train acc:0.0\n",
      "epoch:4 train loss:5.297297954559326 train acc:0.0\n",
      "epoch:4 train loss:5.2985968589782715 train acc:0.0\n",
      "epoch:4 train loss:5.298045635223389 train acc:0.0\n",
      "epoch:4 train loss:5.299360752105713 train acc:0.0625\n",
      "epoch:4 train loss:5.29473352432251 train acc:0.0\n",
      "epoch:4 train loss:5.29606819152832 train acc:0.0\n",
      "epoch:4 train loss:5.299022674560547 train acc:0.0\n",
      "epoch:4 train loss:5.303218841552734 train acc:0.0\n",
      "epoch:4 train loss:5.29611873626709 train acc:0.0\n",
      "epoch:4 train loss:5.297994613647461 train acc:0.0\n",
      "epoch:4 train loss:5.299404621124268 train acc:0.0\n",
      "epoch:4 train loss:5.295298099517822 train acc:0.0\n",
      "epoch:4 train loss:5.298170566558838 train acc:0.0\n",
      "epoch:4 train loss:5.3017778396606445 train acc:0.0\n",
      "epoch:4 train loss:5.296596050262451 train acc:0.0\n",
      "epoch:4 train loss:5.298765659332275 train acc:0.0\n",
      "epoch:4 train loss:5.301831245422363 train acc:0.0\n",
      "epoch:4 train loss:5.300515651702881 train acc:0.0\n",
      "epoch:4 train loss:5.3000593185424805 train acc:0.0\n",
      "epoch:4 train loss:5.298613548278809 train acc:0.0\n",
      "epoch:4 train loss:5.296327590942383 train acc:0.0\n",
      "epoch:4 train loss:5.302181720733643 train acc:0.0\n",
      "epoch:4 train loss:5.296611309051514 train acc:0.0\n",
      "epoch:4 train loss:5.301620006561279 train acc:0.0\n",
      "epoch:4 train loss:5.299717426300049 train acc:0.0\n",
      "epoch:4 train loss:5.301484107971191 train acc:0.0\n",
      "epoch:4 train loss:5.29871129989624 train acc:0.0\n",
      "epoch:4 train loss:5.2985053062438965 train acc:0.0\n",
      "epoch:4 train loss:5.296871662139893 train acc:0.0\n",
      "epoch:4 train loss:5.299373626708984 train acc:0.0\n",
      "epoch:4 train loss:5.292913436889648 train acc:0.0\n",
      "epoch:4 train loss:5.29777717590332 train acc:0.0\n",
      "epoch:4 train loss:5.298434734344482 train acc:0.0\n",
      "epoch:4 train loss:5.296562194824219 train acc:0.0\n",
      "epoch:4 train loss:5.299225330352783 train acc:0.0\n",
      "epoch:4 train loss:5.297092437744141 train acc:0.0\n",
      "epoch:4 train loss:5.299808025360107 train acc:0.0\n",
      "epoch:4 train loss:5.298798561096191 train acc:0.0\n",
      "epoch:4 train loss:5.299162864685059 train acc:0.0\n",
      "epoch:4 train loss:5.2999491691589355 train acc:0.0\n",
      "epoch:4 train loss:5.2983245849609375 train acc:0.0\n",
      "epoch:4 train loss:5.296533107757568 train acc:0.0\n",
      "epoch:4 train loss:5.294842720031738 train acc:0.0\n",
      "epoch:4 train loss:5.297245979309082 train acc:0.0\n",
      "epoch:4 train loss:5.296590805053711 train acc:0.0\n",
      "epoch:4 train loss:5.297928810119629 train acc:0.0\n",
      "epoch:4 train loss:5.298701286315918 train acc:0.0\n",
      "epoch:4 train loss:5.299182415008545 train acc:0.0\n",
      "epoch:4 train loss:5.29781436920166 train acc:0.0\n",
      "epoch:4 train loss:5.298343181610107 train acc:0.125\n",
      "epoch:4 train loss:5.302107810974121 train acc:0.0\n",
      "epoch:4 train loss:5.298491954803467 train acc:0.0\n",
      "epoch:4 train loss:5.301355361938477 train acc:0.0\n",
      "epoch:4 train loss:5.296363353729248 train acc:0.0\n",
      "epoch:4 train loss:5.296032428741455 train acc:0.0\n",
      "epoch:4 train loss:5.299134731292725 train acc:0.0\n",
      "epoch:4 train loss:5.299295425415039 train acc:0.0\n",
      "epoch:4 train loss:5.2992448806762695 train acc:0.0\n",
      "epoch:4 train loss:5.297460556030273 train acc:0.0\n",
      "epoch:4 train loss:5.297131061553955 train acc:0.0\n",
      "epoch:4 train loss:5.295844554901123 train acc:0.0\n",
      "epoch:4 train loss:5.301246166229248 train acc:0.0\n",
      "epoch:4 train loss:5.2962212562561035 train acc:0.0625\n",
      "epoch:4 train loss:5.298227787017822 train acc:0.0\n",
      "epoch:4 train loss:5.296477317810059 train acc:0.0625\n",
      "epoch:4 train loss:5.298306941986084 train acc:0.0\n",
      "epoch:4 train loss:5.297842979431152 train acc:0.0\n",
      "epoch:4 train loss:5.2981343269348145 train acc:0.0\n",
      "epoch:4 train loss:5.299795627593994 train acc:0.0\n",
      "epoch:4 train loss:5.299130916595459 train acc:0.0625\n",
      "epoch:4 train loss:5.296474933624268 train acc:0.0\n",
      "epoch:4 train loss:5.297048091888428 train acc:0.0\n",
      "epoch:4 train loss:5.297659397125244 train acc:0.0\n",
      "epoch:4 train loss:5.298904895782471 train acc:0.0\n",
      "epoch:4 train loss:5.298582553863525 train acc:0.0\n",
      "epoch:4 train loss:5.2974090576171875 train acc:0.0625\n",
      "epoch:4 train loss:5.29945707321167 train acc:0.0\n",
      "epoch:4 train loss:5.298727035522461 train acc:0.0\n",
      "epoch:4 train loss:5.300951957702637 train acc:0.0\n",
      "epoch:4 train loss:5.29987907409668 train acc:0.0\n",
      "epoch:4 train loss:5.298771381378174 train acc:0.0\n",
      "epoch:4 train loss:5.299096584320068 train acc:0.0\n",
      "epoch:4 train loss:5.2995429039001465 train acc:0.0\n",
      "epoch:4 train loss:5.296470642089844 train acc:0.0\n",
      "epoch:4 train loss:5.298525333404541 train acc:0.0\n",
      "epoch:4 train loss:5.300566673278809 train acc:0.0\n",
      "epoch:4 train loss:5.29801082611084 train acc:0.0\n",
      "epoch:4 train loss:5.296115875244141 train acc:0.0\n",
      "epoch:4 train loss:5.301029205322266 train acc:0.0\n",
      "epoch:4 train loss:5.29933500289917 train acc:0.0625\n",
      "epoch:4 train loss:5.296382427215576 train acc:0.0\n",
      "epoch:4 train loss:5.2975850105285645 train acc:0.0\n",
      "epoch:4 train loss:5.294983863830566 train acc:0.0\n",
      "epoch:4 train loss:5.298379898071289 train acc:0.0\n",
      "epoch:4 train loss:5.298702239990234 train acc:0.0625\n",
      "epoch:4 train loss:5.298343658447266 train acc:0.0\n",
      "epoch:4 train loss:5.297880172729492 train acc:0.0\n",
      "epoch:4 train loss:5.298004150390625 train acc:0.0\n",
      "epoch:4 train loss:5.301238536834717 train acc:0.0\n",
      "epoch:4 train loss:5.299368381500244 train acc:0.0\n",
      "epoch:4 train loss:5.29699182510376 train acc:0.0\n",
      "epoch:4 train loss:5.301970958709717 train acc:0.0\n",
      "epoch:4 train loss:5.300373077392578 train acc:0.0\n",
      "epoch:4 train loss:5.299898624420166 train acc:0.0\n",
      "epoch:4 train loss:5.295612335205078 train acc:0.0\n",
      "epoch:4 train loss:5.299051284790039 train acc:0.0\n",
      "epoch:4 train loss:5.294569969177246 train acc:0.0\n",
      "epoch:4 train loss:5.298162460327148 train acc:0.125\n",
      "epoch:4 train loss:5.296542167663574 train acc:0.0\n",
      "epoch:4 train loss:5.301051139831543 train acc:0.0\n",
      "epoch:4 train loss:5.299511909484863 train acc:0.0\n",
      "epoch:4 train loss:5.295799732208252 train acc:0.0\n",
      "epoch:4 train loss:5.299383163452148 train acc:0.0\n",
      "epoch:4 train loss:5.300032615661621 train acc:0.0\n",
      "epoch:4 train loss:5.2959136962890625 train acc:0.0\n",
      "epoch:4 train loss:5.29652738571167 train acc:0.0\n",
      "epoch:4 train loss:5.298130989074707 train acc:0.0\n",
      "epoch:4 train loss:5.298174858093262 train acc:0.0\n",
      "epoch:4 train loss:5.2979021072387695 train acc:0.0625\n",
      "epoch:4 train loss:5.298856258392334 train acc:0.0\n",
      "epoch:4 train loss:5.298527717590332 train acc:0.0\n",
      "epoch:4 train loss:5.299408912658691 train acc:0.0\n",
      "epoch:4 train loss:5.300681114196777 train acc:0.0\n",
      "epoch:4 train loss:5.29849910736084 train acc:0.0\n",
      "epoch:4 train loss:5.298965930938721 train acc:0.0\n",
      "epoch:4 train loss:5.297604560852051 train acc:0.0\n",
      "epoch:4 train loss:5.295562744140625 train acc:0.0\n",
      "epoch:4 train loss:5.2989935874938965 train acc:0.0\n",
      "epoch:4 train loss:5.297038555145264 train acc:0.0\n",
      "epoch:4 train loss:5.297717094421387 train acc:0.0\n",
      "epoch:4 train loss:5.297468185424805 train acc:0.0\n",
      "epoch:4 train loss:5.299281120300293 train acc:0.0\n",
      "epoch:4 train loss:5.297650337219238 train acc:0.0\n",
      "epoch:4 train loss:5.298273086547852 train acc:0.0\n",
      "epoch:4 train loss:5.299079895019531 train acc:0.0\n",
      "epoch:4 train loss:5.297076225280762 train acc:0.0\n",
      "epoch:4 train loss:5.29647159576416 train acc:0.0\n",
      "epoch:4 train loss:5.295556545257568 train acc:0.0\n",
      "epoch:4 train loss:5.297560691833496 train acc:0.0\n",
      "epoch:4 train loss:5.298225402832031 train acc:0.0\n",
      "epoch:4 train loss:5.298064231872559 train acc:0.0\n",
      "epoch:4 train loss:5.299091339111328 train acc:0.0\n",
      "epoch:4 train loss:5.294875144958496 train acc:0.0\n",
      "epoch:4 train loss:5.299108982086182 train acc:0.0\n",
      "epoch:4 train loss:5.3000688552856445 train acc:0.0\n",
      "epoch:4 train loss:5.298626899719238 train acc:0.0\n",
      "epoch:4 train loss:5.296536445617676 train acc:0.0\n",
      "epoch:4 train loss:5.297201633453369 train acc:0.0\n",
      "epoch:4 train loss:5.294214248657227 train acc:0.0\n",
      "epoch:4 train loss:5.2980241775512695 train acc:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 train loss:5.2994818687438965 train acc:0.0\n",
      "epoch:4 train loss:5.300606727600098 train acc:0.0\n",
      "epoch:4 train loss:5.299722194671631 train acc:0.0\n",
      "epoch:4 train loss:5.30001974105835 train acc:0.0\n",
      "epoch:4 train loss:5.2990217208862305 train acc:0.0\n",
      "epoch:4 train loss:5.299363613128662 train acc:0.0\n",
      "epoch:4 train loss:5.2986836433410645 train acc:0.0\n",
      "epoch:4 train loss:5.2978081703186035 train acc:0.0\n",
      "epoch:4 train loss:5.298384666442871 train acc:0.0\n",
      "epoch:4 train loss:5.299614906311035 train acc:0.0\n",
      "epoch:4 train loss:5.298681735992432 train acc:0.0\n",
      "epoch:4 train loss:5.300464153289795 train acc:0.0625\n",
      "epoch:4 train loss:5.300093650817871 train acc:0.0\n",
      "epoch:4 train loss:5.3009233474731445 train acc:0.0\n",
      "epoch:4 train loss:5.3002610206604 train acc:0.0\n",
      "epoch:4 train loss:5.296963214874268 train acc:0.0\n",
      "epoch:4 train loss:5.296417713165283 train acc:0.0\n",
      "epoch:4 train loss:5.298097610473633 train acc:0.0\n",
      "epoch:4 train loss:5.300309181213379 train acc:0.0\n",
      "epoch:4 train loss:5.296087741851807 train acc:0.0\n",
      "epoch:4 train loss:5.301539421081543 train acc:0.0\n",
      "epoch:4 train loss:5.298323631286621 train acc:0.0\n",
      "epoch:4 train loss:5.29918098449707 train acc:0.0\n",
      "epoch:4 train loss:5.298654079437256 train acc:0.0625\n",
      "epoch:4 train loss:5.298979759216309 train acc:0.0\n",
      "epoch:4 train loss:5.297641277313232 train acc:0.0\n",
      "epoch:4 train loss:5.299435138702393 train acc:0.0\n",
      "epoch:4 train loss:5.2967095375061035 train acc:0.0\n",
      "epoch:4 train loss:5.299322128295898 train acc:0.0\n",
      "epoch:4 train loss:5.3002028465271 train acc:0.0\n",
      "epoch:4 train loss:5.297053337097168 train acc:0.0\n",
      "epoch:4 train loss:5.3023529052734375 train acc:0.0\n",
      "epoch:4 train loss:5.29917573928833 train acc:0.0\n",
      "epoch:4 train loss:5.296351432800293 train acc:0.0\n",
      "epoch:4 train loss:5.297223091125488 train acc:0.0\n",
      "epoch:4 train loss:5.296497821807861 train acc:0.0\n",
      "epoch:4 train loss:5.2999043464660645 train acc:0.0\n",
      "epoch:4 train loss:5.2924323081970215 train acc:0.0625\n",
      "epoch:4 train loss:5.300176620483398 train acc:0.0\n",
      "epoch:4 train loss:5.297268390655518 train acc:0.0\n",
      "epoch:4 train loss:5.298491954803467 train acc:0.0\n",
      "epoch:4 train loss:5.297937870025635 train acc:0.0\n",
      "epoch:4 train loss:5.299680233001709 train acc:0.0\n",
      "epoch:4 train loss:5.298480033874512 train acc:0.0\n",
      "epoch:4 train loss:5.295350551605225 train acc:0.0\n",
      "epoch:4 train loss:5.2948784828186035 train acc:0.0\n",
      "epoch:4 train loss:5.297403335571289 train acc:0.0\n",
      "epoch:4 train loss:5.296809673309326 train acc:0.0\n",
      "epoch:4 train loss:5.2946929931640625 train acc:0.0\n",
      "epoch:4 train loss:5.296576023101807 train acc:0.0\n",
      "epoch:4 train loss:5.297367572784424 train acc:0.0\n",
      "epoch:4 train loss:5.2981791496276855 train acc:0.0\n",
      "epoch:4 train loss:5.292303562164307 train acc:0.0\n",
      "epoch:4 train loss:5.294126510620117 train acc:0.0\n",
      "epoch:4 train loss:5.295384883880615 train acc:0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033719b39bc54087807e16c9b5cba231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=363), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:5 train loss:5.297094821929932 train acc:0.0625\n",
      "epoch:5 train loss:5.299805164337158 train acc:0.0\n",
      "epoch:5 train loss:5.299352645874023 train acc:0.0\n",
      "epoch:5 train loss:5.299484729766846 train acc:0.0\n",
      "epoch:5 train loss:5.299354553222656 train acc:0.0\n",
      "epoch:5 train loss:5.298647880554199 train acc:0.0\n",
      "epoch:5 train loss:5.295302867889404 train acc:0.0\n",
      "epoch:5 train loss:5.297398090362549 train acc:0.0625\n",
      "epoch:5 train loss:5.29752254486084 train acc:0.0\n",
      "epoch:5 train loss:5.297981262207031 train acc:0.0\n",
      "epoch:5 train loss:5.298742294311523 train acc:0.0\n",
      "epoch:5 train loss:5.29599666595459 train acc:0.0\n",
      "epoch:5 train loss:5.3003363609313965 train acc:0.0625\n",
      "epoch:5 train loss:5.298507213592529 train acc:0.0\n",
      "epoch:5 train loss:5.298564434051514 train acc:0.0\n",
      "epoch:5 train loss:5.299243927001953 train acc:0.0\n",
      "epoch:5 train loss:5.298691749572754 train acc:0.0\n",
      "epoch:5 train loss:5.298595905303955 train acc:0.0\n",
      "epoch:5 train loss:5.299068450927734 train acc:0.0\n",
      "epoch:5 train loss:5.3023457527160645 train acc:0.0\n",
      "epoch:5 train loss:5.297588348388672 train acc:0.0\n",
      "epoch:5 train loss:5.297601699829102 train acc:0.0\n",
      "epoch:5 train loss:5.297059059143066 train acc:0.0\n",
      "epoch:5 train loss:5.298666954040527 train acc:0.0625\n",
      "epoch:5 train loss:5.299501419067383 train acc:0.0\n",
      "epoch:5 train loss:5.297464847564697 train acc:0.0\n",
      "epoch:5 train loss:5.296852111816406 train acc:0.0\n",
      "epoch:5 train loss:5.297284126281738 train acc:0.0\n",
      "epoch:5 train loss:5.297705173492432 train acc:0.0\n",
      "epoch:5 train loss:5.30119514465332 train acc:0.0\n",
      "epoch:5 train loss:5.298679351806641 train acc:0.0\n",
      "epoch:5 train loss:5.295780181884766 train acc:0.0625\n",
      "epoch:5 train loss:5.29667329788208 train acc:0.0\n",
      "epoch:5 train loss:5.2984466552734375 train acc:0.0\n",
      "epoch:5 train loss:5.300875663757324 train acc:0.0\n",
      "epoch:5 train loss:5.2992353439331055 train acc:0.0\n",
      "epoch:5 train loss:5.298839569091797 train acc:0.0\n",
      "epoch:5 train loss:5.2954421043396 train acc:0.0\n",
      "epoch:5 train loss:5.2998809814453125 train acc:0.0\n",
      "epoch:5 train loss:5.29955530166626 train acc:0.0\n",
      "epoch:5 train loss:5.298631191253662 train acc:0.0\n",
      "epoch:5 train loss:5.29777717590332 train acc:0.0\n",
      "epoch:5 train loss:5.296039581298828 train acc:0.0\n",
      "epoch:5 train loss:5.300314903259277 train acc:0.0\n",
      "epoch:5 train loss:5.2982563972473145 train acc:0.0\n",
      "epoch:5 train loss:5.301846981048584 train acc:0.0\n",
      "epoch:5 train loss:5.299575328826904 train acc:0.0\n",
      "epoch:5 train loss:5.2994704246521 train acc:0.0\n",
      "epoch:5 train loss:5.300678253173828 train acc:0.0\n",
      "epoch:5 train loss:5.296736240386963 train acc:0.0625\n",
      "epoch:5 train loss:5.295608997344971 train acc:0.0\n",
      "epoch:5 train loss:5.295841217041016 train acc:0.0\n",
      "epoch:5 train loss:5.29913854598999 train acc:0.0\n",
      "epoch:5 train loss:5.300802230834961 train acc:0.0\n",
      "epoch:5 train loss:5.302398204803467 train acc:0.0\n",
      "epoch:5 train loss:5.297157287597656 train acc:0.0625\n",
      "epoch:5 train loss:5.297688007354736 train acc:0.0\n",
      "epoch:5 train loss:5.300414562225342 train acc:0.0\n",
      "epoch:5 train loss:5.300974369049072 train acc:0.0\n",
      "epoch:5 train loss:5.298264026641846 train acc:0.0\n",
      "epoch:5 train loss:5.295301914215088 train acc:0.0\n",
      "epoch:5 train loss:5.299510955810547 train acc:0.0\n",
      "epoch:5 train loss:5.2961297035217285 train acc:0.0\n",
      "epoch:5 train loss:5.300022602081299 train acc:0.0\n",
      "epoch:5 train loss:5.298559665679932 train acc:0.0\n",
      "epoch:5 train loss:5.299651145935059 train acc:0.0\n",
      "epoch:5 train loss:5.296194553375244 train acc:0.0625\n",
      "epoch:5 train loss:5.298041343688965 train acc:0.0\n",
      "epoch:5 train loss:5.298211574554443 train acc:0.0\n",
      "epoch:5 train loss:5.295578956604004 train acc:0.0\n",
      "epoch:5 train loss:5.300792217254639 train acc:0.0\n",
      "epoch:5 train loss:5.29857873916626 train acc:0.0\n",
      "epoch:5 train loss:5.297796249389648 train acc:0.0\n",
      "epoch:5 train loss:5.298386573791504 train acc:0.0\n",
      "epoch:5 train loss:5.2973856925964355 train acc:0.0\n",
      "epoch:5 train loss:5.296852111816406 train acc:0.0\n",
      "epoch:5 train loss:5.299263954162598 train acc:0.0\n",
      "epoch:5 train loss:5.298194408416748 train acc:0.0\n",
      "epoch:5 train loss:5.2974629402160645 train acc:0.0\n",
      "epoch:5 train loss:5.298229217529297 train acc:0.0\n",
      "epoch:5 train loss:5.298199653625488 train acc:0.0\n",
      "epoch:5 train loss:5.2953338623046875 train acc:0.0\n",
      "epoch:5 train loss:5.298313140869141 train acc:0.0\n",
      "epoch:5 train loss:5.29878044128418 train acc:0.0\n",
      "epoch:5 train loss:5.2964253425598145 train acc:0.0\n",
      "epoch:5 train loss:5.296928405761719 train acc:0.0\n",
      "epoch:5 train loss:5.29571008682251 train acc:0.0625\n",
      "epoch:5 train loss:5.298877239227295 train acc:0.0\n",
      "epoch:5 train loss:5.299281120300293 train acc:0.0\n",
      "epoch:5 train loss:5.295650482177734 train acc:0.0\n",
      "epoch:5 train loss:5.297472953796387 train acc:0.0\n",
      "epoch:5 train loss:5.299098491668701 train acc:0.0\n",
      "epoch:5 train loss:5.298555374145508 train acc:0.0\n",
      "epoch:5 train loss:5.302058696746826 train acc:0.0\n",
      "epoch:5 train loss:5.300466060638428 train acc:0.0\n",
      "epoch:5 train loss:5.298012733459473 train acc:0.0\n",
      "epoch:5 train loss:5.297578811645508 train acc:0.0\n",
      "epoch:5 train loss:5.298583507537842 train acc:0.0\n",
      "epoch:5 train loss:5.298263072967529 train acc:0.0\n",
      "epoch:5 train loss:5.298738479614258 train acc:0.0\n",
      "epoch:5 train loss:5.302488327026367 train acc:0.0\n",
      "epoch:5 train loss:5.297286510467529 train acc:0.0\n",
      "epoch:5 train loss:5.298038005828857 train acc:0.0\n",
      "epoch:5 train loss:5.298063278198242 train acc:0.0\n",
      "epoch:5 train loss:5.296180248260498 train acc:0.0\n",
      "epoch:5 train loss:5.297952175140381 train acc:0.0\n",
      "epoch:5 train loss:5.298375606536865 train acc:0.0\n",
      "epoch:5 train loss:5.300930500030518 train acc:0.0\n",
      "epoch:5 train loss:5.300805568695068 train acc:0.0\n",
      "epoch:5 train loss:5.298436164855957 train acc:0.0\n",
      "epoch:5 train loss:5.296932697296143 train acc:0.0\n",
      "epoch:5 train loss:5.299482345581055 train acc:0.0\n",
      "epoch:5 train loss:5.298511981964111 train acc:0.0\n",
      "epoch:5 train loss:5.297369003295898 train acc:0.0\n",
      "epoch:5 train loss:5.300766944885254 train acc:0.0\n",
      "epoch:5 train loss:5.295076847076416 train acc:0.0\n",
      "epoch:5 train loss:5.297755718231201 train acc:0.0\n",
      "epoch:5 train loss:5.299308776855469 train acc:0.0\n",
      "epoch:5 train loss:5.299473762512207 train acc:0.0\n",
      "epoch:5 train loss:5.295980453491211 train acc:0.0\n",
      "epoch:5 train loss:5.299442768096924 train acc:0.0\n",
      "epoch:5 train loss:5.299971580505371 train acc:0.0\n",
      "epoch:5 train loss:5.2954936027526855 train acc:0.0\n",
      "epoch:5 train loss:5.297302722930908 train acc:0.0\n",
      "epoch:5 train loss:5.300370216369629 train acc:0.0\n",
      "epoch:5 train loss:5.299722194671631 train acc:0.0\n",
      "epoch:5 train loss:5.298305034637451 train acc:0.0\n",
      "epoch:5 train loss:5.296544075012207 train acc:0.0\n",
      "epoch:5 train loss:5.3002753257751465 train acc:0.0\n",
      "epoch:5 train loss:5.297332286834717 train acc:0.0\n",
      "epoch:5 train loss:5.2989630699157715 train acc:0.0\n",
      "epoch:5 train loss:5.297287940979004 train acc:0.0\n",
      "epoch:5 train loss:5.29994010925293 train acc:0.0\n",
      "epoch:5 train loss:5.301380634307861 train acc:0.0\n",
      "epoch:5 train loss:5.300806522369385 train acc:0.0\n",
      "epoch:5 train loss:5.298720836639404 train acc:0.0\n",
      "epoch:5 train loss:5.3009538650512695 train acc:0.0\n",
      "epoch:5 train loss:5.294632434844971 train acc:0.0\n",
      "epoch:5 train loss:5.293371677398682 train acc:0.125\n",
      "epoch:5 train loss:5.298171520233154 train acc:0.0\n",
      "epoch:5 train loss:5.298867702484131 train acc:0.0\n",
      "epoch:5 train loss:5.300093650817871 train acc:0.0\n",
      "epoch:5 train loss:5.293716907501221 train acc:0.0\n",
      "epoch:5 train loss:5.297865390777588 train acc:0.0\n",
      "epoch:5 train loss:5.300042629241943 train acc:0.0\n",
      "epoch:5 train loss:5.298797130584717 train acc:0.0\n",
      "epoch:5 train loss:5.298980236053467 train acc:0.0\n",
      "epoch:5 train loss:5.2968902587890625 train acc:0.0625\n",
      "epoch:5 train loss:5.297657012939453 train acc:0.0\n",
      "epoch:5 train loss:5.299692153930664 train acc:0.0\n",
      "epoch:5 train loss:5.298389911651611 train acc:0.0\n",
      "epoch:5 train loss:5.301029205322266 train acc:0.0\n",
      "epoch:5 train loss:5.299158573150635 train acc:0.0\n",
      "epoch:5 train loss:5.297222137451172 train acc:0.0\n",
      "epoch:5 train loss:5.298722743988037 train acc:0.0\n",
      "epoch:5 train loss:5.297418117523193 train acc:0.0\n",
      "epoch:5 train loss:5.298419952392578 train acc:0.0\n",
      "epoch:5 train loss:5.297891616821289 train acc:0.0\n",
      "epoch:5 train loss:5.298303127288818 train acc:0.0\n",
      "epoch:5 train loss:5.299879550933838 train acc:0.0\n",
      "epoch:5 train loss:5.296534538269043 train acc:0.0\n",
      "epoch:5 train loss:5.298028469085693 train acc:0.0\n",
      "epoch:5 train loss:5.297712802886963 train acc:0.0\n",
      "epoch:5 train loss:5.300328731536865 train acc:0.0\n",
      "epoch:5 train loss:5.302134990692139 train acc:0.0\n",
      "epoch:5 train loss:5.29803991317749 train acc:0.0\n",
      "epoch:5 train loss:5.297659397125244 train acc:0.0\n",
      "epoch:5 train loss:5.300637722015381 train acc:0.0\n",
      "epoch:5 train loss:5.298062324523926 train acc:0.0\n",
      "epoch:5 train loss:5.300174713134766 train acc:0.0\n",
      "epoch:5 train loss:5.294441223144531 train acc:0.0\n",
      "epoch:5 train loss:5.29954195022583 train acc:0.0\n",
      "epoch:5 train loss:5.299414157867432 train acc:0.0\n",
      "epoch:5 train loss:5.297107696533203 train acc:0.0625\n",
      "epoch:5 train loss:5.298903942108154 train acc:0.0\n",
      "epoch:5 train loss:5.2984442710876465 train acc:0.0\n",
      "epoch:5 train loss:5.2972283363342285 train acc:0.0\n",
      "epoch:5 train loss:5.301107883453369 train acc:0.0\n",
      "epoch:5 train loss:5.2987565994262695 train acc:0.0\n",
      "epoch:5 train loss:5.2985520362854 train acc:0.0\n",
      "epoch:5 train loss:5.29945182800293 train acc:0.0\n",
      "epoch:5 train loss:5.299342155456543 train acc:0.0\n",
      "epoch:5 train loss:5.300523281097412 train acc:0.0\n",
      "epoch:5 train loss:5.2976975440979 train acc:0.0\n",
      "epoch:5 train loss:5.296472072601318 train acc:0.0\n",
      "epoch:5 train loss:5.298617362976074 train acc:0.0\n",
      "epoch:5 train loss:5.298644065856934 train acc:0.0\n",
      "epoch:5 train loss:5.296871662139893 train acc:0.0\n",
      "epoch:5 train loss:5.299850940704346 train acc:0.0\n",
      "epoch:5 train loss:5.29815149307251 train acc:0.0\n",
      "epoch:5 train loss:5.2985968589782715 train acc:0.0625\n",
      "epoch:5 train loss:5.297046661376953 train acc:0.0\n",
      "epoch:5 train loss:5.299263954162598 train acc:0.0\n",
      "epoch:5 train loss:5.297125816345215 train acc:0.0\n",
      "epoch:5 train loss:5.299368381500244 train acc:0.0\n",
      "epoch:5 train loss:5.296103477478027 train acc:0.0\n",
      "epoch:5 train loss:5.298990249633789 train acc:0.0\n",
      "epoch:5 train loss:5.296419143676758 train acc:0.0\n",
      "epoch:5 train loss:5.299273490905762 train acc:0.0\n",
      "epoch:5 train loss:5.298182010650635 train acc:0.0\n",
      "epoch:5 train loss:5.295483112335205 train acc:0.0\n",
      "epoch:5 train loss:5.2993669509887695 train acc:0.0\n",
      "epoch:5 train loss:5.297957420349121 train acc:0.0\n",
      "epoch:5 train loss:5.298121452331543 train acc:0.0\n",
      "epoch:5 train loss:5.2973952293396 train acc:0.0\n",
      "epoch:5 train loss:5.300795078277588 train acc:0.0\n",
      "epoch:5 train loss:5.3002729415893555 train acc:0.0\n",
      "epoch:5 train loss:5.296672821044922 train acc:0.0\n",
      "epoch:5 train loss:5.298846244812012 train acc:0.0\n",
      "epoch:5 train loss:5.295637607574463 train acc:0.0\n",
      "epoch:5 train loss:5.296718597412109 train acc:0.0\n",
      "epoch:5 train loss:5.300830364227295 train acc:0.0\n",
      "epoch:5 train loss:5.298093795776367 train acc:0.0\n",
      "epoch:5 train loss:5.298210620880127 train acc:0.0\n",
      "epoch:5 train loss:5.299561500549316 train acc:0.0\n",
      "epoch:5 train loss:5.298361301422119 train acc:0.0\n",
      "epoch:5 train loss:5.2954840660095215 train acc:0.0\n",
      "epoch:5 train loss:5.294414520263672 train acc:0.0\n",
      "epoch:5 train loss:5.297080993652344 train acc:0.0625\n",
      "epoch:5 train loss:5.296043395996094 train acc:0.0\n",
      "epoch:5 train loss:5.296637535095215 train acc:0.0625\n",
      "epoch:5 train loss:5.298135757446289 train acc:0.125\n",
      "epoch:5 train loss:5.298965930938721 train acc:0.0\n",
      "epoch:5 train loss:5.297919273376465 train acc:0.0\n",
      "epoch:5 train loss:5.296097278594971 train acc:0.0\n",
      "epoch:5 train loss:5.296482086181641 train acc:0.0\n",
      "epoch:5 train loss:5.294445037841797 train acc:0.0625\n",
      "epoch:5 train loss:5.298733711242676 train acc:0.0\n",
      "epoch:5 train loss:5.298610687255859 train acc:0.0\n",
      "epoch:5 train loss:5.298098564147949 train acc:0.0\n",
      "epoch:5 train loss:5.302068710327148 train acc:0.0\n",
      "epoch:5 train loss:5.298885822296143 train acc:0.0\n",
      "epoch:5 train loss:5.298792362213135 train acc:0.0\n",
      "epoch:5 train loss:5.296227931976318 train acc:0.0\n",
      "epoch:5 train loss:5.296485424041748 train acc:0.0\n",
      "epoch:5 train loss:5.300838947296143 train acc:0.0\n",
      "epoch:5 train loss:5.297974586486816 train acc:0.0\n",
      "epoch:5 train loss:5.299581050872803 train acc:0.0\n",
      "epoch:5 train loss:5.298164367675781 train acc:0.0\n",
      "epoch:5 train loss:5.299941062927246 train acc:0.0\n",
      "epoch:5 train loss:5.294610023498535 train acc:0.0\n",
      "epoch:5 train loss:5.298914432525635 train acc:0.0\n",
      "epoch:5 train loss:5.296245574951172 train acc:0.0\n",
      "epoch:5 train loss:5.298977375030518 train acc:0.0\n",
      "epoch:5 train loss:5.2979302406311035 train acc:0.0\n",
      "epoch:5 train loss:5.299445152282715 train acc:0.0\n",
      "epoch:5 train loss:5.299911975860596 train acc:0.0\n",
      "epoch:5 train loss:5.299579620361328 train acc:0.0\n",
      "epoch:5 train loss:5.299129962921143 train acc:0.0\n",
      "epoch:5 train loss:5.298582553863525 train acc:0.0\n",
      "epoch:5 train loss:5.297374725341797 train acc:0.0\n",
      "epoch:5 train loss:5.2996392250061035 train acc:0.0\n",
      "epoch:5 train loss:5.2977213859558105 train acc:0.0\n",
      "epoch:5 train loss:5.298907279968262 train acc:0.0\n",
      "epoch:5 train loss:5.29736852645874 train acc:0.0\n",
      "epoch:5 train loss:5.296511173248291 train acc:0.0\n",
      "epoch:5 train loss:5.298033237457275 train acc:0.0\n",
      "epoch:5 train loss:5.298958778381348 train acc:0.0\n",
      "epoch:5 train loss:5.298214912414551 train acc:0.0\n",
      "epoch:5 train loss:5.300157070159912 train acc:0.0\n",
      "epoch:5 train loss:5.297865390777588 train acc:0.0\n",
      "epoch:5 train loss:5.299095630645752 train acc:0.0\n",
      "epoch:5 train loss:5.298111438751221 train acc:0.0\n",
      "epoch:5 train loss:5.298946857452393 train acc:0.0\n",
      "epoch:5 train loss:5.29811429977417 train acc:0.0\n",
      "epoch:5 train loss:5.296828269958496 train acc:0.0\n",
      "epoch:5 train loss:5.300319671630859 train acc:0.0\n",
      "epoch:5 train loss:5.29949951171875 train acc:0.0\n",
      "epoch:5 train loss:5.298696041107178 train acc:0.0\n",
      "epoch:5 train loss:5.2982659339904785 train acc:0.0\n",
      "epoch:5 train loss:5.30111026763916 train acc:0.0\n",
      "epoch:5 train loss:5.30079984664917 train acc:0.0\n",
      "epoch:5 train loss:5.300088882446289 train acc:0.0\n",
      "epoch:5 train loss:5.29874324798584 train acc:0.0625\n",
      "epoch:5 train loss:5.296262264251709 train acc:0.0\n",
      "epoch:5 train loss:5.298436164855957 train acc:0.0\n",
      "epoch:5 train loss:5.299447536468506 train acc:0.0\n",
      "epoch:5 train loss:5.299641132354736 train acc:0.0\n",
      "epoch:5 train loss:5.297947406768799 train acc:0.0\n",
      "epoch:5 train loss:5.297231197357178 train acc:0.0\n",
      "epoch:5 train loss:5.2969970703125 train acc:0.0\n",
      "epoch:5 train loss:5.296408653259277 train acc:0.0\n",
      "epoch:5 train loss:5.299927234649658 train acc:0.0\n",
      "epoch:5 train loss:5.299562454223633 train acc:0.0\n",
      "epoch:5 train loss:5.2978925704956055 train acc:0.0\n",
      "epoch:5 train loss:5.296954154968262 train acc:0.0\n",
      "epoch:5 train loss:5.301499366760254 train acc:0.0\n",
      "epoch:5 train loss:5.298252105712891 train acc:0.0\n",
      "epoch:5 train loss:5.294804573059082 train acc:0.0625\n",
      "epoch:5 train loss:5.299785137176514 train acc:0.0\n",
      "epoch:5 train loss:5.296125888824463 train acc:0.0\n",
      "epoch:5 train loss:5.298140525817871 train acc:0.0\n",
      "epoch:5 train loss:5.299436569213867 train acc:0.0\n",
      "epoch:5 train loss:5.298420429229736 train acc:0.0\n",
      "epoch:5 train loss:5.300016403198242 train acc:0.0\n",
      "epoch:5 train loss:5.297474384307861 train acc:0.0\n",
      "epoch:5 train loss:5.301819324493408 train acc:0.0\n",
      "epoch:5 train loss:5.298676490783691 train acc:0.0\n",
      "epoch:5 train loss:5.296030044555664 train acc:0.0625\n",
      "epoch:5 train loss:5.296389102935791 train acc:0.0\n",
      "epoch:5 train loss:5.298691749572754 train acc:0.0\n",
      "epoch:5 train loss:5.299902439117432 train acc:0.0\n",
      "epoch:5 train loss:5.298958778381348 train acc:0.0\n",
      "epoch:5 train loss:5.298064708709717 train acc:0.0\n",
      "epoch:5 train loss:5.299361705780029 train acc:0.0\n",
      "epoch:5 train loss:5.296120643615723 train acc:0.0\n",
      "epoch:5 train loss:5.296630382537842 train acc:0.0\n",
      "epoch:5 train loss:5.299506187438965 train acc:0.0\n",
      "epoch:5 train loss:5.296480178833008 train acc:0.0625\n",
      "epoch:5 train loss:5.297529220581055 train acc:0.0\n",
      "epoch:5 train loss:5.299363136291504 train acc:0.0\n",
      "epoch:5 train loss:5.29865026473999 train acc:0.0\n",
      "epoch:5 train loss:5.299698829650879 train acc:0.0\n",
      "epoch:5 train loss:5.299396991729736 train acc:0.0\n",
      "epoch:5 train loss:5.297987461090088 train acc:0.0\n",
      "epoch:5 train loss:5.296548366546631 train acc:0.0\n",
      "epoch:5 train loss:5.302093982696533 train acc:0.0\n",
      "epoch:5 train loss:5.296665668487549 train acc:0.0625\n",
      "epoch:5 train loss:5.294898509979248 train acc:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 train loss:5.298140525817871 train acc:0.0\n",
      "epoch:5 train loss:5.296896457672119 train acc:0.0\n",
      "epoch:5 train loss:5.300300598144531 train acc:0.0\n",
      "epoch:5 train loss:5.2984395027160645 train acc:0.0\n",
      "epoch:5 train loss:5.2971391677856445 train acc:0.0\n",
      "epoch:5 train loss:5.297495365142822 train acc:0.0\n",
      "epoch:5 train loss:5.300411701202393 train acc:0.0\n",
      "epoch:5 train loss:5.299904823303223 train acc:0.0\n",
      "epoch:5 train loss:5.297513961791992 train acc:0.0\n",
      "epoch:5 train loss:5.296920299530029 train acc:0.0\n",
      "epoch:5 train loss:5.299563884735107 train acc:0.0625\n",
      "epoch:5 train loss:5.296277046203613 train acc:0.0\n",
      "epoch:5 train loss:5.296623229980469 train acc:0.0\n",
      "epoch:5 train loss:5.301269054412842 train acc:0.0\n",
      "epoch:5 train loss:5.29599142074585 train acc:0.0\n",
      "epoch:5 train loss:5.30143404006958 train acc:0.0\n",
      "epoch:5 train loss:5.299649715423584 train acc:0.0625\n",
      "epoch:5 train loss:5.298337936401367 train acc:0.0\n",
      "epoch:5 train loss:5.300445556640625 train acc:0.0\n",
      "epoch:5 train loss:5.299846649169922 train acc:0.0\n",
      "epoch:5 train loss:5.296605110168457 train acc:0.0\n",
      "epoch:5 train loss:5.298012733459473 train acc:0.0\n",
      "epoch:5 train loss:5.298295974731445 train acc:0.0\n",
      "epoch:5 train loss:5.296078205108643 train acc:0.0\n",
      "epoch:5 train loss:5.300989151000977 train acc:0.0625\n",
      "epoch:5 train loss:5.297924995422363 train acc:0.0\n",
      "epoch:5 train loss:5.296741485595703 train acc:0.0\n",
      "epoch:5 train loss:5.296387672424316 train acc:0.0\n",
      "epoch:5 train loss:5.298444747924805 train acc:0.0\n",
      "epoch:5 train loss:5.296696662902832 train acc:0.0\n",
      "epoch:5 train loss:5.2987260818481445 train acc:0.0\n",
      "epoch:5 train loss:5.301205635070801 train acc:0.0\n",
      "epoch:5 train loss:5.299017429351807 train acc:0.0\n",
      "epoch:5 train loss:5.297983646392822 train acc:0.0\n",
      "epoch:5 train loss:5.298709392547607 train acc:0.0\n",
      "epoch:5 train loss:5.295049667358398 train acc:0.0\n",
      "epoch:5 train loss:5.2983832359313965 train acc:0.0\n",
      "epoch:5 train loss:5.29880952835083 train acc:0.0\n",
      "epoch:5 train loss:5.302425384521484 train acc:0.0\n",
      "epoch:5 train loss:5.301702499389648 train acc:0.0\n",
      "epoch:5 train loss:5.297260761260986 train acc:0.0\n",
      "epoch:5 train loss:5.298965930938721 train acc:0.0\n",
      "epoch:5 train loss:5.299408435821533 train acc:0.0\n",
      "epoch:5 train loss:5.295272350311279 train acc:0.0\n",
      "epoch:5 train loss:5.30087423324585 train acc:0.0\n",
      "epoch:5 train loss:5.298543930053711 train acc:0.0\n",
      "epoch:5 train loss:5.30189847946167 train acc:0.0\n",
      "epoch:5 train loss:5.295754909515381 train acc:0.0\n",
      "epoch:5 train loss:5.29832649230957 train acc:0.0\n",
      "epoch:5 train loss:5.299825668334961 train acc:0.0\n",
      "epoch:5 train loss:5.297345161437988 train acc:0.0\n",
      "epoch:5 train loss:5.297701358795166 train acc:0.0\n",
      "epoch:5 train loss:5.302831649780273 train acc:0.0\n",
      "epoch:5 train loss:5.297726631164551 train acc:0.0\n",
      "epoch:5 train loss:5.295683860778809 train acc:0.0\n",
      "epoch:5 train loss:5.299166679382324 train acc:0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0252640eb67d4ea790e099c32e2a86a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=363), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\App\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-bd90bddd2cc3>\", line 60, in <module>\n",
      "    val_result.extend(torch.argmax(logits, dim=-1).cpu().data.numpy())\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\App\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\App\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\App\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\App\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\App\\Anaconda3\\lib\\inspect.py\", line 1500, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\App\\Anaconda3\\lib\\inspect.py\", line 1458, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\App\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\App\\Anaconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\App\\Anaconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\App\\Anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\App\\Anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#初始化\n",
    "torch.cuda.empty_cache()\n",
    "epoch=0\n",
    "train_acc_arr=[]\n",
    "val_acc_arr=[]\n",
    "\n",
    "#加载数据\n",
    "dataset=Loader(args=args)\n",
    "dataloader=DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "#加载模型\n",
    "model=get_model(args=args)\n",
    "\n",
    "#加载参数\n",
    "loss_func=get_loss_func(args=args)\n",
    "    \n",
    "#训练\n",
    "if args.continue_train:\n",
    "    model.load_state_dict(torch.load(args.train_model))\n",
    "    check_dict=load_check_point(args=args)\n",
    "    epoch=check_dict['epoch']\n",
    "    train_acc_arr=check_dict['train_acc_arr']\n",
    "    val_acc_arr=check_dict['val_acc_arr']\n",
    "    \n",
    "while epoch <args.epoch:\n",
    "    opt = get_finetune_optimizer(args, model)\n",
    "    \n",
    "    train_result = []\n",
    "    train_label = []\n",
    "    val_result = []\n",
    "    val_label = []\n",
    "    \n",
    "    for step, (img_id, img, label, bbox) in enumerate(dataloader):\n",
    "        img = img.cuda()\n",
    "        label = label.cuda()\n",
    "\n",
    "        logits, cam = model.forward(img)\n",
    "        loss = loss_func(logits, label)\n",
    "        acc = cal_acc(logits, label)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        print('epoch:{} train loss:{} train acc:{}'.format(epoch, loss, acc))\n",
    "\n",
    "        train_result.extend(torch.argmax(logits, dim=-1).cpu().data.numpy())\n",
    "        train_label.extend(label.cpu().data.numpy())\n",
    "    \n",
    "    train_acc_arr.append(np.mean(np.array(train_result) == np.array(train_label)))\n",
    "    \n",
    "    # validation\n",
    "    dataset.to_val()\n",
    "    val_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    for step, (img_id, img, label, bbox) in enumerate(tqdm_notebook(val_dataloader)):\n",
    "        img = img.cuda()\n",
    "        label = label.cuda()\n",
    "\n",
    "        logits, cam = model.forward(img)\n",
    "        val_result.extend(torch.argmax(logits, dim=-1).cpu().data.numpy())\n",
    "        val_label.extend(label.cpu().data.numpy())\n",
    "        \n",
    "        if step==0:\n",
    "            target_cls=torch.argmax(logits,dim=-1)\n",
    "            \n",
    "            plot_dict={}\n",
    "            plot_dict['raw_imgs']=get_raw_imgs_by_id(args,img_id[:5],dataset)\n",
    "            target_cams=[]\n",
    "            for i in range(5):\n",
    "                raw_img_size=plot_dict['raw_imgs'][i].size\n",
    "                target_cam=cam[i][target_cls[i]].unsqueeze(0).unsqueeze(0).detach().cpu().data\n",
    "                up_target_cam=F.upsample(target_cam, size=(raw_img_size[1],raw_img_size[0]), mode='bilinear', align_corners=True)\n",
    "                target_cams.append(up_target_cam.squeeze())\n",
    "                \n",
    "            plot_dict['cams']=target_cams\n",
    "            plot_different_figs(plot_dict)\n",
    "            \n",
    "\n",
    "    val_acc_arr.append(np.mean(np.array(val_result) == np.array(val_label)))\n",
    "    \n",
    "    if len(val_acc_arr)==1 or val_acc_arr[-1]>=val_acc_arr[-2]:\n",
    "        torch.save(model.state_dict(), args.train_model)\n",
    "    \n",
    "    #plot\n",
    "    plot_train_process(args,[train_acc_arr,val_acc_arr])\n",
    "    \n",
    "    #save check point\n",
    "    epoch+=1\n",
    "    save_check_point(args=args,check_dict={\n",
    "        'epoch':epoch,\n",
    "        'train_acc_arr':train_acc_arr,\n",
    "        'val_acc_arr':val_acc_arr\n",
    "    })\n",
    "    \n",
    "    dataset.to_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dict=load_check_point(args=args)\n",
    "epoch=check_dict['epoch']\n",
    "train_acc_arr=check_dict['train_acc_arr']\n",
    "val_acc_arr=check_dict['val_acc_arr']\n",
    "\n",
    "print('train acc:{}'.format(train_acc_arr[-1]))\n",
    "print('val acc:{}'.format(val_acc_arr[-1]))\n",
    "\n",
    "plt.imshow(read_one_fig(args.train_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算validation数据集的iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dataset.to_val()\n",
    "val_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "iou_result=[]\n",
    "cls_result=[]\n",
    "for step, (img_id, img, label, bbox) in enumerate(tqdm_notebook(val_dataloader)):\n",
    "    img = img.cuda()\n",
    "    label = label.cuda()\n",
    "\n",
    "    logits, cam = model.forward(img)\n",
    "    cls_predict=torch.argmax(logits,dim=-1)\n",
    "    raw_imgs=get_raw_imgs_by_id(args,img_id,dataset)\n",
    "    for i in range(len(img_id)):\n",
    "        target_cam=cam[i][cls_predict[i]].unsqueeze(0).unsqueeze(0)\n",
    "        raw_img_size=raw_imgs[i].size\n",
    "        up_cam=F.upsample(target_cam.detach().cpu().data, size=(raw_img_size[1],raw_img_size[0]), mode='bilinear', align_corners=True)\n",
    "        binary_cam=model.norm_cam_2_binary(up_cam)\n",
    "        largest_binary_cam=get_max_binary_area(binary_cam.squeeze().numpy())\n",
    "        gen_bbox=get_bbox_from_binary_cam(largest_binary_cam)\n",
    "        iou_result.append(get_iou(gen_bbox,[float(x) for x in bbox[i].split(' ')]))\n",
    "        \n",
    "    cls_result.extend(cls_predict.cpu().numpy()==label.cpu().numpy())\n",
    "    \n",
    "print('iou result on validation is:{}'.format(np.mean((np.array(iou_result)>0.5)*np.array(cls_result))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
